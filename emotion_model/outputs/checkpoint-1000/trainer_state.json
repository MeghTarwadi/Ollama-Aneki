{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.05343735805701766,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 5.343735805701766e-05,
      "grad_norm": 11.970237731933594,
      "learning_rate": 4e-05,
      "loss": 7.7476,
      "step": 1
    },
    {
      "epoch": 0.00010687471611403532,
      "grad_norm": 10.701306343078613,
      "learning_rate": 8e-05,
      "loss": 7.9686,
      "step": 2
    },
    {
      "epoch": 0.00016031207417105297,
      "grad_norm": 11.011762619018555,
      "learning_rate": 0.00012,
      "loss": 7.7773,
      "step": 3
    },
    {
      "epoch": 0.00021374943222807064,
      "grad_norm": 9.025511741638184,
      "learning_rate": 0.00016,
      "loss": 7.3149,
      "step": 4
    },
    {
      "epoch": 0.0002671867902850883,
      "grad_norm": 13.776436805725098,
      "learning_rate": 0.0002,
      "loss": 6.8264,
      "step": 5
    },
    {
      "epoch": 0.00032062414834210595,
      "grad_norm": 12.221830368041992,
      "learning_rate": 0.0001999893093863588,
      "loss": 5.7921,
      "step": 6
    },
    {
      "epoch": 0.0003740615063991236,
      "grad_norm": 9.13806438446045,
      "learning_rate": 0.00019997861877271757,
      "loss": 5.5478,
      "step": 7
    },
    {
      "epoch": 0.0004274988644561413,
      "grad_norm": 12.186979293823242,
      "learning_rate": 0.00019996792815907635,
      "loss": 4.7754,
      "step": 8
    },
    {
      "epoch": 0.00048093622251315894,
      "grad_norm": 12.555595397949219,
      "learning_rate": 0.00019995723754543513,
      "loss": 4.6827,
      "step": 9
    },
    {
      "epoch": 0.0005343735805701766,
      "grad_norm": 18.793758392333984,
      "learning_rate": 0.00019994654693179388,
      "loss": 4.0134,
      "step": 10
    },
    {
      "epoch": 0.0005878109386271942,
      "grad_norm": 14.1957368850708,
      "learning_rate": 0.0001999358563181527,
      "loss": 3.7564,
      "step": 11
    },
    {
      "epoch": 0.0006412482966842119,
      "grad_norm": 13.329666137695312,
      "learning_rate": 0.00019992516570451144,
      "loss": 3.9015,
      "step": 12
    },
    {
      "epoch": 0.0006946856547412296,
      "grad_norm": 22.324115753173828,
      "learning_rate": 0.00019991447509087022,
      "loss": 3.82,
      "step": 13
    },
    {
      "epoch": 0.0007481230127982472,
      "grad_norm": 11.132011413574219,
      "learning_rate": 0.000199903784477229,
      "loss": 3.1148,
      "step": 14
    },
    {
      "epoch": 0.0008015603708552649,
      "grad_norm": 13.737192153930664,
      "learning_rate": 0.00019989309386358778,
      "loss": 3.2672,
      "step": 15
    },
    {
      "epoch": 0.0008549977289122826,
      "grad_norm": 8.938203811645508,
      "learning_rate": 0.00019988240324994656,
      "loss": 3.1601,
      "step": 16
    },
    {
      "epoch": 0.0009084350869693002,
      "grad_norm": 7.181746959686279,
      "learning_rate": 0.00019987171263630534,
      "loss": 2.8089,
      "step": 17
    },
    {
      "epoch": 0.0009618724450263179,
      "grad_norm": 6.332230567932129,
      "learning_rate": 0.0001998610220226641,
      "loss": 2.9769,
      "step": 18
    },
    {
      "epoch": 0.0010153098030833356,
      "grad_norm": 9.867545127868652,
      "learning_rate": 0.0001998503314090229,
      "loss": 2.9206,
      "step": 19
    },
    {
      "epoch": 0.0010687471611403531,
      "grad_norm": 4.944268703460693,
      "learning_rate": 0.00019983964079538168,
      "loss": 2.878,
      "step": 20
    },
    {
      "epoch": 0.0011221845191973709,
      "grad_norm": 3.3289639949798584,
      "learning_rate": 0.00019982895018174043,
      "loss": 2.6061,
      "step": 21
    },
    {
      "epoch": 0.0011756218772543884,
      "grad_norm": 3.9966819286346436,
      "learning_rate": 0.0001998182595680992,
      "loss": 2.4531,
      "step": 22
    },
    {
      "epoch": 0.0012290592353114062,
      "grad_norm": 3.5843348503112793,
      "learning_rate": 0.000199807568954458,
      "loss": 2.5386,
      "step": 23
    },
    {
      "epoch": 0.0012824965933684238,
      "grad_norm": 3.203960418701172,
      "learning_rate": 0.00019979687834081677,
      "loss": 2.5833,
      "step": 24
    },
    {
      "epoch": 0.0013359339514254416,
      "grad_norm": 4.620530128479004,
      "learning_rate": 0.00019978618772717555,
      "loss": 2.7913,
      "step": 25
    },
    {
      "epoch": 0.0013893713094824591,
      "grad_norm": 3.376312732696533,
      "learning_rate": 0.00019977549711353433,
      "loss": 2.7973,
      "step": 26
    },
    {
      "epoch": 0.0014428086675394769,
      "grad_norm": 3.6215131282806396,
      "learning_rate": 0.0001997648064998931,
      "loss": 2.6454,
      "step": 27
    },
    {
      "epoch": 0.0014962460255964944,
      "grad_norm": 3.8309831619262695,
      "learning_rate": 0.0001997541158862519,
      "loss": 2.6092,
      "step": 28
    },
    {
      "epoch": 0.0015496833836535122,
      "grad_norm": 4.693153381347656,
      "learning_rate": 0.00019974342527261064,
      "loss": 2.5944,
      "step": 29
    },
    {
      "epoch": 0.0016031207417105298,
      "grad_norm": 3.486189365386963,
      "learning_rate": 0.00019973273465896945,
      "loss": 2.4438,
      "step": 30
    },
    {
      "epoch": 0.0016565580997675476,
      "grad_norm": 3.6962625980377197,
      "learning_rate": 0.00019972204404532823,
      "loss": 2.7958,
      "step": 31
    },
    {
      "epoch": 0.0017099954578245651,
      "grad_norm": 2.6351699829101562,
      "learning_rate": 0.00019971135343168698,
      "loss": 2.5024,
      "step": 32
    },
    {
      "epoch": 0.0017634328158815829,
      "grad_norm": 2.681365966796875,
      "learning_rate": 0.00019970066281804576,
      "loss": 2.5983,
      "step": 33
    },
    {
      "epoch": 0.0018168701739386004,
      "grad_norm": 2.860957384109497,
      "learning_rate": 0.00019968997220440457,
      "loss": 2.5358,
      "step": 34
    },
    {
      "epoch": 0.0018703075319956182,
      "grad_norm": 2.709834337234497,
      "learning_rate": 0.00019967928159076332,
      "loss": 2.4489,
      "step": 35
    },
    {
      "epoch": 0.0019237448900526358,
      "grad_norm": 2.413336753845215,
      "learning_rate": 0.0001996685909771221,
      "loss": 2.7474,
      "step": 36
    },
    {
      "epoch": 0.0019771822481096536,
      "grad_norm": 2.816228151321411,
      "learning_rate": 0.00019965790036348088,
      "loss": 2.6311,
      "step": 37
    },
    {
      "epoch": 0.002030619606166671,
      "grad_norm": 1.9335052967071533,
      "learning_rate": 0.00019964720974983966,
      "loss": 2.4506,
      "step": 38
    },
    {
      "epoch": 0.0020840569642236887,
      "grad_norm": 2.1110637187957764,
      "learning_rate": 0.00019963651913619844,
      "loss": 2.4318,
      "step": 39
    },
    {
      "epoch": 0.0021374943222807062,
      "grad_norm": 3.6771485805511475,
      "learning_rate": 0.0001996258285225572,
      "loss": 2.5202,
      "step": 40
    },
    {
      "epoch": 0.0021909316803377242,
      "grad_norm": 2.4947004318237305,
      "learning_rate": 0.00019961513790891597,
      "loss": 2.3713,
      "step": 41
    },
    {
      "epoch": 0.0022443690383947418,
      "grad_norm": 3.3227171897888184,
      "learning_rate": 0.00019960444729527478,
      "loss": 2.4246,
      "step": 42
    },
    {
      "epoch": 0.0022978063964517593,
      "grad_norm": 3.305040121078491,
      "learning_rate": 0.00019959375668163353,
      "loss": 2.3929,
      "step": 43
    },
    {
      "epoch": 0.002351243754508777,
      "grad_norm": 3.740335464477539,
      "learning_rate": 0.0001995830660679923,
      "loss": 2.4188,
      "step": 44
    },
    {
      "epoch": 0.002404681112565795,
      "grad_norm": 2.857349157333374,
      "learning_rate": 0.0001995723754543511,
      "loss": 2.4117,
      "step": 45
    },
    {
      "epoch": 0.0024581184706228124,
      "grad_norm": 3.1033103466033936,
      "learning_rate": 0.00019956168484070987,
      "loss": 2.6588,
      "step": 46
    },
    {
      "epoch": 0.00251155582867983,
      "grad_norm": 3.260343551635742,
      "learning_rate": 0.00019955099422706865,
      "loss": 2.5025,
      "step": 47
    },
    {
      "epoch": 0.0025649931867368476,
      "grad_norm": 3.641688346862793,
      "learning_rate": 0.00019954030361342743,
      "loss": 2.4434,
      "step": 48
    },
    {
      "epoch": 0.0026184305447938656,
      "grad_norm": 2.715123176574707,
      "learning_rate": 0.00019952961299978618,
      "loss": 2.3227,
      "step": 49
    },
    {
      "epoch": 0.002671867902850883,
      "grad_norm": 2.881333112716675,
      "learning_rate": 0.00019951892238614499,
      "loss": 2.3485,
      "step": 50
    },
    {
      "epoch": 0.0027253052609079007,
      "grad_norm": 3.0994696617126465,
      "learning_rate": 0.00019950823177250374,
      "loss": 2.4613,
      "step": 51
    },
    {
      "epoch": 0.0027787426189649182,
      "grad_norm": 2.9128899574279785,
      "learning_rate": 0.00019949754115886252,
      "loss": 2.2512,
      "step": 52
    },
    {
      "epoch": 0.0028321799770219362,
      "grad_norm": 3.4108967781066895,
      "learning_rate": 0.00019948685054522132,
      "loss": 2.7673,
      "step": 53
    },
    {
      "epoch": 0.0028856173350789538,
      "grad_norm": 3.5357625484466553,
      "learning_rate": 0.00019947615993158008,
      "loss": 2.2592,
      "step": 54
    },
    {
      "epoch": 0.0029390546931359713,
      "grad_norm": 2.1669394969940186,
      "learning_rate": 0.00019946546931793886,
      "loss": 2.2161,
      "step": 55
    },
    {
      "epoch": 0.002992492051192989,
      "grad_norm": 2.566709041595459,
      "learning_rate": 0.00019945477870429764,
      "loss": 2.3943,
      "step": 56
    },
    {
      "epoch": 0.003045929409250007,
      "grad_norm": 2.169023036956787,
      "learning_rate": 0.00019944408809065642,
      "loss": 2.4839,
      "step": 57
    },
    {
      "epoch": 0.0030993667673070244,
      "grad_norm": 2.2498955726623535,
      "learning_rate": 0.0001994333974770152,
      "loss": 2.1503,
      "step": 58
    },
    {
      "epoch": 0.003152804125364042,
      "grad_norm": 1.9470359086990356,
      "learning_rate": 0.00019942270686337398,
      "loss": 2.0655,
      "step": 59
    },
    {
      "epoch": 0.0032062414834210596,
      "grad_norm": 1.4817441701889038,
      "learning_rate": 0.00019941201624973273,
      "loss": 2.1665,
      "step": 60
    },
    {
      "epoch": 0.003259678841478077,
      "grad_norm": 2.762702703475952,
      "learning_rate": 0.00019940132563609153,
      "loss": 2.6132,
      "step": 61
    },
    {
      "epoch": 0.003313116199535095,
      "grad_norm": 1.7849444150924683,
      "learning_rate": 0.00019939063502245031,
      "loss": 2.1542,
      "step": 62
    },
    {
      "epoch": 0.0033665535575921127,
      "grad_norm": 2.055856466293335,
      "learning_rate": 0.00019937994440880907,
      "loss": 2.2057,
      "step": 63
    },
    {
      "epoch": 0.0034199909156491302,
      "grad_norm": 2.6720077991485596,
      "learning_rate": 0.00019936925379516785,
      "loss": 2.3751,
      "step": 64
    },
    {
      "epoch": 0.0034734282737061478,
      "grad_norm": 2.4066174030303955,
      "learning_rate": 0.00019935856318152663,
      "loss": 2.3271,
      "step": 65
    },
    {
      "epoch": 0.0035268656317631658,
      "grad_norm": 2.374605178833008,
      "learning_rate": 0.0001993478725678854,
      "loss": 2.3435,
      "step": 66
    },
    {
      "epoch": 0.0035803029898201833,
      "grad_norm": 2.8954312801361084,
      "learning_rate": 0.00019933718195424419,
      "loss": 2.404,
      "step": 67
    },
    {
      "epoch": 0.003633740347877201,
      "grad_norm": 4.533381938934326,
      "learning_rate": 0.00019932649134060294,
      "loss": 2.6031,
      "step": 68
    },
    {
      "epoch": 0.0036871777059342184,
      "grad_norm": 1.7986892461776733,
      "learning_rate": 0.00019931580072696174,
      "loss": 2.1513,
      "step": 69
    },
    {
      "epoch": 0.0037406150639912364,
      "grad_norm": 3.086799383163452,
      "learning_rate": 0.00019930511011332052,
      "loss": 2.695,
      "step": 70
    },
    {
      "epoch": 0.003794052422048254,
      "grad_norm": 2.4031572341918945,
      "learning_rate": 0.00019929441949967928,
      "loss": 2.4092,
      "step": 71
    },
    {
      "epoch": 0.0038474897801052716,
      "grad_norm": 2.7464113235473633,
      "learning_rate": 0.00019928372888603808,
      "loss": 2.2453,
      "step": 72
    },
    {
      "epoch": 0.003900927138162289,
      "grad_norm": 4.422228813171387,
      "learning_rate": 0.00019927303827239686,
      "loss": 2.3859,
      "step": 73
    },
    {
      "epoch": 0.003954364496219307,
      "grad_norm": 2.501262903213501,
      "learning_rate": 0.00019926234765875562,
      "loss": 2.2071,
      "step": 74
    },
    {
      "epoch": 0.004007801854276324,
      "grad_norm": 2.0280261039733887,
      "learning_rate": 0.0001992516570451144,
      "loss": 2.1383,
      "step": 75
    },
    {
      "epoch": 0.004061239212333342,
      "grad_norm": 1.853374719619751,
      "learning_rate": 0.00019924096643147317,
      "loss": 2.4608,
      "step": 76
    },
    {
      "epoch": 0.00411467657039036,
      "grad_norm": 1.887791395187378,
      "learning_rate": 0.00019923027581783195,
      "loss": 2.2432,
      "step": 77
    },
    {
      "epoch": 0.004168113928447377,
      "grad_norm": 2.1457149982452393,
      "learning_rate": 0.00019921958520419073,
      "loss": 2.44,
      "step": 78
    },
    {
      "epoch": 0.004221551286504395,
      "grad_norm": 2.7834243774414062,
      "learning_rate": 0.0001992088945905495,
      "loss": 2.3315,
      "step": 79
    },
    {
      "epoch": 0.0042749886445614125,
      "grad_norm": 2.035062074661255,
      "learning_rate": 0.0001991982039769083,
      "loss": 2.1417,
      "step": 80
    },
    {
      "epoch": 0.0043284260026184304,
      "grad_norm": 2.0203514099121094,
      "learning_rate": 0.00019918751336326707,
      "loss": 2.4737,
      "step": 81
    },
    {
      "epoch": 0.0043818633606754484,
      "grad_norm": 2.110635757446289,
      "learning_rate": 0.00019917682274962583,
      "loss": 2.1977,
      "step": 82
    },
    {
      "epoch": 0.0044353007187324656,
      "grad_norm": 2.089613199234009,
      "learning_rate": 0.0001991661321359846,
      "loss": 2.3423,
      "step": 83
    },
    {
      "epoch": 0.0044887380767894836,
      "grad_norm": 2.3507418632507324,
      "learning_rate": 0.0001991554415223434,
      "loss": 2.4084,
      "step": 84
    },
    {
      "epoch": 0.0045421754348465015,
      "grad_norm": 3.0323617458343506,
      "learning_rate": 0.00019914475090870216,
      "loss": 2.1922,
      "step": 85
    },
    {
      "epoch": 0.004595612792903519,
      "grad_norm": 2.4876773357391357,
      "learning_rate": 0.00019913406029506094,
      "loss": 2.3589,
      "step": 86
    },
    {
      "epoch": 0.004649050150960537,
      "grad_norm": 2.5492403507232666,
      "learning_rate": 0.00019912336968141972,
      "loss": 2.4409,
      "step": 87
    },
    {
      "epoch": 0.004702487509017554,
      "grad_norm": 2.1316730976104736,
      "learning_rate": 0.0001991126790677785,
      "loss": 2.2339,
      "step": 88
    },
    {
      "epoch": 0.004755924867074572,
      "grad_norm": 2.2884395122528076,
      "learning_rate": 0.00019910198845413728,
      "loss": 2.2245,
      "step": 89
    },
    {
      "epoch": 0.00480936222513159,
      "grad_norm": 2.0484306812286377,
      "learning_rate": 0.00019909129784049606,
      "loss": 2.5584,
      "step": 90
    },
    {
      "epoch": 0.004862799583188607,
      "grad_norm": 4.033507823944092,
      "learning_rate": 0.00019908060722685482,
      "loss": 2.5435,
      "step": 91
    },
    {
      "epoch": 0.004916236941245625,
      "grad_norm": 3.3569581508636475,
      "learning_rate": 0.00019906991661321362,
      "loss": 2.7566,
      "step": 92
    },
    {
      "epoch": 0.004969674299302643,
      "grad_norm": 1.7402949333190918,
      "learning_rate": 0.00019905922599957237,
      "loss": 1.9427,
      "step": 93
    },
    {
      "epoch": 0.00502311165735966,
      "grad_norm": 2.9608747959136963,
      "learning_rate": 0.00019904853538593115,
      "loss": 2.3295,
      "step": 94
    },
    {
      "epoch": 0.005076549015416678,
      "grad_norm": 2.370903491973877,
      "learning_rate": 0.00019903784477228996,
      "loss": 2.2999,
      "step": 95
    },
    {
      "epoch": 0.005129986373473695,
      "grad_norm": 2.3415687084198,
      "learning_rate": 0.00019902715415864871,
      "loss": 2.3111,
      "step": 96
    },
    {
      "epoch": 0.005183423731530713,
      "grad_norm": 1.7979177236557007,
      "learning_rate": 0.0001990164635450075,
      "loss": 2.3156,
      "step": 97
    },
    {
      "epoch": 0.005236861089587731,
      "grad_norm": 3.4317069053649902,
      "learning_rate": 0.00019900577293136627,
      "loss": 2.6324,
      "step": 98
    },
    {
      "epoch": 0.005290298447644748,
      "grad_norm": 2.148063898086548,
      "learning_rate": 0.00019899508231772505,
      "loss": 2.3485,
      "step": 99
    },
    {
      "epoch": 0.005343735805701766,
      "grad_norm": 3.534181833267212,
      "learning_rate": 0.00019898439170408383,
      "loss": 2.5574,
      "step": 100
    },
    {
      "epoch": 0.005397173163758783,
      "grad_norm": 2.7186295986175537,
      "learning_rate": 0.0001989737010904426,
      "loss": 2.2784,
      "step": 101
    },
    {
      "epoch": 0.005450610521815801,
      "grad_norm": 1.6162762641906738,
      "learning_rate": 0.00019896301047680136,
      "loss": 2.0946,
      "step": 102
    },
    {
      "epoch": 0.005504047879872819,
      "grad_norm": 1.6976977586746216,
      "learning_rate": 0.00019895231986316017,
      "loss": 2.2792,
      "step": 103
    },
    {
      "epoch": 0.0055574852379298365,
      "grad_norm": 1.8707302808761597,
      "learning_rate": 0.00019894162924951892,
      "loss": 2.3543,
      "step": 104
    },
    {
      "epoch": 0.0056109225959868544,
      "grad_norm": 2.0895769596099854,
      "learning_rate": 0.0001989309386358777,
      "loss": 2.3004,
      "step": 105
    },
    {
      "epoch": 0.0056643599540438724,
      "grad_norm": 2.0797436237335205,
      "learning_rate": 0.00019892024802223648,
      "loss": 2.2402,
      "step": 106
    },
    {
      "epoch": 0.0057177973121008896,
      "grad_norm": 2.4910085201263428,
      "learning_rate": 0.00019890955740859526,
      "loss": 2.3601,
      "step": 107
    },
    {
      "epoch": 0.0057712346701579076,
      "grad_norm": 2.357196807861328,
      "learning_rate": 0.00019889886679495404,
      "loss": 2.3339,
      "step": 108
    },
    {
      "epoch": 0.005824672028214925,
      "grad_norm": 1.7829447984695435,
      "learning_rate": 0.00019888817618131282,
      "loss": 2.2579,
      "step": 109
    },
    {
      "epoch": 0.005878109386271943,
      "grad_norm": 1.9549471139907837,
      "learning_rate": 0.00019887748556767157,
      "loss": 2.2803,
      "step": 110
    },
    {
      "epoch": 0.005931546744328961,
      "grad_norm": 2.221635341644287,
      "learning_rate": 0.00019886679495403038,
      "loss": 2.3409,
      "step": 111
    },
    {
      "epoch": 0.005984984102385978,
      "grad_norm": 2.5541090965270996,
      "learning_rate": 0.00019885610434038916,
      "loss": 2.4929,
      "step": 112
    },
    {
      "epoch": 0.006038421460442996,
      "grad_norm": 2.1521177291870117,
      "learning_rate": 0.0001988454137267479,
      "loss": 2.3489,
      "step": 113
    },
    {
      "epoch": 0.006091858818500014,
      "grad_norm": 2.678241014480591,
      "learning_rate": 0.0001988347231131067,
      "loss": 2.2437,
      "step": 114
    },
    {
      "epoch": 0.006145296176557031,
      "grad_norm": 2.028127908706665,
      "learning_rate": 0.00019882403249946547,
      "loss": 2.2929,
      "step": 115
    },
    {
      "epoch": 0.006198733534614049,
      "grad_norm": 2.691897392272949,
      "learning_rate": 0.00019881334188582425,
      "loss": 2.3493,
      "step": 116
    },
    {
      "epoch": 0.006252170892671066,
      "grad_norm": 2.0189437866210938,
      "learning_rate": 0.00019880265127218303,
      "loss": 2.2464,
      "step": 117
    },
    {
      "epoch": 0.006305608250728084,
      "grad_norm": 3.2763943672180176,
      "learning_rate": 0.0001987919606585418,
      "loss": 2.5909,
      "step": 118
    },
    {
      "epoch": 0.006359045608785102,
      "grad_norm": 2.2224647998809814,
      "learning_rate": 0.0001987812700449006,
      "loss": 2.4513,
      "step": 119
    },
    {
      "epoch": 0.006412482966842119,
      "grad_norm": 1.892526388168335,
      "learning_rate": 0.00019877057943125937,
      "loss": 2.3511,
      "step": 120
    },
    {
      "epoch": 0.006465920324899137,
      "grad_norm": 3.0634074211120605,
      "learning_rate": 0.00019875988881761812,
      "loss": 2.5005,
      "step": 121
    },
    {
      "epoch": 0.006519357682956154,
      "grad_norm": 3.1696016788482666,
      "learning_rate": 0.00019874919820397693,
      "loss": 2.5338,
      "step": 122
    },
    {
      "epoch": 0.006572795041013172,
      "grad_norm": 2.0189437866210938,
      "learning_rate": 0.0001987385075903357,
      "loss": 2.2577,
      "step": 123
    },
    {
      "epoch": 0.00662623239907019,
      "grad_norm": 2.5295908451080322,
      "learning_rate": 0.00019872781697669446,
      "loss": 2.4013,
      "step": 124
    },
    {
      "epoch": 0.006679669757127207,
      "grad_norm": 2.1718509197235107,
      "learning_rate": 0.00019871712636305324,
      "loss": 2.1857,
      "step": 125
    },
    {
      "epoch": 0.006733107115184225,
      "grad_norm": 2.6861414909362793,
      "learning_rate": 0.00019870643574941202,
      "loss": 2.6086,
      "step": 126
    },
    {
      "epoch": 0.006786544473241243,
      "grad_norm": 1.8559672832489014,
      "learning_rate": 0.0001986957451357708,
      "loss": 2.2779,
      "step": 127
    },
    {
      "epoch": 0.0068399818312982604,
      "grad_norm": 1.7880313396453857,
      "learning_rate": 0.00019868505452212958,
      "loss": 2.555,
      "step": 128
    },
    {
      "epoch": 0.0068934191893552784,
      "grad_norm": 2.1239988803863525,
      "learning_rate": 0.00019867436390848836,
      "loss": 2.31,
      "step": 129
    },
    {
      "epoch": 0.0069468565474122956,
      "grad_norm": 1.6436840295791626,
      "learning_rate": 0.00019866367329484714,
      "loss": 2.255,
      "step": 130
    },
    {
      "epoch": 0.0070002939054693136,
      "grad_norm": 1.931606411933899,
      "learning_rate": 0.00019865298268120592,
      "loss": 2.3587,
      "step": 131
    },
    {
      "epoch": 0.0070537312635263315,
      "grad_norm": 2.388674259185791,
      "learning_rate": 0.00019864229206756467,
      "loss": 2.4263,
      "step": 132
    },
    {
      "epoch": 0.007107168621583349,
      "grad_norm": 2.4609148502349854,
      "learning_rate": 0.00019863160145392345,
      "loss": 2.0797,
      "step": 133
    },
    {
      "epoch": 0.007160605979640367,
      "grad_norm": 1.9375379085540771,
      "learning_rate": 0.00019862091084028226,
      "loss": 2.4952,
      "step": 134
    },
    {
      "epoch": 0.007214043337697385,
      "grad_norm": 2.3791916370391846,
      "learning_rate": 0.000198610220226641,
      "loss": 2.4797,
      "step": 135
    },
    {
      "epoch": 0.007267480695754402,
      "grad_norm": 2.1189119815826416,
      "learning_rate": 0.0001985995296129998,
      "loss": 2.6165,
      "step": 136
    },
    {
      "epoch": 0.00732091805381142,
      "grad_norm": 2.1463654041290283,
      "learning_rate": 0.00019858883899935857,
      "loss": 2.2578,
      "step": 137
    },
    {
      "epoch": 0.007374355411868437,
      "grad_norm": 1.3656697273254395,
      "learning_rate": 0.00019857814838571735,
      "loss": 2.09,
      "step": 138
    },
    {
      "epoch": 0.007427792769925455,
      "grad_norm": 3.0640625953674316,
      "learning_rate": 0.00019856745777207613,
      "loss": 2.5065,
      "step": 139
    },
    {
      "epoch": 0.007481230127982473,
      "grad_norm": 1.8807659149169922,
      "learning_rate": 0.0001985567671584349,
      "loss": 2.4108,
      "step": 140
    },
    {
      "epoch": 0.00753466748603949,
      "grad_norm": 1.5126312971115112,
      "learning_rate": 0.0001985460765447937,
      "loss": 2.1425,
      "step": 141
    },
    {
      "epoch": 0.007588104844096508,
      "grad_norm": 1.6577357053756714,
      "learning_rate": 0.00019853538593115247,
      "loss": 2.4614,
      "step": 142
    },
    {
      "epoch": 0.007641542202153525,
      "grad_norm": 1.877882480621338,
      "learning_rate": 0.00019852469531751122,
      "loss": 2.2686,
      "step": 143
    },
    {
      "epoch": 0.007694979560210543,
      "grad_norm": 1.9157358407974243,
      "learning_rate": 0.00019851400470387,
      "loss": 2.0299,
      "step": 144
    },
    {
      "epoch": 0.007748416918267561,
      "grad_norm": 1.752771258354187,
      "learning_rate": 0.0001985033140902288,
      "loss": 2.3542,
      "step": 145
    },
    {
      "epoch": 0.007801854276324578,
      "grad_norm": 2.1627349853515625,
      "learning_rate": 0.00019849262347658756,
      "loss": 2.2514,
      "step": 146
    },
    {
      "epoch": 0.007855291634381596,
      "grad_norm": 1.1296221017837524,
      "learning_rate": 0.00019848193286294634,
      "loss": 2.0682,
      "step": 147
    },
    {
      "epoch": 0.007908728992438614,
      "grad_norm": 1.6073658466339111,
      "learning_rate": 0.00019847124224930512,
      "loss": 2.1776,
      "step": 148
    },
    {
      "epoch": 0.007962166350495632,
      "grad_norm": 1.6716794967651367,
      "learning_rate": 0.0001984605516356639,
      "loss": 2.4486,
      "step": 149
    },
    {
      "epoch": 0.008015603708552648,
      "grad_norm": 1.9482555389404297,
      "learning_rate": 0.00019844986102202268,
      "loss": 2.3839,
      "step": 150
    },
    {
      "epoch": 0.008069041066609666,
      "grad_norm": 1.4163340330123901,
      "learning_rate": 0.00019843917040838146,
      "loss": 2.1088,
      "step": 151
    },
    {
      "epoch": 0.008122478424666684,
      "grad_norm": 1.71939218044281,
      "learning_rate": 0.0001984284797947402,
      "loss": 2.2511,
      "step": 152
    },
    {
      "epoch": 0.008175915782723702,
      "grad_norm": 1.5992496013641357,
      "learning_rate": 0.00019841778918109902,
      "loss": 2.4163,
      "step": 153
    },
    {
      "epoch": 0.00822935314078072,
      "grad_norm": 1.8481218814849854,
      "learning_rate": 0.00019840709856745777,
      "loss": 2.271,
      "step": 154
    },
    {
      "epoch": 0.008282790498837737,
      "grad_norm": 1.4616034030914307,
      "learning_rate": 0.00019839640795381655,
      "loss": 2.1638,
      "step": 155
    },
    {
      "epoch": 0.008336227856894755,
      "grad_norm": 2.3104922771453857,
      "learning_rate": 0.00019838571734017533,
      "loss": 2.3733,
      "step": 156
    },
    {
      "epoch": 0.008389665214951773,
      "grad_norm": 1.8859935998916626,
      "learning_rate": 0.0001983750267265341,
      "loss": 2.0947,
      "step": 157
    },
    {
      "epoch": 0.00844310257300879,
      "grad_norm": 1.828004002571106,
      "learning_rate": 0.0001983643361128929,
      "loss": 2.0471,
      "step": 158
    },
    {
      "epoch": 0.008496539931065809,
      "grad_norm": 2.4341464042663574,
      "learning_rate": 0.00019835364549925167,
      "loss": 2.2798,
      "step": 159
    },
    {
      "epoch": 0.008549977289122825,
      "grad_norm": 2.6948931217193604,
      "learning_rate": 0.00019834295488561042,
      "loss": 2.4895,
      "step": 160
    },
    {
      "epoch": 0.008603414647179843,
      "grad_norm": 1.510308027267456,
      "learning_rate": 0.00019833226427196923,
      "loss": 2.2279,
      "step": 161
    },
    {
      "epoch": 0.008656852005236861,
      "grad_norm": 1.9699573516845703,
      "learning_rate": 0.000198321573658328,
      "loss": 2.2115,
      "step": 162
    },
    {
      "epoch": 0.008710289363293879,
      "grad_norm": 1.791363000869751,
      "learning_rate": 0.00019831088304468676,
      "loss": 2.0016,
      "step": 163
    },
    {
      "epoch": 0.008763726721350897,
      "grad_norm": 2.740567684173584,
      "learning_rate": 0.00019830019243104557,
      "loss": 2.2513,
      "step": 164
    },
    {
      "epoch": 0.008817164079407915,
      "grad_norm": 1.9829981327056885,
      "learning_rate": 0.00019828950181740432,
      "loss": 2.2649,
      "step": 165
    },
    {
      "epoch": 0.008870601437464931,
      "grad_norm": 2.47855544090271,
      "learning_rate": 0.0001982788112037631,
      "loss": 2.4758,
      "step": 166
    },
    {
      "epoch": 0.008924038795521949,
      "grad_norm": 2.771904706954956,
      "learning_rate": 0.00019826812059012188,
      "loss": 2.3403,
      "step": 167
    },
    {
      "epoch": 0.008977476153578967,
      "grad_norm": 1.563524842262268,
      "learning_rate": 0.00019825742997648066,
      "loss": 2.219,
      "step": 168
    },
    {
      "epoch": 0.009030913511635985,
      "grad_norm": 2.464757204055786,
      "learning_rate": 0.00019824673936283944,
      "loss": 2.3529,
      "step": 169
    },
    {
      "epoch": 0.009084350869693003,
      "grad_norm": 2.588552474975586,
      "learning_rate": 0.00019823604874919822,
      "loss": 2.5771,
      "step": 170
    },
    {
      "epoch": 0.00913778822775002,
      "grad_norm": 2.417567253112793,
      "learning_rate": 0.00019822535813555697,
      "loss": 2.0823,
      "step": 171
    },
    {
      "epoch": 0.009191225585807037,
      "grad_norm": 1.8058382272720337,
      "learning_rate": 0.00019821466752191578,
      "loss": 2.3618,
      "step": 172
    },
    {
      "epoch": 0.009244662943864055,
      "grad_norm": 2.4998278617858887,
      "learning_rate": 0.00019820397690827456,
      "loss": 2.6167,
      "step": 173
    },
    {
      "epoch": 0.009298100301921073,
      "grad_norm": 2.038851261138916,
      "learning_rate": 0.0001981932862946333,
      "loss": 2.3543,
      "step": 174
    },
    {
      "epoch": 0.009351537659978091,
      "grad_norm": 2.487422466278076,
      "learning_rate": 0.0001981825956809921,
      "loss": 2.068,
      "step": 175
    },
    {
      "epoch": 0.009404975018035108,
      "grad_norm": 1.7561699151992798,
      "learning_rate": 0.0001981719050673509,
      "loss": 2.0829,
      "step": 176
    },
    {
      "epoch": 0.009458412376092126,
      "grad_norm": 1.5048489570617676,
      "learning_rate": 0.00019816121445370965,
      "loss": 2.1128,
      "step": 177
    },
    {
      "epoch": 0.009511849734149144,
      "grad_norm": 1.6518044471740723,
      "learning_rate": 0.00019815052384006843,
      "loss": 2.2952,
      "step": 178
    },
    {
      "epoch": 0.009565287092206162,
      "grad_norm": 1.6791489124298096,
      "learning_rate": 0.0001981398332264272,
      "loss": 2.2058,
      "step": 179
    },
    {
      "epoch": 0.00961872445026318,
      "grad_norm": 1.806159496307373,
      "learning_rate": 0.000198129142612786,
      "loss": 2.2239,
      "step": 180
    },
    {
      "epoch": 0.009672161808320196,
      "grad_norm": 2.2939858436584473,
      "learning_rate": 0.00019811845199914477,
      "loss": 2.2677,
      "step": 181
    },
    {
      "epoch": 0.009725599166377214,
      "grad_norm": 1.5588476657867432,
      "learning_rate": 0.00019810776138550352,
      "loss": 2.2894,
      "step": 182
    },
    {
      "epoch": 0.009779036524434232,
      "grad_norm": 2.135014295578003,
      "learning_rate": 0.00019809707077186233,
      "loss": 2.3482,
      "step": 183
    },
    {
      "epoch": 0.00983247388249125,
      "grad_norm": 3.130028486251831,
      "learning_rate": 0.0001980863801582211,
      "loss": 2.4595,
      "step": 184
    },
    {
      "epoch": 0.009885911240548268,
      "grad_norm": 2.3617007732391357,
      "learning_rate": 0.00019807568954457986,
      "loss": 2.2736,
      "step": 185
    },
    {
      "epoch": 0.009939348598605286,
      "grad_norm": 1.951005220413208,
      "learning_rate": 0.00019806499893093864,
      "loss": 2.3125,
      "step": 186
    },
    {
      "epoch": 0.009992785956662302,
      "grad_norm": 1.8079955577850342,
      "learning_rate": 0.00019805430831729744,
      "loss": 2.2513,
      "step": 187
    },
    {
      "epoch": 0.01004622331471932,
      "grad_norm": 1.51261305809021,
      "learning_rate": 0.0001980436177036562,
      "loss": 2.1954,
      "step": 188
    },
    {
      "epoch": 0.010099660672776338,
      "grad_norm": 1.9357469081878662,
      "learning_rate": 0.00019803292709001498,
      "loss": 2.267,
      "step": 189
    },
    {
      "epoch": 0.010153098030833356,
      "grad_norm": 1.9593473672866821,
      "learning_rate": 0.00019802223647637376,
      "loss": 2.2414,
      "step": 190
    },
    {
      "epoch": 0.010206535388890374,
      "grad_norm": 1.5358587503433228,
      "learning_rate": 0.00019801154586273254,
      "loss": 2.2082,
      "step": 191
    },
    {
      "epoch": 0.01025997274694739,
      "grad_norm": 1.7860612869262695,
      "learning_rate": 0.00019800085524909132,
      "loss": 2.2528,
      "step": 192
    },
    {
      "epoch": 0.010313410105004408,
      "grad_norm": 1.4999393224716187,
      "learning_rate": 0.00019799016463545007,
      "loss": 2.1751,
      "step": 193
    },
    {
      "epoch": 0.010366847463061426,
      "grad_norm": 1.9045521020889282,
      "learning_rate": 0.00019797947402180885,
      "loss": 1.9488,
      "step": 194
    },
    {
      "epoch": 0.010420284821118444,
      "grad_norm": 1.627297282218933,
      "learning_rate": 0.00019796878340816766,
      "loss": 2.211,
      "step": 195
    },
    {
      "epoch": 0.010473722179175462,
      "grad_norm": 1.5917466878890991,
      "learning_rate": 0.0001979580927945264,
      "loss": 2.1858,
      "step": 196
    },
    {
      "epoch": 0.010527159537232478,
      "grad_norm": 1.4751759767532349,
      "learning_rate": 0.0001979474021808852,
      "loss": 2.0659,
      "step": 197
    },
    {
      "epoch": 0.010580596895289496,
      "grad_norm": 1.794964075088501,
      "learning_rate": 0.00019793671156724397,
      "loss": 2.1209,
      "step": 198
    },
    {
      "epoch": 0.010634034253346514,
      "grad_norm": 1.8415664434432983,
      "learning_rate": 0.00019792602095360275,
      "loss": 2.3496,
      "step": 199
    },
    {
      "epoch": 0.010687471611403532,
      "grad_norm": 1.5526354312896729,
      "learning_rate": 0.00019791533033996153,
      "loss": 2.0418,
      "step": 200
    },
    {
      "epoch": 0.01074090896946055,
      "grad_norm": 1.6219415664672852,
      "learning_rate": 0.0001979046397263203,
      "loss": 2.1485,
      "step": 201
    },
    {
      "epoch": 0.010794346327517567,
      "grad_norm": 2.909191846847534,
      "learning_rate": 0.00019789394911267906,
      "loss": 2.3218,
      "step": 202
    },
    {
      "epoch": 0.010847783685574585,
      "grad_norm": 1.991490364074707,
      "learning_rate": 0.00019788325849903787,
      "loss": 2.1348,
      "step": 203
    },
    {
      "epoch": 0.010901221043631603,
      "grad_norm": 1.8083890676498413,
      "learning_rate": 0.00019787256788539664,
      "loss": 2.2096,
      "step": 204
    },
    {
      "epoch": 0.01095465840168862,
      "grad_norm": 2.298269510269165,
      "learning_rate": 0.0001978618772717554,
      "loss": 2.0963,
      "step": 205
    },
    {
      "epoch": 0.011008095759745639,
      "grad_norm": 2.2239108085632324,
      "learning_rate": 0.0001978511866581142,
      "loss": 2.2159,
      "step": 206
    },
    {
      "epoch": 0.011061533117802657,
      "grad_norm": 2.3935747146606445,
      "learning_rate": 0.00019784049604447296,
      "loss": 2.4161,
      "step": 207
    },
    {
      "epoch": 0.011114970475859673,
      "grad_norm": 1.877626895904541,
      "learning_rate": 0.00019782980543083174,
      "loss": 2.3907,
      "step": 208
    },
    {
      "epoch": 0.011168407833916691,
      "grad_norm": 2.1585710048675537,
      "learning_rate": 0.00019781911481719052,
      "loss": 2.2906,
      "step": 209
    },
    {
      "epoch": 0.011221845191973709,
      "grad_norm": 2.2746617794036865,
      "learning_rate": 0.0001978084242035493,
      "loss": 2.5741,
      "step": 210
    },
    {
      "epoch": 0.011275282550030727,
      "grad_norm": 2.2482640743255615,
      "learning_rate": 0.00019779773358990808,
      "loss": 2.2525,
      "step": 211
    },
    {
      "epoch": 0.011328719908087745,
      "grad_norm": 2.741502285003662,
      "learning_rate": 0.00019778704297626685,
      "loss": 2.3312,
      "step": 212
    },
    {
      "epoch": 0.011382157266144761,
      "grad_norm": 1.865246295928955,
      "learning_rate": 0.0001977763523626256,
      "loss": 2.2199,
      "step": 213
    },
    {
      "epoch": 0.011435594624201779,
      "grad_norm": 1.6434811353683472,
      "learning_rate": 0.00019776566174898441,
      "loss": 2.0947,
      "step": 214
    },
    {
      "epoch": 0.011489031982258797,
      "grad_norm": 2.6523470878601074,
      "learning_rate": 0.0001977549711353432,
      "loss": 2.1767,
      "step": 215
    },
    {
      "epoch": 0.011542469340315815,
      "grad_norm": 1.6652511358261108,
      "learning_rate": 0.00019774428052170195,
      "loss": 2.2073,
      "step": 216
    },
    {
      "epoch": 0.011595906698372833,
      "grad_norm": 2.195754051208496,
      "learning_rate": 0.00019773358990806073,
      "loss": 2.3311,
      "step": 217
    },
    {
      "epoch": 0.01164934405642985,
      "grad_norm": 3.438298463821411,
      "learning_rate": 0.0001977228992944195,
      "loss": 2.5587,
      "step": 218
    },
    {
      "epoch": 0.011702781414486867,
      "grad_norm": 1.3485708236694336,
      "learning_rate": 0.00019771220868077829,
      "loss": 2.1136,
      "step": 219
    },
    {
      "epoch": 0.011756218772543885,
      "grad_norm": 3.2579479217529297,
      "learning_rate": 0.00019770151806713706,
      "loss": 2.2569,
      "step": 220
    },
    {
      "epoch": 0.011809656130600903,
      "grad_norm": 2.0421736240386963,
      "learning_rate": 0.00019769082745349582,
      "loss": 2.2771,
      "step": 221
    },
    {
      "epoch": 0.011863093488657921,
      "grad_norm": 2.9052767753601074,
      "learning_rate": 0.00019768013683985462,
      "loss": 2.5058,
      "step": 222
    },
    {
      "epoch": 0.011916530846714938,
      "grad_norm": 1.5456726551055908,
      "learning_rate": 0.0001976694462262134,
      "loss": 2.2721,
      "step": 223
    },
    {
      "epoch": 0.011969968204771956,
      "grad_norm": 1.706896185874939,
      "learning_rate": 0.00019765875561257216,
      "loss": 2.2572,
      "step": 224
    },
    {
      "epoch": 0.012023405562828974,
      "grad_norm": 1.8504962921142578,
      "learning_rate": 0.00019764806499893094,
      "loss": 2.1956,
      "step": 225
    },
    {
      "epoch": 0.012076842920885992,
      "grad_norm": 1.4890812635421753,
      "learning_rate": 0.00019763737438528974,
      "loss": 2.2131,
      "step": 226
    },
    {
      "epoch": 0.01213028027894301,
      "grad_norm": 1.6949875354766846,
      "learning_rate": 0.0001976266837716485,
      "loss": 2.1467,
      "step": 227
    },
    {
      "epoch": 0.012183717637000028,
      "grad_norm": 1.5813792943954468,
      "learning_rate": 0.00019761599315800728,
      "loss": 1.8115,
      "step": 228
    },
    {
      "epoch": 0.012237154995057044,
      "grad_norm": 2.0067131519317627,
      "learning_rate": 0.00019760530254436605,
      "loss": 2.3508,
      "step": 229
    },
    {
      "epoch": 0.012290592353114062,
      "grad_norm": 1.1246662139892578,
      "learning_rate": 0.00019759461193072483,
      "loss": 2.1677,
      "step": 230
    },
    {
      "epoch": 0.01234402971117108,
      "grad_norm": 1.7800709009170532,
      "learning_rate": 0.00019758392131708361,
      "loss": 2.3487,
      "step": 231
    },
    {
      "epoch": 0.012397467069228098,
      "grad_norm": 1.4524718523025513,
      "learning_rate": 0.0001975732307034424,
      "loss": 2.1984,
      "step": 232
    },
    {
      "epoch": 0.012450904427285116,
      "grad_norm": 2.1085140705108643,
      "learning_rate": 0.00019756254008980117,
      "loss": 2.4997,
      "step": 233
    },
    {
      "epoch": 0.012504341785342132,
      "grad_norm": 1.248300313949585,
      "learning_rate": 0.00019755184947615995,
      "loss": 2.2101,
      "step": 234
    },
    {
      "epoch": 0.01255777914339915,
      "grad_norm": 2.640291929244995,
      "learning_rate": 0.0001975411588625187,
      "loss": 2.6063,
      "step": 235
    },
    {
      "epoch": 0.012611216501456168,
      "grad_norm": 1.8572418689727783,
      "learning_rate": 0.00019753046824887749,
      "loss": 2.2879,
      "step": 236
    },
    {
      "epoch": 0.012664653859513186,
      "grad_norm": 1.9282485246658325,
      "learning_rate": 0.0001975197776352363,
      "loss": 2.0593,
      "step": 237
    },
    {
      "epoch": 0.012718091217570204,
      "grad_norm": 1.3157105445861816,
      "learning_rate": 0.00019750908702159504,
      "loss": 2.2296,
      "step": 238
    },
    {
      "epoch": 0.01277152857562722,
      "grad_norm": 1.6582422256469727,
      "learning_rate": 0.00019749839640795382,
      "loss": 2.407,
      "step": 239
    },
    {
      "epoch": 0.012824965933684238,
      "grad_norm": 1.5244755744934082,
      "learning_rate": 0.0001974877057943126,
      "loss": 2.2811,
      "step": 240
    },
    {
      "epoch": 0.012878403291741256,
      "grad_norm": 1.8624200820922852,
      "learning_rate": 0.00019747701518067138,
      "loss": 2.436,
      "step": 241
    },
    {
      "epoch": 0.012931840649798274,
      "grad_norm": 2.400510787963867,
      "learning_rate": 0.00019746632456703016,
      "loss": 2.4986,
      "step": 242
    },
    {
      "epoch": 0.012985278007855292,
      "grad_norm": 2.416327953338623,
      "learning_rate": 0.00019745563395338894,
      "loss": 2.3875,
      "step": 243
    },
    {
      "epoch": 0.013038715365912308,
      "grad_norm": 1.6085342168807983,
      "learning_rate": 0.0001974449433397477,
      "loss": 2.2275,
      "step": 244
    },
    {
      "epoch": 0.013092152723969326,
      "grad_norm": 1.516238808631897,
      "learning_rate": 0.0001974342527261065,
      "loss": 2.1047,
      "step": 245
    },
    {
      "epoch": 0.013145590082026344,
      "grad_norm": 1.732181191444397,
      "learning_rate": 0.00019742356211246525,
      "loss": 2.361,
      "step": 246
    },
    {
      "epoch": 0.013199027440083362,
      "grad_norm": 1.6561458110809326,
      "learning_rate": 0.00019741287149882403,
      "loss": 2.3346,
      "step": 247
    },
    {
      "epoch": 0.01325246479814038,
      "grad_norm": 1.286171317100525,
      "learning_rate": 0.00019740218088518281,
      "loss": 1.8738,
      "step": 248
    },
    {
      "epoch": 0.013305902156197398,
      "grad_norm": 1.586031198501587,
      "learning_rate": 0.0001973914902715416,
      "loss": 2.318,
      "step": 249
    },
    {
      "epoch": 0.013359339514254415,
      "grad_norm": 1.7317025661468506,
      "learning_rate": 0.00019738079965790037,
      "loss": 2.1821,
      "step": 250
    },
    {
      "epoch": 0.013412776872311433,
      "grad_norm": 6.731567859649658,
      "learning_rate": 0.00019737010904425915,
      "loss": 2.3858,
      "step": 251
    },
    {
      "epoch": 0.01346621423036845,
      "grad_norm": 1.4971562623977661,
      "learning_rate": 0.00019735941843061793,
      "loss": 2.1827,
      "step": 252
    },
    {
      "epoch": 0.013519651588425469,
      "grad_norm": 1.8602439165115356,
      "learning_rate": 0.0001973487278169767,
      "loss": 2.3221,
      "step": 253
    },
    {
      "epoch": 0.013573088946482487,
      "grad_norm": 2.0484297275543213,
      "learning_rate": 0.0001973380372033355,
      "loss": 2.3549,
      "step": 254
    },
    {
      "epoch": 0.013626526304539503,
      "grad_norm": 2.347074508666992,
      "learning_rate": 0.00019732734658969424,
      "loss": 2.262,
      "step": 255
    },
    {
      "epoch": 0.013679963662596521,
      "grad_norm": 2.5021588802337646,
      "learning_rate": 0.00019731665597605305,
      "loss": 2.3532,
      "step": 256
    },
    {
      "epoch": 0.013733401020653539,
      "grad_norm": 1.3558979034423828,
      "learning_rate": 0.0001973059653624118,
      "loss": 2.2396,
      "step": 257
    },
    {
      "epoch": 0.013786838378710557,
      "grad_norm": 1.9154551029205322,
      "learning_rate": 0.00019729527474877058,
      "loss": 2.228,
      "step": 258
    },
    {
      "epoch": 0.013840275736767575,
      "grad_norm": 3.1598494052886963,
      "learning_rate": 0.00019728458413512936,
      "loss": 2.0794,
      "step": 259
    },
    {
      "epoch": 0.013893713094824591,
      "grad_norm": 1.345278024673462,
      "learning_rate": 0.00019727389352148814,
      "loss": 2.3169,
      "step": 260
    },
    {
      "epoch": 0.013947150452881609,
      "grad_norm": 1.6322556734085083,
      "learning_rate": 0.00019726320290784692,
      "loss": 2.1137,
      "step": 261
    },
    {
      "epoch": 0.014000587810938627,
      "grad_norm": 1.275966763496399,
      "learning_rate": 0.0001972525122942057,
      "loss": 2.0456,
      "step": 262
    },
    {
      "epoch": 0.014054025168995645,
      "grad_norm": 1.6600183248519897,
      "learning_rate": 0.00019724182168056445,
      "loss": 2.2741,
      "step": 263
    },
    {
      "epoch": 0.014107462527052663,
      "grad_norm": 1.759407877922058,
      "learning_rate": 0.00019723113106692326,
      "loss": 2.364,
      "step": 264
    },
    {
      "epoch": 0.01416089988510968,
      "grad_norm": 1.834096074104309,
      "learning_rate": 0.00019722044045328204,
      "loss": 2.5592,
      "step": 265
    },
    {
      "epoch": 0.014214337243166697,
      "grad_norm": 1.7968651056289673,
      "learning_rate": 0.0001972097498396408,
      "loss": 2.3194,
      "step": 266
    },
    {
      "epoch": 0.014267774601223715,
      "grad_norm": 1.9356722831726074,
      "learning_rate": 0.00019719905922599957,
      "loss": 2.3197,
      "step": 267
    },
    {
      "epoch": 0.014321211959280733,
      "grad_norm": 1.4751859903335571,
      "learning_rate": 0.00019718836861235835,
      "loss": 2.3349,
      "step": 268
    },
    {
      "epoch": 0.014374649317337751,
      "grad_norm": 1.7063785791397095,
      "learning_rate": 0.00019717767799871713,
      "loss": 2.2312,
      "step": 269
    },
    {
      "epoch": 0.01442808667539477,
      "grad_norm": 2.111907720565796,
      "learning_rate": 0.0001971669873850759,
      "loss": 2.2744,
      "step": 270
    },
    {
      "epoch": 0.014481524033451786,
      "grad_norm": 1.9191499948501587,
      "learning_rate": 0.0001971562967714347,
      "loss": 2.2765,
      "step": 271
    },
    {
      "epoch": 0.014534961391508804,
      "grad_norm": 1.8256696462631226,
      "learning_rate": 0.00019714560615779347,
      "loss": 2.2111,
      "step": 272
    },
    {
      "epoch": 0.014588398749565822,
      "grad_norm": 2.2760839462280273,
      "learning_rate": 0.00019713491554415225,
      "loss": 2.3981,
      "step": 273
    },
    {
      "epoch": 0.01464183610762284,
      "grad_norm": 1.6807100772857666,
      "learning_rate": 0.000197124224930511,
      "loss": 2.2342,
      "step": 274
    },
    {
      "epoch": 0.014695273465679858,
      "grad_norm": 2.155566930770874,
      "learning_rate": 0.0001971135343168698,
      "loss": 2.463,
      "step": 275
    },
    {
      "epoch": 0.014748710823736874,
      "grad_norm": 1.4235788583755493,
      "learning_rate": 0.0001971028437032286,
      "loss": 2.3791,
      "step": 276
    },
    {
      "epoch": 0.014802148181793892,
      "grad_norm": 1.179872989654541,
      "learning_rate": 0.00019709215308958734,
      "loss": 2.0417,
      "step": 277
    },
    {
      "epoch": 0.01485558553985091,
      "grad_norm": 1.2439278364181519,
      "learning_rate": 0.00019708146247594612,
      "loss": 2.111,
      "step": 278
    },
    {
      "epoch": 0.014909022897907928,
      "grad_norm": 1.444427728652954,
      "learning_rate": 0.0001970707718623049,
      "loss": 2.2418,
      "step": 279
    },
    {
      "epoch": 0.014962460255964946,
      "grad_norm": 1.4565761089324951,
      "learning_rate": 0.00019706008124866368,
      "loss": 2.3222,
      "step": 280
    },
    {
      "epoch": 0.015015897614021962,
      "grad_norm": 1.575871229171753,
      "learning_rate": 0.00019704939063502246,
      "loss": 2.3932,
      "step": 281
    },
    {
      "epoch": 0.01506933497207898,
      "grad_norm": 1.6027491092681885,
      "learning_rate": 0.00019703870002138124,
      "loss": 2.2566,
      "step": 282
    },
    {
      "epoch": 0.015122772330135998,
      "grad_norm": 1.521506667137146,
      "learning_rate": 0.00019702800940774002,
      "loss": 2.0971,
      "step": 283
    },
    {
      "epoch": 0.015176209688193016,
      "grad_norm": 1.4854158163070679,
      "learning_rate": 0.0001970173187940988,
      "loss": 2.1723,
      "step": 284
    },
    {
      "epoch": 0.015229647046250034,
      "grad_norm": 1.490531325340271,
      "learning_rate": 0.00019700662818045755,
      "loss": 2.0702,
      "step": 285
    },
    {
      "epoch": 0.01528308440430705,
      "grad_norm": 1.8931411504745483,
      "learning_rate": 0.00019699593756681633,
      "loss": 2.4066,
      "step": 286
    },
    {
      "epoch": 0.015336521762364068,
      "grad_norm": 1.7550147771835327,
      "learning_rate": 0.00019698524695317514,
      "loss": 2.1709,
      "step": 287
    },
    {
      "epoch": 0.015389959120421086,
      "grad_norm": 1.12906813621521,
      "learning_rate": 0.0001969745563395339,
      "loss": 2.0291,
      "step": 288
    },
    {
      "epoch": 0.015443396478478104,
      "grad_norm": 2.241873025894165,
      "learning_rate": 0.00019696386572589267,
      "loss": 2.4844,
      "step": 289
    },
    {
      "epoch": 0.015496833836535122,
      "grad_norm": 2.7168161869049072,
      "learning_rate": 0.00019695317511225145,
      "loss": 2.4032,
      "step": 290
    },
    {
      "epoch": 0.01555027119459214,
      "grad_norm": 2.783116340637207,
      "learning_rate": 0.00019694248449861023,
      "loss": 2.4211,
      "step": 291
    },
    {
      "epoch": 0.015603708552649156,
      "grad_norm": 2.0058441162109375,
      "learning_rate": 0.000196931793884969,
      "loss": 2.4713,
      "step": 292
    },
    {
      "epoch": 0.015657145910706174,
      "grad_norm": 2.6176083087921143,
      "learning_rate": 0.0001969211032713278,
      "loss": 2.3305,
      "step": 293
    },
    {
      "epoch": 0.015710583268763192,
      "grad_norm": 1.164573073387146,
      "learning_rate": 0.00019691041265768657,
      "loss": 2.1476,
      "step": 294
    },
    {
      "epoch": 0.01576402062682021,
      "grad_norm": 1.9512273073196411,
      "learning_rate": 0.00019689972204404535,
      "loss": 2.2602,
      "step": 295
    },
    {
      "epoch": 0.01581745798487723,
      "grad_norm": 1.9704699516296387,
      "learning_rate": 0.0001968890314304041,
      "loss": 2.3751,
      "step": 296
    },
    {
      "epoch": 0.015870895342934246,
      "grad_norm": 1.685815691947937,
      "learning_rate": 0.00019687834081676288,
      "loss": 2.2835,
      "step": 297
    },
    {
      "epoch": 0.015924332700991264,
      "grad_norm": 1.6485916376113892,
      "learning_rate": 0.0001968676502031217,
      "loss": 2.3439,
      "step": 298
    },
    {
      "epoch": 0.01597777005904828,
      "grad_norm": 1.8568354845046997,
      "learning_rate": 0.00019685695958948044,
      "loss": 2.2396,
      "step": 299
    },
    {
      "epoch": 0.016031207417105297,
      "grad_norm": 1.53928804397583,
      "learning_rate": 0.00019684626897583922,
      "loss": 2.3245,
      "step": 300
    },
    {
      "epoch": 0.016084644775162315,
      "grad_norm": 2.2608256340026855,
      "learning_rate": 0.000196835578362198,
      "loss": 2.3091,
      "step": 301
    },
    {
      "epoch": 0.016138082133219333,
      "grad_norm": 1.516178011894226,
      "learning_rate": 0.00019682488774855678,
      "loss": 2.2187,
      "step": 302
    },
    {
      "epoch": 0.01619151949127635,
      "grad_norm": 2.634747266769409,
      "learning_rate": 0.00019681419713491556,
      "loss": 2.4919,
      "step": 303
    },
    {
      "epoch": 0.01624495684933337,
      "grad_norm": 1.651749849319458,
      "learning_rate": 0.00019680350652127434,
      "loss": 2.3897,
      "step": 304
    },
    {
      "epoch": 0.016298394207390387,
      "grad_norm": 1.9723024368286133,
      "learning_rate": 0.0001967928159076331,
      "loss": 2.1688,
      "step": 305
    },
    {
      "epoch": 0.016351831565447405,
      "grad_norm": 2.0453081130981445,
      "learning_rate": 0.0001967821252939919,
      "loss": 2.4991,
      "step": 306
    },
    {
      "epoch": 0.016405268923504423,
      "grad_norm": 1.9980307817459106,
      "learning_rate": 0.00019677143468035065,
      "loss": 2.2217,
      "step": 307
    },
    {
      "epoch": 0.01645870628156144,
      "grad_norm": 1.6497584581375122,
      "learning_rate": 0.00019676074406670943,
      "loss": 2.3419,
      "step": 308
    },
    {
      "epoch": 0.01651214363961846,
      "grad_norm": 1.9262864589691162,
      "learning_rate": 0.0001967500534530682,
      "loss": 2.2534,
      "step": 309
    },
    {
      "epoch": 0.016565580997675473,
      "grad_norm": 2.180208206176758,
      "learning_rate": 0.000196739362839427,
      "loss": 2.314,
      "step": 310
    },
    {
      "epoch": 0.01661901835573249,
      "grad_norm": 2.1183111667633057,
      "learning_rate": 0.00019672867222578577,
      "loss": 2.1948,
      "step": 311
    },
    {
      "epoch": 0.01667245571378951,
      "grad_norm": 1.4080612659454346,
      "learning_rate": 0.00019671798161214455,
      "loss": 2.1517,
      "step": 312
    },
    {
      "epoch": 0.016725893071846527,
      "grad_norm": 1.6375370025634766,
      "learning_rate": 0.0001967072909985033,
      "loss": 2.1603,
      "step": 313
    },
    {
      "epoch": 0.016779330429903545,
      "grad_norm": 1.4604445695877075,
      "learning_rate": 0.0001966966003848621,
      "loss": 2.2201,
      "step": 314
    },
    {
      "epoch": 0.016832767787960563,
      "grad_norm": 1.3064701557159424,
      "learning_rate": 0.0001966859097712209,
      "loss": 2.1033,
      "step": 315
    },
    {
      "epoch": 0.01688620514601758,
      "grad_norm": 1.5236225128173828,
      "learning_rate": 0.00019667521915757964,
      "loss": 2.0369,
      "step": 316
    },
    {
      "epoch": 0.0169396425040746,
      "grad_norm": 1.3869305849075317,
      "learning_rate": 0.00019666452854393845,
      "loss": 2.1949,
      "step": 317
    },
    {
      "epoch": 0.016993079862131617,
      "grad_norm": 1.2172577381134033,
      "learning_rate": 0.00019665383793029723,
      "loss": 2.0486,
      "step": 318
    },
    {
      "epoch": 0.017046517220188635,
      "grad_norm": 1.3590731620788574,
      "learning_rate": 0.00019664314731665598,
      "loss": 1.8919,
      "step": 319
    },
    {
      "epoch": 0.01709995457824565,
      "grad_norm": 1.4523439407348633,
      "learning_rate": 0.00019663245670301476,
      "loss": 2.1908,
      "step": 320
    },
    {
      "epoch": 0.017153391936302668,
      "grad_norm": 2.2986013889312744,
      "learning_rate": 0.00019662176608937354,
      "loss": 2.3291,
      "step": 321
    },
    {
      "epoch": 0.017206829294359686,
      "grad_norm": 1.6117777824401855,
      "learning_rate": 0.00019661107547573232,
      "loss": 2.2605,
      "step": 322
    },
    {
      "epoch": 0.017260266652416704,
      "grad_norm": 1.4682368040084839,
      "learning_rate": 0.0001966003848620911,
      "loss": 2.2054,
      "step": 323
    },
    {
      "epoch": 0.017313704010473722,
      "grad_norm": 1.5568195581436157,
      "learning_rate": 0.00019658969424844985,
      "loss": 2.1177,
      "step": 324
    },
    {
      "epoch": 0.01736714136853074,
      "grad_norm": 2.0581018924713135,
      "learning_rate": 0.00019657900363480866,
      "loss": 2.3674,
      "step": 325
    },
    {
      "epoch": 0.017420578726587758,
      "grad_norm": 1.8323602676391602,
      "learning_rate": 0.00019656831302116744,
      "loss": 2.1817,
      "step": 326
    },
    {
      "epoch": 0.017474016084644776,
      "grad_norm": 1.8304784297943115,
      "learning_rate": 0.0001965576224075262,
      "loss": 2.3724,
      "step": 327
    },
    {
      "epoch": 0.017527453442701794,
      "grad_norm": 1.864850401878357,
      "learning_rate": 0.00019654693179388497,
      "loss": 2.5782,
      "step": 328
    },
    {
      "epoch": 0.017580890800758812,
      "grad_norm": 1.7029331922531128,
      "learning_rate": 0.00019653624118024378,
      "loss": 2.1581,
      "step": 329
    },
    {
      "epoch": 0.01763432815881583,
      "grad_norm": 2.1743996143341064,
      "learning_rate": 0.00019652555056660253,
      "loss": 2.1713,
      "step": 330
    },
    {
      "epoch": 0.017687765516872844,
      "grad_norm": 3.396580457687378,
      "learning_rate": 0.0001965148599529613,
      "loss": 2.3627,
      "step": 331
    },
    {
      "epoch": 0.017741202874929862,
      "grad_norm": 2.4321463108062744,
      "learning_rate": 0.0001965041693393201,
      "loss": 2.2084,
      "step": 332
    },
    {
      "epoch": 0.01779464023298688,
      "grad_norm": 2.184349298477173,
      "learning_rate": 0.00019649347872567887,
      "loss": 1.9508,
      "step": 333
    },
    {
      "epoch": 0.017848077591043898,
      "grad_norm": 2.2527313232421875,
      "learning_rate": 0.00019648278811203765,
      "loss": 2.1596,
      "step": 334
    },
    {
      "epoch": 0.017901514949100916,
      "grad_norm": 1.2998868227005005,
      "learning_rate": 0.0001964720974983964,
      "loss": 1.9677,
      "step": 335
    },
    {
      "epoch": 0.017954952307157934,
      "grad_norm": 1.8274343013763428,
      "learning_rate": 0.00019646140688475518,
      "loss": 2.3404,
      "step": 336
    },
    {
      "epoch": 0.018008389665214952,
      "grad_norm": 1.9581352472305298,
      "learning_rate": 0.00019645071627111399,
      "loss": 2.0904,
      "step": 337
    },
    {
      "epoch": 0.01806182702327197,
      "grad_norm": 2.3672029972076416,
      "learning_rate": 0.00019644002565747274,
      "loss": 2.2664,
      "step": 338
    },
    {
      "epoch": 0.018115264381328988,
      "grad_norm": 1.6438535451889038,
      "learning_rate": 0.00019642933504383152,
      "loss": 2.2047,
      "step": 339
    },
    {
      "epoch": 0.018168701739386006,
      "grad_norm": 2.1392667293548584,
      "learning_rate": 0.00019641864443019032,
      "loss": 2.3227,
      "step": 340
    },
    {
      "epoch": 0.01822213909744302,
      "grad_norm": 2.389693260192871,
      "learning_rate": 0.00019640795381654908,
      "loss": 2.3894,
      "step": 341
    },
    {
      "epoch": 0.01827557645550004,
      "grad_norm": 1.3576246500015259,
      "learning_rate": 0.00019639726320290786,
      "loss": 1.9348,
      "step": 342
    },
    {
      "epoch": 0.018329013813557057,
      "grad_norm": 2.067734956741333,
      "learning_rate": 0.00019638657258926664,
      "loss": 2.4078,
      "step": 343
    },
    {
      "epoch": 0.018382451171614075,
      "grad_norm": 1.8153337240219116,
      "learning_rate": 0.00019637588197562542,
      "loss": 2.4434,
      "step": 344
    },
    {
      "epoch": 0.018435888529671093,
      "grad_norm": 1.6775426864624023,
      "learning_rate": 0.0001963651913619842,
      "loss": 2.3513,
      "step": 345
    },
    {
      "epoch": 0.01848932588772811,
      "grad_norm": 2.236999988555908,
      "learning_rate": 0.00019635450074834298,
      "loss": 2.3394,
      "step": 346
    },
    {
      "epoch": 0.01854276324578513,
      "grad_norm": 2.1314921379089355,
      "learning_rate": 0.00019634381013470173,
      "loss": 2.2935,
      "step": 347
    },
    {
      "epoch": 0.018596200603842147,
      "grad_norm": 1.52981698513031,
      "learning_rate": 0.00019633311952106053,
      "loss": 2.1449,
      "step": 348
    },
    {
      "epoch": 0.018649637961899165,
      "grad_norm": 1.2679015398025513,
      "learning_rate": 0.0001963224289074193,
      "loss": 2.0439,
      "step": 349
    },
    {
      "epoch": 0.018703075319956183,
      "grad_norm": 1.2990000247955322,
      "learning_rate": 0.00019631173829377807,
      "loss": 2.1577,
      "step": 350
    },
    {
      "epoch": 0.0187565126780132,
      "grad_norm": 1.254313588142395,
      "learning_rate": 0.00019630104768013685,
      "loss": 2.0955,
      "step": 351
    },
    {
      "epoch": 0.018809950036070215,
      "grad_norm": 1.9866148233413696,
      "learning_rate": 0.00019629035706649563,
      "loss": 2.1631,
      "step": 352
    },
    {
      "epoch": 0.018863387394127233,
      "grad_norm": 1.3959647417068481,
      "learning_rate": 0.0001962796664528544,
      "loss": 2.1566,
      "step": 353
    },
    {
      "epoch": 0.01891682475218425,
      "grad_norm": 1.6550424098968506,
      "learning_rate": 0.00019626897583921319,
      "loss": 2.2306,
      "step": 354
    },
    {
      "epoch": 0.01897026211024127,
      "grad_norm": 2.0562872886657715,
      "learning_rate": 0.00019625828522557194,
      "loss": 2.2609,
      "step": 355
    },
    {
      "epoch": 0.019023699468298287,
      "grad_norm": 1.5405733585357666,
      "learning_rate": 0.00019624759461193074,
      "loss": 2.0369,
      "step": 356
    },
    {
      "epoch": 0.019077136826355305,
      "grad_norm": 1.3865865468978882,
      "learning_rate": 0.00019623690399828952,
      "loss": 2.2382,
      "step": 357
    },
    {
      "epoch": 0.019130574184412323,
      "grad_norm": 2.133028268814087,
      "learning_rate": 0.00019622621338464828,
      "loss": 2.3701,
      "step": 358
    },
    {
      "epoch": 0.01918401154246934,
      "grad_norm": 1.4404391050338745,
      "learning_rate": 0.00019621552277100706,
      "loss": 2.1121,
      "step": 359
    },
    {
      "epoch": 0.01923744890052636,
      "grad_norm": 2.182229518890381,
      "learning_rate": 0.00019620483215736584,
      "loss": 2.1723,
      "step": 360
    },
    {
      "epoch": 0.019290886258583377,
      "grad_norm": 2.014420509338379,
      "learning_rate": 0.00019619414154372462,
      "loss": 2.3511,
      "step": 361
    },
    {
      "epoch": 0.01934432361664039,
      "grad_norm": 1.757541537284851,
      "learning_rate": 0.0001961834509300834,
      "loss": 2.1759,
      "step": 362
    },
    {
      "epoch": 0.01939776097469741,
      "grad_norm": 1.9685503244400024,
      "learning_rate": 0.00019617276031644218,
      "loss": 2.1858,
      "step": 363
    },
    {
      "epoch": 0.019451198332754428,
      "grad_norm": 1.4649550914764404,
      "learning_rate": 0.00019616206970280095,
      "loss": 2.2868,
      "step": 364
    },
    {
      "epoch": 0.019504635690811446,
      "grad_norm": 1.843182921409607,
      "learning_rate": 0.00019615137908915973,
      "loss": 2.3602,
      "step": 365
    },
    {
      "epoch": 0.019558073048868464,
      "grad_norm": 2.1367135047912598,
      "learning_rate": 0.0001961406884755185,
      "loss": 2.6718,
      "step": 366
    },
    {
      "epoch": 0.01961151040692548,
      "grad_norm": 2.3372647762298584,
      "learning_rate": 0.0001961299978618773,
      "loss": 2.3883,
      "step": 367
    },
    {
      "epoch": 0.0196649477649825,
      "grad_norm": 1.8020437955856323,
      "learning_rate": 0.00019611930724823607,
      "loss": 2.1933,
      "step": 368
    },
    {
      "epoch": 0.019718385123039518,
      "grad_norm": 1.6743323802947998,
      "learning_rate": 0.00019610861663459483,
      "loss": 1.9604,
      "step": 369
    },
    {
      "epoch": 0.019771822481096536,
      "grad_norm": 1.011966347694397,
      "learning_rate": 0.0001960979260209536,
      "loss": 2.0275,
      "step": 370
    },
    {
      "epoch": 0.019825259839153554,
      "grad_norm": 1.484629511833191,
      "learning_rate": 0.00019608723540731239,
      "loss": 2.3198,
      "step": 371
    },
    {
      "epoch": 0.01987869719721057,
      "grad_norm": 1.866796612739563,
      "learning_rate": 0.00019607654479367116,
      "loss": 2.0776,
      "step": 372
    },
    {
      "epoch": 0.019932134555267586,
      "grad_norm": 1.4470391273498535,
      "learning_rate": 0.00019606585418002994,
      "loss": 2.2892,
      "step": 373
    },
    {
      "epoch": 0.019985571913324604,
      "grad_norm": 1.2080490589141846,
      "learning_rate": 0.00019605516356638872,
      "loss": 2.0423,
      "step": 374
    },
    {
      "epoch": 0.020039009271381622,
      "grad_norm": 2.18241548538208,
      "learning_rate": 0.0001960444729527475,
      "loss": 2.2523,
      "step": 375
    },
    {
      "epoch": 0.02009244662943864,
      "grad_norm": 1.9202834367752075,
      "learning_rate": 0.00019603378233910628,
      "loss": 2.2496,
      "step": 376
    },
    {
      "epoch": 0.020145883987495658,
      "grad_norm": 1.365255355834961,
      "learning_rate": 0.00019602309172546504,
      "loss": 2.2767,
      "step": 377
    },
    {
      "epoch": 0.020199321345552676,
      "grad_norm": 1.7156450748443604,
      "learning_rate": 0.00019601240111182382,
      "loss": 2.3301,
      "step": 378
    },
    {
      "epoch": 0.020252758703609694,
      "grad_norm": 1.8139128684997559,
      "learning_rate": 0.00019600171049818262,
      "loss": 2.1138,
      "step": 379
    },
    {
      "epoch": 0.020306196061666712,
      "grad_norm": 2.0475480556488037,
      "learning_rate": 0.00019599101988454138,
      "loss": 2.2855,
      "step": 380
    },
    {
      "epoch": 0.02035963341972373,
      "grad_norm": 1.5645244121551514,
      "learning_rate": 0.00019598032927090015,
      "loss": 2.4369,
      "step": 381
    },
    {
      "epoch": 0.020413070777780748,
      "grad_norm": 1.7425687313079834,
      "learning_rate": 0.00019596963865725893,
      "loss": 2.4459,
      "step": 382
    },
    {
      "epoch": 0.020466508135837762,
      "grad_norm": 1.7663627862930298,
      "learning_rate": 0.00019595894804361771,
      "loss": 2.4063,
      "step": 383
    },
    {
      "epoch": 0.02051994549389478,
      "grad_norm": 1.4760801792144775,
      "learning_rate": 0.0001959482574299765,
      "loss": 2.3433,
      "step": 384
    },
    {
      "epoch": 0.0205733828519518,
      "grad_norm": 1.4966237545013428,
      "learning_rate": 0.00019593756681633527,
      "loss": 2.187,
      "step": 385
    },
    {
      "epoch": 0.020626820210008816,
      "grad_norm": 1.5994693040847778,
      "learning_rate": 0.00019592687620269405,
      "loss": 2.1935,
      "step": 386
    },
    {
      "epoch": 0.020680257568065834,
      "grad_norm": 1.582716464996338,
      "learning_rate": 0.00019591618558905283,
      "loss": 2.367,
      "step": 387
    },
    {
      "epoch": 0.020733694926122852,
      "grad_norm": 1.410301923751831,
      "learning_rate": 0.00019590549497541159,
      "loss": 2.2841,
      "step": 388
    },
    {
      "epoch": 0.02078713228417987,
      "grad_norm": 1.8106001615524292,
      "learning_rate": 0.00019589480436177036,
      "loss": 2.4835,
      "step": 389
    },
    {
      "epoch": 0.02084056964223689,
      "grad_norm": 1.8069345951080322,
      "learning_rate": 0.00019588411374812917,
      "loss": 2.3393,
      "step": 390
    },
    {
      "epoch": 0.020894007000293906,
      "grad_norm": 1.4058692455291748,
      "learning_rate": 0.00019587342313448792,
      "loss": 2.0817,
      "step": 391
    },
    {
      "epoch": 0.020947444358350924,
      "grad_norm": 1.6656426191329956,
      "learning_rate": 0.0001958627325208467,
      "loss": 2.2399,
      "step": 392
    },
    {
      "epoch": 0.021000881716407942,
      "grad_norm": 1.5856181383132935,
      "learning_rate": 0.00019585204190720548,
      "loss": 2.4157,
      "step": 393
    },
    {
      "epoch": 0.021054319074464957,
      "grad_norm": 2.1450612545013428,
      "learning_rate": 0.00019584135129356426,
      "loss": 2.1656,
      "step": 394
    },
    {
      "epoch": 0.021107756432521975,
      "grad_norm": 1.409704327583313,
      "learning_rate": 0.00019583066067992304,
      "loss": 2.2677,
      "step": 395
    },
    {
      "epoch": 0.021161193790578993,
      "grad_norm": 1.6665501594543457,
      "learning_rate": 0.00019581997006628182,
      "loss": 2.0794,
      "step": 396
    },
    {
      "epoch": 0.02121463114863601,
      "grad_norm": 1.0707180500030518,
      "learning_rate": 0.00019580927945264057,
      "loss": 2.1483,
      "step": 397
    },
    {
      "epoch": 0.02126806850669303,
      "grad_norm": 1.2828142642974854,
      "learning_rate": 0.00019579858883899938,
      "loss": 2.1464,
      "step": 398
    },
    {
      "epoch": 0.021321505864750047,
      "grad_norm": 2.5235960483551025,
      "learning_rate": 0.00019578789822535813,
      "loss": 2.2938,
      "step": 399
    },
    {
      "epoch": 0.021374943222807065,
      "grad_norm": 2.111114025115967,
      "learning_rate": 0.00019577720761171691,
      "loss": 2.241,
      "step": 400
    },
    {
      "epoch": 0.021428380580864083,
      "grad_norm": 1.5108869075775146,
      "learning_rate": 0.0001957665169980757,
      "loss": 2.0496,
      "step": 401
    },
    {
      "epoch": 0.0214818179389211,
      "grad_norm": 1.4925700426101685,
      "learning_rate": 0.00019575582638443447,
      "loss": 2.2275,
      "step": 402
    },
    {
      "epoch": 0.02153525529697812,
      "grad_norm": 1.7199853658676147,
      "learning_rate": 0.00019574513577079325,
      "loss": 2.2018,
      "step": 403
    },
    {
      "epoch": 0.021588692655035133,
      "grad_norm": 1.8864065408706665,
      "learning_rate": 0.00019573444515715203,
      "loss": 2.123,
      "step": 404
    },
    {
      "epoch": 0.02164213001309215,
      "grad_norm": 1.1504452228546143,
      "learning_rate": 0.0001957237545435108,
      "loss": 1.9894,
      "step": 405
    },
    {
      "epoch": 0.02169556737114917,
      "grad_norm": 1.6304064989089966,
      "learning_rate": 0.0001957130639298696,
      "loss": 2.2186,
      "step": 406
    },
    {
      "epoch": 0.021749004729206187,
      "grad_norm": 1.850245475769043,
      "learning_rate": 0.00019570237331622837,
      "loss": 2.2485,
      "step": 407
    },
    {
      "epoch": 0.021802442087263205,
      "grad_norm": 1.1990010738372803,
      "learning_rate": 0.00019569168270258712,
      "loss": 2.1127,
      "step": 408
    },
    {
      "epoch": 0.021855879445320223,
      "grad_norm": 2.3066883087158203,
      "learning_rate": 0.00019568099208894593,
      "loss": 2.3212,
      "step": 409
    },
    {
      "epoch": 0.02190931680337724,
      "grad_norm": 1.4544934034347534,
      "learning_rate": 0.00019567030147530468,
      "loss": 2.2913,
      "step": 410
    },
    {
      "epoch": 0.02196275416143426,
      "grad_norm": 1.6724469661712646,
      "learning_rate": 0.00019565961086166346,
      "loss": 2.304,
      "step": 411
    },
    {
      "epoch": 0.022016191519491277,
      "grad_norm": 1.6279677152633667,
      "learning_rate": 0.00019564892024802224,
      "loss": 2.3738,
      "step": 412
    },
    {
      "epoch": 0.022069628877548295,
      "grad_norm": 1.3418816328048706,
      "learning_rate": 0.00019563822963438102,
      "loss": 2.1854,
      "step": 413
    },
    {
      "epoch": 0.022123066235605313,
      "grad_norm": 1.5234036445617676,
      "learning_rate": 0.0001956275390207398,
      "loss": 2.2349,
      "step": 414
    },
    {
      "epoch": 0.022176503593662328,
      "grad_norm": 1.4872546195983887,
      "learning_rate": 0.00019561684840709858,
      "loss": 2.3764,
      "step": 415
    },
    {
      "epoch": 0.022229940951719346,
      "grad_norm": 1.5069973468780518,
      "learning_rate": 0.00019560615779345733,
      "loss": 2.2336,
      "step": 416
    },
    {
      "epoch": 0.022283378309776364,
      "grad_norm": 1.6426215171813965,
      "learning_rate": 0.00019559546717981614,
      "loss": 2.2119,
      "step": 417
    },
    {
      "epoch": 0.022336815667833382,
      "grad_norm": 2.135556697845459,
      "learning_rate": 0.00019558477656617492,
      "loss": 2.3547,
      "step": 418
    },
    {
      "epoch": 0.0223902530258904,
      "grad_norm": 2.499321699142456,
      "learning_rate": 0.00019557408595253367,
      "loss": 2.2448,
      "step": 419
    },
    {
      "epoch": 0.022443690383947418,
      "grad_norm": 1.5638816356658936,
      "learning_rate": 0.00019556339533889245,
      "loss": 2.0937,
      "step": 420
    },
    {
      "epoch": 0.022497127742004436,
      "grad_norm": 1.658165454864502,
      "learning_rate": 0.00019555270472525123,
      "loss": 2.177,
      "step": 421
    },
    {
      "epoch": 0.022550565100061454,
      "grad_norm": 1.5044136047363281,
      "learning_rate": 0.00019554201411161,
      "loss": 2.2475,
      "step": 422
    },
    {
      "epoch": 0.022604002458118472,
      "grad_norm": 1.7413270473480225,
      "learning_rate": 0.0001955313234979688,
      "loss": 2.1585,
      "step": 423
    },
    {
      "epoch": 0.02265743981617549,
      "grad_norm": 1.666377067565918,
      "learning_rate": 0.00019552063288432757,
      "loss": 2.3403,
      "step": 424
    },
    {
      "epoch": 0.022710877174232504,
      "grad_norm": 1.6102144718170166,
      "learning_rate": 0.00019550994227068635,
      "loss": 2.1854,
      "step": 425
    },
    {
      "epoch": 0.022764314532289522,
      "grad_norm": 1.4352996349334717,
      "learning_rate": 0.00019549925165704513,
      "loss": 2.1658,
      "step": 426
    },
    {
      "epoch": 0.02281775189034654,
      "grad_norm": 1.4771225452423096,
      "learning_rate": 0.00019548856104340388,
      "loss": 2.1162,
      "step": 427
    },
    {
      "epoch": 0.022871189248403558,
      "grad_norm": 1.5282219648361206,
      "learning_rate": 0.0001954778704297627,
      "loss": 2.1798,
      "step": 428
    },
    {
      "epoch": 0.022924626606460576,
      "grad_norm": 1.9245152473449707,
      "learning_rate": 0.00019546717981612147,
      "loss": 2.485,
      "step": 429
    },
    {
      "epoch": 0.022978063964517594,
      "grad_norm": 1.490512728691101,
      "learning_rate": 0.00019545648920248022,
      "loss": 2.0756,
      "step": 430
    },
    {
      "epoch": 0.023031501322574612,
      "grad_norm": 3.5692527294158936,
      "learning_rate": 0.000195445798588839,
      "loss": 2.6035,
      "step": 431
    },
    {
      "epoch": 0.02308493868063163,
      "grad_norm": 1.775394082069397,
      "learning_rate": 0.0001954351079751978,
      "loss": 2.3631,
      "step": 432
    },
    {
      "epoch": 0.023138376038688648,
      "grad_norm": 1.9869366884231567,
      "learning_rate": 0.00019542441736155656,
      "loss": 2.1336,
      "step": 433
    },
    {
      "epoch": 0.023191813396745666,
      "grad_norm": 1.7394651174545288,
      "learning_rate": 0.00019541372674791534,
      "loss": 2.3423,
      "step": 434
    },
    {
      "epoch": 0.023245250754802684,
      "grad_norm": 1.5531123876571655,
      "learning_rate": 0.00019540303613427412,
      "loss": 2.1749,
      "step": 435
    },
    {
      "epoch": 0.0232986881128597,
      "grad_norm": 1.1321598291397095,
      "learning_rate": 0.0001953923455206329,
      "loss": 2.1495,
      "step": 436
    },
    {
      "epoch": 0.023352125470916717,
      "grad_norm": 1.6347970962524414,
      "learning_rate": 0.00019538165490699168,
      "loss": 2.3563,
      "step": 437
    },
    {
      "epoch": 0.023405562828973735,
      "grad_norm": 1.37310791015625,
      "learning_rate": 0.00019537096429335043,
      "loss": 2.3454,
      "step": 438
    },
    {
      "epoch": 0.023459000187030753,
      "grad_norm": 2.4041125774383545,
      "learning_rate": 0.0001953602736797092,
      "loss": 2.4931,
      "step": 439
    },
    {
      "epoch": 0.02351243754508777,
      "grad_norm": 1.7578755617141724,
      "learning_rate": 0.00019534958306606802,
      "loss": 2.1209,
      "step": 440
    },
    {
      "epoch": 0.02356587490314479,
      "grad_norm": 1.4815069437026978,
      "learning_rate": 0.00019533889245242677,
      "loss": 2.3518,
      "step": 441
    },
    {
      "epoch": 0.023619312261201807,
      "grad_norm": 1.346793293952942,
      "learning_rate": 0.00019532820183878555,
      "loss": 2.1413,
      "step": 442
    },
    {
      "epoch": 0.023672749619258825,
      "grad_norm": 1.59746253490448,
      "learning_rate": 0.00019531751122514433,
      "loss": 2.2829,
      "step": 443
    },
    {
      "epoch": 0.023726186977315843,
      "grad_norm": 1.4369903802871704,
      "learning_rate": 0.0001953068206115031,
      "loss": 2.2007,
      "step": 444
    },
    {
      "epoch": 0.02377962433537286,
      "grad_norm": 1.640157699584961,
      "learning_rate": 0.0001952961299978619,
      "loss": 2.2621,
      "step": 445
    },
    {
      "epoch": 0.023833061693429875,
      "grad_norm": 1.7678275108337402,
      "learning_rate": 0.00019528543938422067,
      "loss": 2.1785,
      "step": 446
    },
    {
      "epoch": 0.023886499051486893,
      "grad_norm": 2.1613359451293945,
      "learning_rate": 0.00019527474877057942,
      "loss": 2.3874,
      "step": 447
    },
    {
      "epoch": 0.02393993640954391,
      "grad_norm": 1.2673709392547607,
      "learning_rate": 0.00019526405815693823,
      "loss": 2.0311,
      "step": 448
    },
    {
      "epoch": 0.02399337376760093,
      "grad_norm": 1.5374022722244263,
      "learning_rate": 0.00019525336754329698,
      "loss": 2.2203,
      "step": 449
    },
    {
      "epoch": 0.024046811125657947,
      "grad_norm": 1.5975531339645386,
      "learning_rate": 0.00019524267692965576,
      "loss": 2.2332,
      "step": 450
    },
    {
      "epoch": 0.024100248483714965,
      "grad_norm": 1.9423389434814453,
      "learning_rate": 0.00019523198631601457,
      "loss": 2.2466,
      "step": 451
    },
    {
      "epoch": 0.024153685841771983,
      "grad_norm": 1.4145885705947876,
      "learning_rate": 0.00019522129570237332,
      "loss": 2.2765,
      "step": 452
    },
    {
      "epoch": 0.024207123199829,
      "grad_norm": 1.6494556665420532,
      "learning_rate": 0.0001952106050887321,
      "loss": 2.3276,
      "step": 453
    },
    {
      "epoch": 0.02426056055788602,
      "grad_norm": 1.2665228843688965,
      "learning_rate": 0.00019519991447509088,
      "loss": 2.1897,
      "step": 454
    },
    {
      "epoch": 0.024313997915943037,
      "grad_norm": 1.881937026977539,
      "learning_rate": 0.00019518922386144966,
      "loss": 2.4594,
      "step": 455
    },
    {
      "epoch": 0.024367435274000055,
      "grad_norm": 1.4754122495651245,
      "learning_rate": 0.00019517853324780844,
      "loss": 2.3743,
      "step": 456
    },
    {
      "epoch": 0.02442087263205707,
      "grad_norm": 1.5660134553909302,
      "learning_rate": 0.00019516784263416722,
      "loss": 2.0857,
      "step": 457
    },
    {
      "epoch": 0.024474309990114088,
      "grad_norm": 1.8273221254348755,
      "learning_rate": 0.00019515715202052597,
      "loss": 2.3449,
      "step": 458
    },
    {
      "epoch": 0.024527747348171106,
      "grad_norm": 1.7010369300842285,
      "learning_rate": 0.00019514646140688478,
      "loss": 2.1599,
      "step": 459
    },
    {
      "epoch": 0.024581184706228124,
      "grad_norm": 1.334182858467102,
      "learning_rate": 0.00019513577079324356,
      "loss": 2.1056,
      "step": 460
    },
    {
      "epoch": 0.02463462206428514,
      "grad_norm": 1.033002495765686,
      "learning_rate": 0.0001951250801796023,
      "loss": 2.0707,
      "step": 461
    },
    {
      "epoch": 0.02468805942234216,
      "grad_norm": 1.2207106351852417,
      "learning_rate": 0.0001951143895659611,
      "loss": 2.0797,
      "step": 462
    },
    {
      "epoch": 0.024741496780399178,
      "grad_norm": 1.7076621055603027,
      "learning_rate": 0.00019510369895231987,
      "loss": 2.2944,
      "step": 463
    },
    {
      "epoch": 0.024794934138456196,
      "grad_norm": 1.6337462663650513,
      "learning_rate": 0.00019509300833867865,
      "loss": 2.2865,
      "step": 464
    },
    {
      "epoch": 0.024848371496513214,
      "grad_norm": 1.4570035934448242,
      "learning_rate": 0.00019508231772503743,
      "loss": 2.1124,
      "step": 465
    },
    {
      "epoch": 0.02490180885457023,
      "grad_norm": 1.6131442785263062,
      "learning_rate": 0.00019507162711139618,
      "loss": 2.0855,
      "step": 466
    },
    {
      "epoch": 0.024955246212627246,
      "grad_norm": 1.6882671117782593,
      "learning_rate": 0.000195060936497755,
      "loss": 2.3363,
      "step": 467
    },
    {
      "epoch": 0.025008683570684264,
      "grad_norm": 2.1365067958831787,
      "learning_rate": 0.00019505024588411377,
      "loss": 2.1806,
      "step": 468
    },
    {
      "epoch": 0.025062120928741282,
      "grad_norm": 2.0290260314941406,
      "learning_rate": 0.00019503955527047252,
      "loss": 2.1939,
      "step": 469
    },
    {
      "epoch": 0.0251155582867983,
      "grad_norm": 1.8123079538345337,
      "learning_rate": 0.0001950288646568313,
      "loss": 2.3007,
      "step": 470
    },
    {
      "epoch": 0.025168995644855318,
      "grad_norm": 1.6627124547958374,
      "learning_rate": 0.0001950181740431901,
      "loss": 2.2224,
      "step": 471
    },
    {
      "epoch": 0.025222433002912336,
      "grad_norm": 1.4242817163467407,
      "learning_rate": 0.00019500748342954886,
      "loss": 2.1412,
      "step": 472
    },
    {
      "epoch": 0.025275870360969354,
      "grad_norm": 1.7171142101287842,
      "learning_rate": 0.00019499679281590764,
      "loss": 2.2832,
      "step": 473
    },
    {
      "epoch": 0.025329307719026372,
      "grad_norm": 1.901180386543274,
      "learning_rate": 0.00019498610220226642,
      "loss": 2.391,
      "step": 474
    },
    {
      "epoch": 0.02538274507708339,
      "grad_norm": 2.2097671031951904,
      "learning_rate": 0.0001949754115886252,
      "loss": 2.2794,
      "step": 475
    },
    {
      "epoch": 0.025436182435140408,
      "grad_norm": 2.028759717941284,
      "learning_rate": 0.00019496472097498398,
      "loss": 2.3262,
      "step": 476
    },
    {
      "epoch": 0.025489619793197426,
      "grad_norm": 1.851659893989563,
      "learning_rate": 0.00019495403036134273,
      "loss": 2.2633,
      "step": 477
    },
    {
      "epoch": 0.02554305715125444,
      "grad_norm": 1.7000919580459595,
      "learning_rate": 0.00019494333974770154,
      "loss": 2.1925,
      "step": 478
    },
    {
      "epoch": 0.02559649450931146,
      "grad_norm": 1.4994149208068848,
      "learning_rate": 0.00019493264913406032,
      "loss": 2.0432,
      "step": 479
    },
    {
      "epoch": 0.025649931867368476,
      "grad_norm": 1.6292952299118042,
      "learning_rate": 0.00019492195852041907,
      "loss": 2.2275,
      "step": 480
    },
    {
      "epoch": 0.025703369225425494,
      "grad_norm": 1.2673231363296509,
      "learning_rate": 0.00019491126790677785,
      "loss": 2.2141,
      "step": 481
    },
    {
      "epoch": 0.025756806583482512,
      "grad_norm": 1.658721923828125,
      "learning_rate": 0.00019490057729313666,
      "loss": 2.0951,
      "step": 482
    },
    {
      "epoch": 0.02581024394153953,
      "grad_norm": 2.232370138168335,
      "learning_rate": 0.0001948898866794954,
      "loss": 2.2737,
      "step": 483
    },
    {
      "epoch": 0.02586368129959655,
      "grad_norm": 1.5627198219299316,
      "learning_rate": 0.0001948791960658542,
      "loss": 2.3156,
      "step": 484
    },
    {
      "epoch": 0.025917118657653566,
      "grad_norm": 1.6117973327636719,
      "learning_rate": 0.00019486850545221297,
      "loss": 2.3256,
      "step": 485
    },
    {
      "epoch": 0.025970556015710584,
      "grad_norm": 2.249568223953247,
      "learning_rate": 0.00019485781483857175,
      "loss": 2.2664,
      "step": 486
    },
    {
      "epoch": 0.026023993373767602,
      "grad_norm": 1.8979620933532715,
      "learning_rate": 0.00019484712422493053,
      "loss": 2.4969,
      "step": 487
    },
    {
      "epoch": 0.026077430731824617,
      "grad_norm": 2.128509759902954,
      "learning_rate": 0.0001948364336112893,
      "loss": 2.2922,
      "step": 488
    },
    {
      "epoch": 0.026130868089881635,
      "grad_norm": 1.6429524421691895,
      "learning_rate": 0.00019482574299764806,
      "loss": 2.1732,
      "step": 489
    },
    {
      "epoch": 0.026184305447938653,
      "grad_norm": 2.024879217147827,
      "learning_rate": 0.00019481505238400687,
      "loss": 2.143,
      "step": 490
    },
    {
      "epoch": 0.02623774280599567,
      "grad_norm": 1.5489376783370972,
      "learning_rate": 0.00019480436177036562,
      "loss": 2.1844,
      "step": 491
    },
    {
      "epoch": 0.02629118016405269,
      "grad_norm": 1.3861124515533447,
      "learning_rate": 0.0001947936711567244,
      "loss": 2.353,
      "step": 492
    },
    {
      "epoch": 0.026344617522109707,
      "grad_norm": 1.7259782552719116,
      "learning_rate": 0.0001947829805430832,
      "loss": 2.2519,
      "step": 493
    },
    {
      "epoch": 0.026398054880166725,
      "grad_norm": 1.6109014749526978,
      "learning_rate": 0.00019477228992944196,
      "loss": 2.3396,
      "step": 494
    },
    {
      "epoch": 0.026451492238223743,
      "grad_norm": 1.8184728622436523,
      "learning_rate": 0.00019476159931580074,
      "loss": 2.3335,
      "step": 495
    },
    {
      "epoch": 0.02650492959628076,
      "grad_norm": 1.2416975498199463,
      "learning_rate": 0.00019475090870215952,
      "loss": 2.2105,
      "step": 496
    },
    {
      "epoch": 0.02655836695433778,
      "grad_norm": 1.635772466659546,
      "learning_rate": 0.0001947402180885183,
      "loss": 2.0831,
      "step": 497
    },
    {
      "epoch": 0.026611804312394797,
      "grad_norm": 1.3396542072296143,
      "learning_rate": 0.00019472952747487708,
      "loss": 2.0149,
      "step": 498
    },
    {
      "epoch": 0.02666524167045181,
      "grad_norm": 1.4073172807693481,
      "learning_rate": 0.00019471883686123586,
      "loss": 2.2383,
      "step": 499
    },
    {
      "epoch": 0.02671867902850883,
      "grad_norm": 1.2398576736450195,
      "learning_rate": 0.0001947081462475946,
      "loss": 2.2385,
      "step": 500
    },
    {
      "epoch": 0.026772116386565847,
      "grad_norm": 1.3142000436782837,
      "learning_rate": 0.00019469745563395341,
      "loss": 2.1336,
      "step": 501
    },
    {
      "epoch": 0.026825553744622865,
      "grad_norm": 2.0256950855255127,
      "learning_rate": 0.00019468676502031217,
      "loss": 2.3916,
      "step": 502
    },
    {
      "epoch": 0.026878991102679883,
      "grad_norm": 2.338701009750366,
      "learning_rate": 0.00019467607440667095,
      "loss": 2.2554,
      "step": 503
    },
    {
      "epoch": 0.0269324284607369,
      "grad_norm": 1.0974456071853638,
      "learning_rate": 0.00019466538379302973,
      "loss": 2.1999,
      "step": 504
    },
    {
      "epoch": 0.02698586581879392,
      "grad_norm": 1.844046711921692,
      "learning_rate": 0.0001946546931793885,
      "loss": 2.4719,
      "step": 505
    },
    {
      "epoch": 0.027039303176850937,
      "grad_norm": 0.9773042798042297,
      "learning_rate": 0.00019464400256574729,
      "loss": 2.1225,
      "step": 506
    },
    {
      "epoch": 0.027092740534907955,
      "grad_norm": 1.511744737625122,
      "learning_rate": 0.00019463331195210607,
      "loss": 2.2554,
      "step": 507
    },
    {
      "epoch": 0.027146177892964973,
      "grad_norm": 1.4934674501419067,
      "learning_rate": 0.00019462262133846482,
      "loss": 2.2031,
      "step": 508
    },
    {
      "epoch": 0.027199615251021988,
      "grad_norm": 1.9313687086105347,
      "learning_rate": 0.00019461193072482362,
      "loss": 2.2365,
      "step": 509
    },
    {
      "epoch": 0.027253052609079006,
      "grad_norm": 1.7127453088760376,
      "learning_rate": 0.0001946012401111824,
      "loss": 2.0869,
      "step": 510
    },
    {
      "epoch": 0.027306489967136024,
      "grad_norm": 1.6346625089645386,
      "learning_rate": 0.00019459054949754116,
      "loss": 2.407,
      "step": 511
    },
    {
      "epoch": 0.027359927325193042,
      "grad_norm": 1.6943135261535645,
      "learning_rate": 0.00019457985888389994,
      "loss": 2.2212,
      "step": 512
    },
    {
      "epoch": 0.02741336468325006,
      "grad_norm": 1.9423191547393799,
      "learning_rate": 0.00019456916827025872,
      "loss": 2.2137,
      "step": 513
    },
    {
      "epoch": 0.027466802041307078,
      "grad_norm": 1.1779425144195557,
      "learning_rate": 0.0001945584776566175,
      "loss": 2.2278,
      "step": 514
    },
    {
      "epoch": 0.027520239399364096,
      "grad_norm": 1.723721981048584,
      "learning_rate": 0.00019454778704297628,
      "loss": 2.2462,
      "step": 515
    },
    {
      "epoch": 0.027573676757421114,
      "grad_norm": 1.3354990482330322,
      "learning_rate": 0.00019453709642933505,
      "loss": 2.156,
      "step": 516
    },
    {
      "epoch": 0.027627114115478132,
      "grad_norm": 2.3046646118164062,
      "learning_rate": 0.00019452640581569383,
      "loss": 2.2433,
      "step": 517
    },
    {
      "epoch": 0.02768055147353515,
      "grad_norm": 1.220600962638855,
      "learning_rate": 0.00019451571520205261,
      "loss": 2.0083,
      "step": 518
    },
    {
      "epoch": 0.027733988831592168,
      "grad_norm": 1.5465046167373657,
      "learning_rate": 0.00019450502458841137,
      "loss": 2.1271,
      "step": 519
    },
    {
      "epoch": 0.027787426189649182,
      "grad_norm": 1.1995384693145752,
      "learning_rate": 0.00019449433397477017,
      "loss": 2.1132,
      "step": 520
    },
    {
      "epoch": 0.0278408635477062,
      "grad_norm": 1.6440407037734985,
      "learning_rate": 0.00019448364336112895,
      "loss": 2.2017,
      "step": 521
    },
    {
      "epoch": 0.027894300905763218,
      "grad_norm": 1.2458263635635376,
      "learning_rate": 0.0001944729527474877,
      "loss": 2.138,
      "step": 522
    },
    {
      "epoch": 0.027947738263820236,
      "grad_norm": 1.9719293117523193,
      "learning_rate": 0.00019446226213384649,
      "loss": 2.1484,
      "step": 523
    },
    {
      "epoch": 0.028001175621877254,
      "grad_norm": 1.7744888067245483,
      "learning_rate": 0.00019445157152020527,
      "loss": 2.3131,
      "step": 524
    },
    {
      "epoch": 0.028054612979934272,
      "grad_norm": 1.3475861549377441,
      "learning_rate": 0.00019444088090656404,
      "loss": 2.2292,
      "step": 525
    },
    {
      "epoch": 0.02810805033799129,
      "grad_norm": 9.445395469665527,
      "learning_rate": 0.00019443019029292282,
      "loss": 2.6096,
      "step": 526
    },
    {
      "epoch": 0.028161487696048308,
      "grad_norm": 1.2377979755401611,
      "learning_rate": 0.0001944194996792816,
      "loss": 2.1588,
      "step": 527
    },
    {
      "epoch": 0.028214925054105326,
      "grad_norm": 1.9649357795715332,
      "learning_rate": 0.00019440880906564038,
      "loss": 2.5576,
      "step": 528
    },
    {
      "epoch": 0.028268362412162344,
      "grad_norm": 1.4124835729599,
      "learning_rate": 0.00019439811845199916,
      "loss": 2.0712,
      "step": 529
    },
    {
      "epoch": 0.02832179977021936,
      "grad_norm": 1.3399747610092163,
      "learning_rate": 0.00019438742783835792,
      "loss": 2.1809,
      "step": 530
    },
    {
      "epoch": 0.028375237128276377,
      "grad_norm": 1.6877853870391846,
      "learning_rate": 0.0001943767372247167,
      "loss": 2.1646,
      "step": 531
    },
    {
      "epoch": 0.028428674486333395,
      "grad_norm": 1.7558391094207764,
      "learning_rate": 0.0001943660466110755,
      "loss": 2.3478,
      "step": 532
    },
    {
      "epoch": 0.028482111844390413,
      "grad_norm": 1.1842668056488037,
      "learning_rate": 0.00019435535599743425,
      "loss": 2.1867,
      "step": 533
    },
    {
      "epoch": 0.02853554920244743,
      "grad_norm": 1.6987746953964233,
      "learning_rate": 0.00019434466538379303,
      "loss": 2.4122,
      "step": 534
    },
    {
      "epoch": 0.02858898656050445,
      "grad_norm": 1.4259535074234009,
      "learning_rate": 0.00019433397477015181,
      "loss": 2.0508,
      "step": 535
    },
    {
      "epoch": 0.028642423918561467,
      "grad_norm": 1.246730923652649,
      "learning_rate": 0.0001943232841565106,
      "loss": 2.2981,
      "step": 536
    },
    {
      "epoch": 0.028695861276618485,
      "grad_norm": 1.554824709892273,
      "learning_rate": 0.00019431259354286937,
      "loss": 2.236,
      "step": 537
    },
    {
      "epoch": 0.028749298634675503,
      "grad_norm": 2.014029026031494,
      "learning_rate": 0.00019430190292922815,
      "loss": 2.4227,
      "step": 538
    },
    {
      "epoch": 0.02880273599273252,
      "grad_norm": 1.3142985105514526,
      "learning_rate": 0.00019429121231558693,
      "loss": 2.115,
      "step": 539
    },
    {
      "epoch": 0.02885617335078954,
      "grad_norm": 1.3710026741027832,
      "learning_rate": 0.0001942805217019457,
      "loss": 2.2525,
      "step": 540
    },
    {
      "epoch": 0.028909610708846553,
      "grad_norm": 1.5977188348770142,
      "learning_rate": 0.00019426983108830446,
      "loss": 2.1867,
      "step": 541
    },
    {
      "epoch": 0.02896304806690357,
      "grad_norm": 2.108612060546875,
      "learning_rate": 0.00019425914047466324,
      "loss": 2.1683,
      "step": 542
    },
    {
      "epoch": 0.02901648542496059,
      "grad_norm": 1.406044840812683,
      "learning_rate": 0.00019424844986102205,
      "loss": 2.1909,
      "step": 543
    },
    {
      "epoch": 0.029069922783017607,
      "grad_norm": 1.338052749633789,
      "learning_rate": 0.0001942377592473808,
      "loss": 2.5381,
      "step": 544
    },
    {
      "epoch": 0.029123360141074625,
      "grad_norm": 1.2222813367843628,
      "learning_rate": 0.00019422706863373958,
      "loss": 2.0283,
      "step": 545
    },
    {
      "epoch": 0.029176797499131643,
      "grad_norm": 2.5835468769073486,
      "learning_rate": 0.00019421637802009836,
      "loss": 2.2773,
      "step": 546
    },
    {
      "epoch": 0.02923023485718866,
      "grad_norm": 1.7525159120559692,
      "learning_rate": 0.00019420568740645714,
      "loss": 2.2138,
      "step": 547
    },
    {
      "epoch": 0.02928367221524568,
      "grad_norm": 1.307668924331665,
      "learning_rate": 0.00019419499679281592,
      "loss": 2.1131,
      "step": 548
    },
    {
      "epoch": 0.029337109573302697,
      "grad_norm": 1.4153488874435425,
      "learning_rate": 0.0001941843061791747,
      "loss": 2.0879,
      "step": 549
    },
    {
      "epoch": 0.029390546931359715,
      "grad_norm": 1.7380988597869873,
      "learning_rate": 0.00019417361556553345,
      "loss": 2.1686,
      "step": 550
    },
    {
      "epoch": 0.02944398428941673,
      "grad_norm": 1.1158589124679565,
      "learning_rate": 0.00019416292495189226,
      "loss": 2.3333,
      "step": 551
    },
    {
      "epoch": 0.029497421647473748,
      "grad_norm": 2.5812857151031494,
      "learning_rate": 0.00019415223433825101,
      "loss": 2.3925,
      "step": 552
    },
    {
      "epoch": 0.029550859005530766,
      "grad_norm": 1.5314263105392456,
      "learning_rate": 0.0001941415437246098,
      "loss": 2.2688,
      "step": 553
    },
    {
      "epoch": 0.029604296363587784,
      "grad_norm": 1.3873002529144287,
      "learning_rate": 0.00019413085311096857,
      "loss": 2.3305,
      "step": 554
    },
    {
      "epoch": 0.0296577337216448,
      "grad_norm": 1.0984522104263306,
      "learning_rate": 0.00019412016249732735,
      "loss": 2.2066,
      "step": 555
    },
    {
      "epoch": 0.02971117107970182,
      "grad_norm": 1.1515185832977295,
      "learning_rate": 0.00019410947188368613,
      "loss": 2.0687,
      "step": 556
    },
    {
      "epoch": 0.029764608437758838,
      "grad_norm": 1.8756275177001953,
      "learning_rate": 0.0001940987812700449,
      "loss": 2.2873,
      "step": 557
    },
    {
      "epoch": 0.029818045795815856,
      "grad_norm": 1.4606049060821533,
      "learning_rate": 0.00019408809065640366,
      "loss": 2.1353,
      "step": 558
    },
    {
      "epoch": 0.029871483153872874,
      "grad_norm": 1.695835828781128,
      "learning_rate": 0.00019407740004276247,
      "loss": 2.1446,
      "step": 559
    },
    {
      "epoch": 0.02992492051192989,
      "grad_norm": 1.6043890714645386,
      "learning_rate": 0.00019406670942912125,
      "loss": 2.2021,
      "step": 560
    },
    {
      "epoch": 0.02997835786998691,
      "grad_norm": 1.3705354928970337,
      "learning_rate": 0.00019405601881548,
      "loss": 2.2624,
      "step": 561
    },
    {
      "epoch": 0.030031795228043924,
      "grad_norm": 1.1575859785079956,
      "learning_rate": 0.0001940453282018388,
      "loss": 2.2651,
      "step": 562
    },
    {
      "epoch": 0.030085232586100942,
      "grad_norm": 1.5600175857543945,
      "learning_rate": 0.00019403463758819756,
      "loss": 2.114,
      "step": 563
    },
    {
      "epoch": 0.03013866994415796,
      "grad_norm": 1.4908521175384521,
      "learning_rate": 0.00019402394697455634,
      "loss": 1.9556,
      "step": 564
    },
    {
      "epoch": 0.030192107302214978,
      "grad_norm": 1.4462400674819946,
      "learning_rate": 0.00019401325636091512,
      "loss": 2.0967,
      "step": 565
    },
    {
      "epoch": 0.030245544660271996,
      "grad_norm": 2.2806289196014404,
      "learning_rate": 0.0001940025657472739,
      "loss": 2.3221,
      "step": 566
    },
    {
      "epoch": 0.030298982018329014,
      "grad_norm": 1.2984694242477417,
      "learning_rate": 0.00019399187513363268,
      "loss": 2.3351,
      "step": 567
    },
    {
      "epoch": 0.030352419376386032,
      "grad_norm": 1.485783338546753,
      "learning_rate": 0.00019398118451999146,
      "loss": 2.1589,
      "step": 568
    },
    {
      "epoch": 0.03040585673444305,
      "grad_norm": 1.6622333526611328,
      "learning_rate": 0.00019397049390635021,
      "loss": 2.1915,
      "step": 569
    },
    {
      "epoch": 0.030459294092500068,
      "grad_norm": 1.6898832321166992,
      "learning_rate": 0.00019395980329270902,
      "loss": 2.3699,
      "step": 570
    },
    {
      "epoch": 0.030512731450557086,
      "grad_norm": 1.549136757850647,
      "learning_rate": 0.0001939491126790678,
      "loss": 2.0333,
      "step": 571
    },
    {
      "epoch": 0.0305661688086141,
      "grad_norm": 1.4768996238708496,
      "learning_rate": 0.00019393842206542655,
      "loss": 2.2611,
      "step": 572
    },
    {
      "epoch": 0.03061960616667112,
      "grad_norm": 1.8645981550216675,
      "learning_rate": 0.00019392773145178533,
      "loss": 2.5206,
      "step": 573
    },
    {
      "epoch": 0.030673043524728136,
      "grad_norm": 1.8403762578964233,
      "learning_rate": 0.00019391704083814414,
      "loss": 2.3616,
      "step": 574
    },
    {
      "epoch": 0.030726480882785154,
      "grad_norm": 1.6410651206970215,
      "learning_rate": 0.0001939063502245029,
      "loss": 2.0992,
      "step": 575
    },
    {
      "epoch": 0.030779918240842172,
      "grad_norm": 1.3858604431152344,
      "learning_rate": 0.00019389565961086167,
      "loss": 2.3054,
      "step": 576
    },
    {
      "epoch": 0.03083335559889919,
      "grad_norm": 1.4648406505584717,
      "learning_rate": 0.00019388496899722045,
      "loss": 2.1871,
      "step": 577
    },
    {
      "epoch": 0.03088679295695621,
      "grad_norm": 1.4571911096572876,
      "learning_rate": 0.00019387427838357923,
      "loss": 1.9865,
      "step": 578
    },
    {
      "epoch": 0.030940230315013226,
      "grad_norm": 2.2239553928375244,
      "learning_rate": 0.000193863587769938,
      "loss": 2.236,
      "step": 579
    },
    {
      "epoch": 0.030993667673070244,
      "grad_norm": 1.4853217601776123,
      "learning_rate": 0.00019385289715629676,
      "loss": 2.1657,
      "step": 580
    },
    {
      "epoch": 0.031047105031127262,
      "grad_norm": 1.3081672191619873,
      "learning_rate": 0.00019384220654265554,
      "loss": 2.4128,
      "step": 581
    },
    {
      "epoch": 0.03110054238918428,
      "grad_norm": 1.622836709022522,
      "learning_rate": 0.00019383151592901435,
      "loss": 2.045,
      "step": 582
    },
    {
      "epoch": 0.031153979747241295,
      "grad_norm": 1.4885605573654175,
      "learning_rate": 0.0001938208253153731,
      "loss": 2.1411,
      "step": 583
    },
    {
      "epoch": 0.031207417105298313,
      "grad_norm": 1.9271833896636963,
      "learning_rate": 0.00019381013470173188,
      "loss": 2.345,
      "step": 584
    },
    {
      "epoch": 0.03126085446335533,
      "grad_norm": 1.3285125494003296,
      "learning_rate": 0.0001937994440880907,
      "loss": 2.1092,
      "step": 585
    },
    {
      "epoch": 0.03131429182141235,
      "grad_norm": 1.3425664901733398,
      "learning_rate": 0.00019378875347444944,
      "loss": 2.0709,
      "step": 586
    },
    {
      "epoch": 0.03136772917946937,
      "grad_norm": 1.370505690574646,
      "learning_rate": 0.00019377806286080822,
      "loss": 2.1339,
      "step": 587
    },
    {
      "epoch": 0.031421166537526385,
      "grad_norm": 1.2551767826080322,
      "learning_rate": 0.000193767372247167,
      "loss": 2.2462,
      "step": 588
    },
    {
      "epoch": 0.0314746038955834,
      "grad_norm": 1.2221683263778687,
      "learning_rate": 0.00019375668163352578,
      "loss": 2.1341,
      "step": 589
    },
    {
      "epoch": 0.03152804125364042,
      "grad_norm": 1.7038935422897339,
      "learning_rate": 0.00019374599101988456,
      "loss": 2.4229,
      "step": 590
    },
    {
      "epoch": 0.03158147861169744,
      "grad_norm": 1.4849499464035034,
      "learning_rate": 0.0001937353004062433,
      "loss": 2.2407,
      "step": 591
    },
    {
      "epoch": 0.03163491596975446,
      "grad_norm": 1.0743727684020996,
      "learning_rate": 0.0001937246097926021,
      "loss": 2.1336,
      "step": 592
    },
    {
      "epoch": 0.031688353327811475,
      "grad_norm": 1.3986661434173584,
      "learning_rate": 0.0001937139191789609,
      "loss": 2.1598,
      "step": 593
    },
    {
      "epoch": 0.03174179068586849,
      "grad_norm": 1.4312304258346558,
      "learning_rate": 0.00019370322856531965,
      "loss": 2.2655,
      "step": 594
    },
    {
      "epoch": 0.03179522804392551,
      "grad_norm": 1.4784131050109863,
      "learning_rate": 0.00019369253795167843,
      "loss": 2.2395,
      "step": 595
    },
    {
      "epoch": 0.03184866540198253,
      "grad_norm": 1.690409541130066,
      "learning_rate": 0.0001936818473380372,
      "loss": 2.2402,
      "step": 596
    },
    {
      "epoch": 0.03190210276003955,
      "grad_norm": 1.606317400932312,
      "learning_rate": 0.000193671156724396,
      "loss": 2.4965,
      "step": 597
    },
    {
      "epoch": 0.03195554011809656,
      "grad_norm": 1.3836867809295654,
      "learning_rate": 0.00019366046611075477,
      "loss": 2.1531,
      "step": 598
    },
    {
      "epoch": 0.032008977476153576,
      "grad_norm": 1.539427399635315,
      "learning_rate": 0.00019364977549711355,
      "loss": 2.2521,
      "step": 599
    },
    {
      "epoch": 0.032062414834210594,
      "grad_norm": 1.425067663192749,
      "learning_rate": 0.0001936390848834723,
      "loss": 2.4025,
      "step": 600
    },
    {
      "epoch": 0.03211585219226761,
      "grad_norm": 1.8217179775238037,
      "learning_rate": 0.0001936283942698311,
      "loss": 2.2552,
      "step": 601
    },
    {
      "epoch": 0.03216928955032463,
      "grad_norm": 1.4867947101593018,
      "learning_rate": 0.0001936177036561899,
      "loss": 2.4704,
      "step": 602
    },
    {
      "epoch": 0.03222272690838165,
      "grad_norm": 1.1089394092559814,
      "learning_rate": 0.00019360701304254864,
      "loss": 2.042,
      "step": 603
    },
    {
      "epoch": 0.032276164266438666,
      "grad_norm": 1.111799955368042,
      "learning_rate": 0.00019359632242890745,
      "loss": 2.136,
      "step": 604
    },
    {
      "epoch": 0.032329601624495684,
      "grad_norm": 1.902008056640625,
      "learning_rate": 0.0001935856318152662,
      "loss": 2.1942,
      "step": 605
    },
    {
      "epoch": 0.0323830389825527,
      "grad_norm": 1.1513423919677734,
      "learning_rate": 0.00019357494120162498,
      "loss": 2.1654,
      "step": 606
    },
    {
      "epoch": 0.03243647634060972,
      "grad_norm": 1.652424693107605,
      "learning_rate": 0.00019356425058798376,
      "loss": 2.1924,
      "step": 607
    },
    {
      "epoch": 0.03248991369866674,
      "grad_norm": 1.7454652786254883,
      "learning_rate": 0.00019355355997434254,
      "loss": 2.3041,
      "step": 608
    },
    {
      "epoch": 0.032543351056723756,
      "grad_norm": 1.2717794179916382,
      "learning_rate": 0.00019354286936070132,
      "loss": 2.2265,
      "step": 609
    },
    {
      "epoch": 0.032596788414780774,
      "grad_norm": 1.3080018758773804,
      "learning_rate": 0.0001935321787470601,
      "loss": 2.2936,
      "step": 610
    },
    {
      "epoch": 0.03265022577283779,
      "grad_norm": 1.8591549396514893,
      "learning_rate": 0.00019352148813341885,
      "loss": 2.207,
      "step": 611
    },
    {
      "epoch": 0.03270366313089481,
      "grad_norm": 1.2872246503829956,
      "learning_rate": 0.00019351079751977766,
      "loss": 2.2347,
      "step": 612
    },
    {
      "epoch": 0.03275710048895183,
      "grad_norm": 1.4195513725280762,
      "learning_rate": 0.00019350010690613644,
      "loss": 2.2671,
      "step": 613
    },
    {
      "epoch": 0.032810537847008846,
      "grad_norm": 1.4187769889831543,
      "learning_rate": 0.0001934894162924952,
      "loss": 2.2547,
      "step": 614
    },
    {
      "epoch": 0.032863975205065864,
      "grad_norm": 1.4649474620819092,
      "learning_rate": 0.00019347872567885397,
      "loss": 2.1367,
      "step": 615
    },
    {
      "epoch": 0.03291741256312288,
      "grad_norm": 1.4283174276351929,
      "learning_rate": 0.00019346803506521275,
      "loss": 2.1856,
      "step": 616
    },
    {
      "epoch": 0.0329708499211799,
      "grad_norm": 1.2949408292770386,
      "learning_rate": 0.00019345734445157153,
      "loss": 2.2842,
      "step": 617
    },
    {
      "epoch": 0.03302428727923692,
      "grad_norm": 1.3126037120819092,
      "learning_rate": 0.0001934466538379303,
      "loss": 2.1253,
      "step": 618
    },
    {
      "epoch": 0.03307772463729393,
      "grad_norm": 1.140615463256836,
      "learning_rate": 0.00019343596322428906,
      "loss": 2.1382,
      "step": 619
    },
    {
      "epoch": 0.03313116199535095,
      "grad_norm": 1.429225206375122,
      "learning_rate": 0.00019342527261064787,
      "loss": 2.0907,
      "step": 620
    },
    {
      "epoch": 0.033184599353407965,
      "grad_norm": 1.5354669094085693,
      "learning_rate": 0.00019341458199700665,
      "loss": 2.1818,
      "step": 621
    },
    {
      "epoch": 0.03323803671146498,
      "grad_norm": 1.8341164588928223,
      "learning_rate": 0.0001934038913833654,
      "loss": 2.1624,
      "step": 622
    },
    {
      "epoch": 0.033291474069522,
      "grad_norm": 1.8855928182601929,
      "learning_rate": 0.00019339320076972418,
      "loss": 2.3149,
      "step": 623
    },
    {
      "epoch": 0.03334491142757902,
      "grad_norm": 1.4104275703430176,
      "learning_rate": 0.00019338251015608299,
      "loss": 2.1542,
      "step": 624
    },
    {
      "epoch": 0.03339834878563604,
      "grad_norm": 2.076389789581299,
      "learning_rate": 0.00019337181954244174,
      "loss": 2.1413,
      "step": 625
    },
    {
      "epoch": 0.033451786143693055,
      "grad_norm": 1.0135736465454102,
      "learning_rate": 0.00019336112892880052,
      "loss": 2.016,
      "step": 626
    },
    {
      "epoch": 0.03350522350175007,
      "grad_norm": 2.021084785461426,
      "learning_rate": 0.0001933504383151593,
      "loss": 2.3583,
      "step": 627
    },
    {
      "epoch": 0.03355866085980709,
      "grad_norm": 1.93429434299469,
      "learning_rate": 0.00019333974770151808,
      "loss": 2.1787,
      "step": 628
    },
    {
      "epoch": 0.03361209821786411,
      "grad_norm": 1.3149877786636353,
      "learning_rate": 0.00019332905708787686,
      "loss": 2.1245,
      "step": 629
    },
    {
      "epoch": 0.03366553557592113,
      "grad_norm": 1.6661688089370728,
      "learning_rate": 0.00019331836647423564,
      "loss": 2.2005,
      "step": 630
    },
    {
      "epoch": 0.033718972933978145,
      "grad_norm": 1.995787501335144,
      "learning_rate": 0.00019330767586059442,
      "loss": 2.2639,
      "step": 631
    },
    {
      "epoch": 0.03377241029203516,
      "grad_norm": 1.186039924621582,
      "learning_rate": 0.0001932969852469532,
      "loss": 2.1086,
      "step": 632
    },
    {
      "epoch": 0.03382584765009218,
      "grad_norm": 1.4031790494918823,
      "learning_rate": 0.00019328629463331195,
      "loss": 2.0754,
      "step": 633
    },
    {
      "epoch": 0.0338792850081492,
      "grad_norm": 1.5352020263671875,
      "learning_rate": 0.00019327560401967073,
      "loss": 2.2993,
      "step": 634
    },
    {
      "epoch": 0.03393272236620622,
      "grad_norm": 1.470697283744812,
      "learning_rate": 0.00019326491340602954,
      "loss": 2.1864,
      "step": 635
    },
    {
      "epoch": 0.033986159724263235,
      "grad_norm": 1.729833722114563,
      "learning_rate": 0.0001932542227923883,
      "loss": 2.1184,
      "step": 636
    },
    {
      "epoch": 0.03403959708232025,
      "grad_norm": 2.1026992797851562,
      "learning_rate": 0.00019324353217874707,
      "loss": 2.2772,
      "step": 637
    },
    {
      "epoch": 0.03409303444037727,
      "grad_norm": 1.7636154890060425,
      "learning_rate": 0.00019323284156510585,
      "loss": 2.1647,
      "step": 638
    },
    {
      "epoch": 0.03414647179843429,
      "grad_norm": 1.5170955657958984,
      "learning_rate": 0.00019322215095146463,
      "loss": 2.3846,
      "step": 639
    },
    {
      "epoch": 0.0341999091564913,
      "grad_norm": 1.5729316473007202,
      "learning_rate": 0.0001932114603378234,
      "loss": 2.2668,
      "step": 640
    },
    {
      "epoch": 0.03425334651454832,
      "grad_norm": 1.5550216436386108,
      "learning_rate": 0.00019320076972418219,
      "loss": 2.2269,
      "step": 641
    },
    {
      "epoch": 0.034306783872605336,
      "grad_norm": 1.9758868217468262,
      "learning_rate": 0.00019319007911054094,
      "loss": 2.4448,
      "step": 642
    },
    {
      "epoch": 0.034360221230662354,
      "grad_norm": 1.2465736865997314,
      "learning_rate": 0.00019317938849689975,
      "loss": 2.1765,
      "step": 643
    },
    {
      "epoch": 0.03441365858871937,
      "grad_norm": 1.9220821857452393,
      "learning_rate": 0.0001931686978832585,
      "loss": 2.3592,
      "step": 644
    },
    {
      "epoch": 0.03446709594677639,
      "grad_norm": 1.3518892526626587,
      "learning_rate": 0.00019315800726961728,
      "loss": 2.306,
      "step": 645
    },
    {
      "epoch": 0.03452053330483341,
      "grad_norm": 1.3050446510314941,
      "learning_rate": 0.00019314731665597606,
      "loss": 2.1905,
      "step": 646
    },
    {
      "epoch": 0.034573970662890426,
      "grad_norm": 1.6217588186264038,
      "learning_rate": 0.00019313662604233484,
      "loss": 2.3406,
      "step": 647
    },
    {
      "epoch": 0.034627408020947444,
      "grad_norm": 1.060975193977356,
      "learning_rate": 0.00019312593542869362,
      "loss": 2.1587,
      "step": 648
    },
    {
      "epoch": 0.03468084537900446,
      "grad_norm": 1.7444885969161987,
      "learning_rate": 0.0001931152448150524,
      "loss": 2.2488,
      "step": 649
    },
    {
      "epoch": 0.03473428273706148,
      "grad_norm": 1.7485138177871704,
      "learning_rate": 0.00019310455420141118,
      "loss": 2.4073,
      "step": 650
    },
    {
      "epoch": 0.0347877200951185,
      "grad_norm": 1.705076813697815,
      "learning_rate": 0.00019309386358776996,
      "loss": 2.5313,
      "step": 651
    },
    {
      "epoch": 0.034841157453175516,
      "grad_norm": 1.5759954452514648,
      "learning_rate": 0.00019308317297412873,
      "loss": 2.1433,
      "step": 652
    },
    {
      "epoch": 0.034894594811232534,
      "grad_norm": 1.3418350219726562,
      "learning_rate": 0.0001930724823604875,
      "loss": 2.3001,
      "step": 653
    },
    {
      "epoch": 0.03494803216928955,
      "grad_norm": 1.7868655920028687,
      "learning_rate": 0.0001930617917468463,
      "loss": 2.3823,
      "step": 654
    },
    {
      "epoch": 0.03500146952734657,
      "grad_norm": 1.2001113891601562,
      "learning_rate": 0.00019305110113320505,
      "loss": 2.238,
      "step": 655
    },
    {
      "epoch": 0.03505490688540359,
      "grad_norm": 1.2167582511901855,
      "learning_rate": 0.00019304041051956383,
      "loss": 2.3818,
      "step": 656
    },
    {
      "epoch": 0.035108344243460606,
      "grad_norm": 1.3483376502990723,
      "learning_rate": 0.0001930297199059226,
      "loss": 2.1862,
      "step": 657
    },
    {
      "epoch": 0.035161781601517623,
      "grad_norm": 1.822011113166809,
      "learning_rate": 0.00019301902929228139,
      "loss": 2.2824,
      "step": 658
    },
    {
      "epoch": 0.03521521895957464,
      "grad_norm": 1.8485299348831177,
      "learning_rate": 0.00019300833867864017,
      "loss": 2.3416,
      "step": 659
    },
    {
      "epoch": 0.03526865631763166,
      "grad_norm": 1.7511745691299438,
      "learning_rate": 0.00019299764806499894,
      "loss": 2.2131,
      "step": 660
    },
    {
      "epoch": 0.03532209367568867,
      "grad_norm": 1.2927852869033813,
      "learning_rate": 0.0001929869574513577,
      "loss": 2.084,
      "step": 661
    },
    {
      "epoch": 0.03537553103374569,
      "grad_norm": 1.4853521585464478,
      "learning_rate": 0.0001929762668377165,
      "loss": 2.3778,
      "step": 662
    },
    {
      "epoch": 0.035428968391802707,
      "grad_norm": 1.2401342391967773,
      "learning_rate": 0.00019296557622407528,
      "loss": 2.275,
      "step": 663
    },
    {
      "epoch": 0.035482405749859725,
      "grad_norm": 1.3119258880615234,
      "learning_rate": 0.00019295488561043404,
      "loss": 2.2896,
      "step": 664
    },
    {
      "epoch": 0.03553584310791674,
      "grad_norm": 1.374569296836853,
      "learning_rate": 0.00019294419499679282,
      "loss": 2.0259,
      "step": 665
    },
    {
      "epoch": 0.03558928046597376,
      "grad_norm": 1.3899859189987183,
      "learning_rate": 0.0001929335043831516,
      "loss": 2.1519,
      "step": 666
    },
    {
      "epoch": 0.03564271782403078,
      "grad_norm": 1.5045347213745117,
      "learning_rate": 0.00019292281376951038,
      "loss": 2.1474,
      "step": 667
    },
    {
      "epoch": 0.035696155182087796,
      "grad_norm": 2.090165138244629,
      "learning_rate": 0.00019291212315586916,
      "loss": 2.2972,
      "step": 668
    },
    {
      "epoch": 0.035749592540144814,
      "grad_norm": 1.476009726524353,
      "learning_rate": 0.00019290143254222793,
      "loss": 2.2511,
      "step": 669
    },
    {
      "epoch": 0.03580302989820183,
      "grad_norm": 1.694032073020935,
      "learning_rate": 0.00019289074192858671,
      "loss": 2.193,
      "step": 670
    },
    {
      "epoch": 0.03585646725625885,
      "grad_norm": 1.730031967163086,
      "learning_rate": 0.0001928800513149455,
      "loss": 2.2926,
      "step": 671
    },
    {
      "epoch": 0.03590990461431587,
      "grad_norm": 1.614108920097351,
      "learning_rate": 0.00019286936070130425,
      "loss": 2.4566,
      "step": 672
    },
    {
      "epoch": 0.035963341972372886,
      "grad_norm": 2.556645393371582,
      "learning_rate": 0.00019285867008766305,
      "loss": 2.3967,
      "step": 673
    },
    {
      "epoch": 0.036016779330429904,
      "grad_norm": 2.056448459625244,
      "learning_rate": 0.00019284797947402183,
      "loss": 2.4301,
      "step": 674
    },
    {
      "epoch": 0.03607021668848692,
      "grad_norm": 1.5918775796890259,
      "learning_rate": 0.00019283728886038059,
      "loss": 2.1932,
      "step": 675
    },
    {
      "epoch": 0.03612365404654394,
      "grad_norm": 1.3888092041015625,
      "learning_rate": 0.00019282659824673937,
      "loss": 2.0333,
      "step": 676
    },
    {
      "epoch": 0.03617709140460096,
      "grad_norm": 1.3957209587097168,
      "learning_rate": 0.00019281590763309817,
      "loss": 2.0205,
      "step": 677
    },
    {
      "epoch": 0.036230528762657976,
      "grad_norm": 1.5049580335617065,
      "learning_rate": 0.00019280521701945692,
      "loss": 2.2625,
      "step": 678
    },
    {
      "epoch": 0.036283966120714994,
      "grad_norm": 1.2437160015106201,
      "learning_rate": 0.0001927945264058157,
      "loss": 2.2045,
      "step": 679
    },
    {
      "epoch": 0.03633740347877201,
      "grad_norm": 1.089208960533142,
      "learning_rate": 0.00019278383579217448,
      "loss": 1.9988,
      "step": 680
    },
    {
      "epoch": 0.03639084083682903,
      "grad_norm": 1.801600456237793,
      "learning_rate": 0.00019277314517853326,
      "loss": 2.3234,
      "step": 681
    },
    {
      "epoch": 0.03644427819488604,
      "grad_norm": 1.5907106399536133,
      "learning_rate": 0.00019276245456489204,
      "loss": 2.1304,
      "step": 682
    },
    {
      "epoch": 0.03649771555294306,
      "grad_norm": 1.4452266693115234,
      "learning_rate": 0.0001927517639512508,
      "loss": 2.183,
      "step": 683
    },
    {
      "epoch": 0.03655115291100008,
      "grad_norm": 1.920575499534607,
      "learning_rate": 0.00019274107333760958,
      "loss": 2.2345,
      "step": 684
    },
    {
      "epoch": 0.036604590269057095,
      "grad_norm": 1.465707778930664,
      "learning_rate": 0.00019273038272396838,
      "loss": 2.4336,
      "step": 685
    },
    {
      "epoch": 0.03665802762711411,
      "grad_norm": 1.324928879737854,
      "learning_rate": 0.00019271969211032713,
      "loss": 2.0683,
      "step": 686
    },
    {
      "epoch": 0.03671146498517113,
      "grad_norm": 1.1896140575408936,
      "learning_rate": 0.00019270900149668591,
      "loss": 2.1509,
      "step": 687
    },
    {
      "epoch": 0.03676490234322815,
      "grad_norm": 1.3168636560440063,
      "learning_rate": 0.0001926983108830447,
      "loss": 2.1888,
      "step": 688
    },
    {
      "epoch": 0.03681833970128517,
      "grad_norm": 1.6046332120895386,
      "learning_rate": 0.00019268762026940347,
      "loss": 2.3692,
      "step": 689
    },
    {
      "epoch": 0.036871777059342185,
      "grad_norm": 1.7971361875534058,
      "learning_rate": 0.00019267692965576225,
      "loss": 2.3852,
      "step": 690
    },
    {
      "epoch": 0.0369252144173992,
      "grad_norm": 0.9796358942985535,
      "learning_rate": 0.00019266623904212103,
      "loss": 2.1712,
      "step": 691
    },
    {
      "epoch": 0.03697865177545622,
      "grad_norm": 0.7618374228477478,
      "learning_rate": 0.00019265554842847979,
      "loss": 1.8588,
      "step": 692
    },
    {
      "epoch": 0.03703208913351324,
      "grad_norm": 1.5379258394241333,
      "learning_rate": 0.0001926448578148386,
      "loss": 2.4374,
      "step": 693
    },
    {
      "epoch": 0.03708552649157026,
      "grad_norm": 1.6948246955871582,
      "learning_rate": 0.00019263416720119734,
      "loss": 2.2216,
      "step": 694
    },
    {
      "epoch": 0.037138963849627275,
      "grad_norm": 1.5116922855377197,
      "learning_rate": 0.00019262347658755612,
      "loss": 2.2677,
      "step": 695
    },
    {
      "epoch": 0.03719240120768429,
      "grad_norm": 1.3563711643218994,
      "learning_rate": 0.00019261278597391493,
      "loss": 2.0893,
      "step": 696
    },
    {
      "epoch": 0.03724583856574131,
      "grad_norm": 1.4171955585479736,
      "learning_rate": 0.00019260209536027368,
      "loss": 2.3012,
      "step": 697
    },
    {
      "epoch": 0.03729927592379833,
      "grad_norm": 1.3919068574905396,
      "learning_rate": 0.00019259140474663246,
      "loss": 2.2228,
      "step": 698
    },
    {
      "epoch": 0.03735271328185535,
      "grad_norm": 1.4732645750045776,
      "learning_rate": 0.00019258071413299124,
      "loss": 2.12,
      "step": 699
    },
    {
      "epoch": 0.037406150639912365,
      "grad_norm": 1.150924801826477,
      "learning_rate": 0.00019257002351935002,
      "loss": 2.0477,
      "step": 700
    },
    {
      "epoch": 0.03745958799796938,
      "grad_norm": 1.508605718612671,
      "learning_rate": 0.0001925593329057088,
      "loss": 2.3267,
      "step": 701
    },
    {
      "epoch": 0.0375130253560264,
      "grad_norm": 1.3584378957748413,
      "learning_rate": 0.00019254864229206758,
      "loss": 2.1193,
      "step": 702
    },
    {
      "epoch": 0.03756646271408341,
      "grad_norm": 1.12851083278656,
      "learning_rate": 0.00019253795167842633,
      "loss": 2.0581,
      "step": 703
    },
    {
      "epoch": 0.03761990007214043,
      "grad_norm": 1.6076037883758545,
      "learning_rate": 0.00019252726106478514,
      "loss": 2.2609,
      "step": 704
    },
    {
      "epoch": 0.03767333743019745,
      "grad_norm": 1.305631160736084,
      "learning_rate": 0.00019251657045114392,
      "loss": 2.1421,
      "step": 705
    },
    {
      "epoch": 0.037726774788254466,
      "grad_norm": 1.3302642107009888,
      "learning_rate": 0.00019250587983750267,
      "loss": 2.1032,
      "step": 706
    },
    {
      "epoch": 0.037780212146311484,
      "grad_norm": 1.2037323713302612,
      "learning_rate": 0.00019249518922386145,
      "loss": 2.1911,
      "step": 707
    },
    {
      "epoch": 0.0378336495043685,
      "grad_norm": 1.983379602432251,
      "learning_rate": 0.00019248449861022023,
      "loss": 2.4865,
      "step": 708
    },
    {
      "epoch": 0.03788708686242552,
      "grad_norm": 1.8941936492919922,
      "learning_rate": 0.000192473807996579,
      "loss": 2.2883,
      "step": 709
    },
    {
      "epoch": 0.03794052422048254,
      "grad_norm": 1.5908396244049072,
      "learning_rate": 0.0001924631173829378,
      "loss": 2.3647,
      "step": 710
    },
    {
      "epoch": 0.037993961578539556,
      "grad_norm": 1.6741591691970825,
      "learning_rate": 0.00019245242676929654,
      "loss": 2.3694,
      "step": 711
    },
    {
      "epoch": 0.038047398936596574,
      "grad_norm": 1.471516489982605,
      "learning_rate": 0.00019244173615565535,
      "loss": 2.1786,
      "step": 712
    },
    {
      "epoch": 0.03810083629465359,
      "grad_norm": 1.148306131362915,
      "learning_rate": 0.00019243104554201413,
      "loss": 1.9337,
      "step": 713
    },
    {
      "epoch": 0.03815427365271061,
      "grad_norm": 1.1475337743759155,
      "learning_rate": 0.00019242035492837288,
      "loss": 2.1267,
      "step": 714
    },
    {
      "epoch": 0.03820771101076763,
      "grad_norm": 1.7932006120681763,
      "learning_rate": 0.0001924096643147317,
      "loss": 2.4304,
      "step": 715
    },
    {
      "epoch": 0.038261148368824646,
      "grad_norm": 1.6503653526306152,
      "learning_rate": 0.00019239897370109047,
      "loss": 2.2993,
      "step": 716
    },
    {
      "epoch": 0.038314585726881664,
      "grad_norm": 1.4143157005310059,
      "learning_rate": 0.00019238828308744922,
      "loss": 2.1607,
      "step": 717
    },
    {
      "epoch": 0.03836802308493868,
      "grad_norm": 1.7323933839797974,
      "learning_rate": 0.000192377592473808,
      "loss": 2.4244,
      "step": 718
    },
    {
      "epoch": 0.0384214604429957,
      "grad_norm": 1.5399236679077148,
      "learning_rate": 0.00019236690186016678,
      "loss": 2.4355,
      "step": 719
    },
    {
      "epoch": 0.03847489780105272,
      "grad_norm": 1.2913134098052979,
      "learning_rate": 0.00019235621124652556,
      "loss": 2.0885,
      "step": 720
    },
    {
      "epoch": 0.038528335159109736,
      "grad_norm": 1.8611347675323486,
      "learning_rate": 0.00019234552063288434,
      "loss": 2.3302,
      "step": 721
    },
    {
      "epoch": 0.038581772517166754,
      "grad_norm": 1.376860499382019,
      "learning_rate": 0.0001923348300192431,
      "loss": 2.4387,
      "step": 722
    },
    {
      "epoch": 0.03863520987522377,
      "grad_norm": 1.3544220924377441,
      "learning_rate": 0.0001923241394056019,
      "loss": 2.3295,
      "step": 723
    },
    {
      "epoch": 0.03868864723328078,
      "grad_norm": 1.8229951858520508,
      "learning_rate": 0.00019231344879196068,
      "loss": 2.3541,
      "step": 724
    },
    {
      "epoch": 0.0387420845913378,
      "grad_norm": 0.8953661918640137,
      "learning_rate": 0.00019230275817831943,
      "loss": 2.1561,
      "step": 725
    },
    {
      "epoch": 0.03879552194939482,
      "grad_norm": 2.336698293685913,
      "learning_rate": 0.0001922920675646782,
      "loss": 2.2908,
      "step": 726
    },
    {
      "epoch": 0.03884895930745184,
      "grad_norm": 1.4190393686294556,
      "learning_rate": 0.00019228137695103702,
      "loss": 2.1746,
      "step": 727
    },
    {
      "epoch": 0.038902396665508855,
      "grad_norm": 1.3974239826202393,
      "learning_rate": 0.00019227068633739577,
      "loss": 2.3588,
      "step": 728
    },
    {
      "epoch": 0.03895583402356587,
      "grad_norm": 1.7201000452041626,
      "learning_rate": 0.00019225999572375455,
      "loss": 2.2868,
      "step": 729
    },
    {
      "epoch": 0.03900927138162289,
      "grad_norm": 1.2189627885818481,
      "learning_rate": 0.00019224930511011333,
      "loss": 2.1488,
      "step": 730
    },
    {
      "epoch": 0.03906270873967991,
      "grad_norm": 1.4673552513122559,
      "learning_rate": 0.0001922386144964721,
      "loss": 2.2888,
      "step": 731
    },
    {
      "epoch": 0.03911614609773693,
      "grad_norm": 1.4084268808364868,
      "learning_rate": 0.0001922279238828309,
      "loss": 1.9699,
      "step": 732
    },
    {
      "epoch": 0.039169583455793945,
      "grad_norm": 1.5589308738708496,
      "learning_rate": 0.00019221723326918964,
      "loss": 2.2258,
      "step": 733
    },
    {
      "epoch": 0.03922302081385096,
      "grad_norm": 1.6125773191452026,
      "learning_rate": 0.00019220654265554842,
      "loss": 2.302,
      "step": 734
    },
    {
      "epoch": 0.03927645817190798,
      "grad_norm": 1.2845243215560913,
      "learning_rate": 0.00019219585204190723,
      "loss": 2.2323,
      "step": 735
    },
    {
      "epoch": 0.039329895529965,
      "grad_norm": 1.1935590505599976,
      "learning_rate": 0.00019218516142826598,
      "loss": 2.1067,
      "step": 736
    },
    {
      "epoch": 0.03938333288802202,
      "grad_norm": 1.6254839897155762,
      "learning_rate": 0.00019217447081462476,
      "loss": 2.2107,
      "step": 737
    },
    {
      "epoch": 0.039436770246079035,
      "grad_norm": 1.056924819946289,
      "learning_rate": 0.00019216378020098357,
      "loss": 2.1193,
      "step": 738
    },
    {
      "epoch": 0.03949020760413605,
      "grad_norm": 1.818843960762024,
      "learning_rate": 0.00019215308958734232,
      "loss": 2.1516,
      "step": 739
    },
    {
      "epoch": 0.03954364496219307,
      "grad_norm": 1.5182384252548218,
      "learning_rate": 0.0001921423989737011,
      "loss": 2.1917,
      "step": 740
    },
    {
      "epoch": 0.03959708232025009,
      "grad_norm": 1.2093291282653809,
      "learning_rate": 0.00019213170836005988,
      "loss": 2.0492,
      "step": 741
    },
    {
      "epoch": 0.03965051967830711,
      "grad_norm": 1.693192958831787,
      "learning_rate": 0.00019212101774641866,
      "loss": 2.3007,
      "step": 742
    },
    {
      "epoch": 0.039703957036364125,
      "grad_norm": 1.4848439693450928,
      "learning_rate": 0.00019211032713277744,
      "loss": 2.0549,
      "step": 743
    },
    {
      "epoch": 0.03975739439442114,
      "grad_norm": 1.4457166194915771,
      "learning_rate": 0.00019209963651913622,
      "loss": 2.1888,
      "step": 744
    },
    {
      "epoch": 0.039810831752478154,
      "grad_norm": 1.3603407144546509,
      "learning_rate": 0.00019208894590549497,
      "loss": 2.1078,
      "step": 745
    },
    {
      "epoch": 0.03986426911053517,
      "grad_norm": 1.4642853736877441,
      "learning_rate": 0.00019207825529185378,
      "loss": 2.4174,
      "step": 746
    },
    {
      "epoch": 0.03991770646859219,
      "grad_norm": 1.656751036643982,
      "learning_rate": 0.00019206756467821253,
      "loss": 2.3446,
      "step": 747
    },
    {
      "epoch": 0.03997114382664921,
      "grad_norm": 1.841557264328003,
      "learning_rate": 0.0001920568740645713,
      "loss": 2.194,
      "step": 748
    },
    {
      "epoch": 0.040024581184706226,
      "grad_norm": 1.2078144550323486,
      "learning_rate": 0.0001920461834509301,
      "loss": 1.9999,
      "step": 749
    },
    {
      "epoch": 0.040078018542763244,
      "grad_norm": 1.5541423559188843,
      "learning_rate": 0.00019203549283728887,
      "loss": 2.2954,
      "step": 750
    },
    {
      "epoch": 0.04013145590082026,
      "grad_norm": 1.7217576503753662,
      "learning_rate": 0.00019202480222364765,
      "loss": 2.2379,
      "step": 751
    },
    {
      "epoch": 0.04018489325887728,
      "grad_norm": 1.4468308687210083,
      "learning_rate": 0.00019201411161000643,
      "loss": 2.1967,
      "step": 752
    },
    {
      "epoch": 0.0402383306169343,
      "grad_norm": 1.4045405387878418,
      "learning_rate": 0.00019200342099636518,
      "loss": 2.2211,
      "step": 753
    },
    {
      "epoch": 0.040291767974991316,
      "grad_norm": 1.4573277235031128,
      "learning_rate": 0.000191992730382724,
      "loss": 2.0269,
      "step": 754
    },
    {
      "epoch": 0.040345205333048334,
      "grad_norm": 1.2338197231292725,
      "learning_rate": 0.00019198203976908277,
      "loss": 2.0608,
      "step": 755
    },
    {
      "epoch": 0.04039864269110535,
      "grad_norm": 1.498403787612915,
      "learning_rate": 0.00019197134915544152,
      "loss": 2.1584,
      "step": 756
    },
    {
      "epoch": 0.04045208004916237,
      "grad_norm": 1.1624724864959717,
      "learning_rate": 0.0001919606585418003,
      "loss": 2.144,
      "step": 757
    },
    {
      "epoch": 0.04050551740721939,
      "grad_norm": 2.293793201446533,
      "learning_rate": 0.00019194996792815908,
      "loss": 2.4298,
      "step": 758
    },
    {
      "epoch": 0.040558954765276406,
      "grad_norm": 1.3676292896270752,
      "learning_rate": 0.00019193927731451786,
      "loss": 2.3048,
      "step": 759
    },
    {
      "epoch": 0.040612392123333424,
      "grad_norm": 1.639336347579956,
      "learning_rate": 0.00019192858670087664,
      "loss": 1.9894,
      "step": 760
    },
    {
      "epoch": 0.04066582948139044,
      "grad_norm": 1.1904668807983398,
      "learning_rate": 0.00019191789608723542,
      "loss": 2.1527,
      "step": 761
    },
    {
      "epoch": 0.04071926683944746,
      "grad_norm": 1.6730982065200806,
      "learning_rate": 0.0001919072054735942,
      "loss": 2.3292,
      "step": 762
    },
    {
      "epoch": 0.04077270419750448,
      "grad_norm": 1.425299048423767,
      "learning_rate": 0.00019189651485995298,
      "loss": 2.2435,
      "step": 763
    },
    {
      "epoch": 0.040826141555561496,
      "grad_norm": 1.3476836681365967,
      "learning_rate": 0.00019188582424631173,
      "loss": 2.1123,
      "step": 764
    },
    {
      "epoch": 0.040879578913618514,
      "grad_norm": 1.9108693599700928,
      "learning_rate": 0.00019187513363267054,
      "loss": 2.0987,
      "step": 765
    },
    {
      "epoch": 0.040933016271675525,
      "grad_norm": 1.2677574157714844,
      "learning_rate": 0.00019186444301902932,
      "loss": 2.2587,
      "step": 766
    },
    {
      "epoch": 0.04098645362973254,
      "grad_norm": 1.3521994352340698,
      "learning_rate": 0.00019185375240538807,
      "loss": 2.2967,
      "step": 767
    },
    {
      "epoch": 0.04103989098778956,
      "grad_norm": 1.1340030431747437,
      "learning_rate": 0.00019184306179174685,
      "loss": 1.9892,
      "step": 768
    },
    {
      "epoch": 0.04109332834584658,
      "grad_norm": 1.6534284353256226,
      "learning_rate": 0.00019183237117810563,
      "loss": 2.3923,
      "step": 769
    },
    {
      "epoch": 0.0411467657039036,
      "grad_norm": 1.613741397857666,
      "learning_rate": 0.0001918216805644644,
      "loss": 2.2854,
      "step": 770
    },
    {
      "epoch": 0.041200203061960615,
      "grad_norm": 1.1695085763931274,
      "learning_rate": 0.0001918109899508232,
      "loss": 2.2447,
      "step": 771
    },
    {
      "epoch": 0.04125364042001763,
      "grad_norm": 1.3475147485733032,
      "learning_rate": 0.00019180029933718197,
      "loss": 2.0667,
      "step": 772
    },
    {
      "epoch": 0.04130707777807465,
      "grad_norm": 1.9056130647659302,
      "learning_rate": 0.00019178960872354075,
      "loss": 2.293,
      "step": 773
    },
    {
      "epoch": 0.04136051513613167,
      "grad_norm": 1.151181936264038,
      "learning_rate": 0.00019177891810989953,
      "loss": 2.1148,
      "step": 774
    },
    {
      "epoch": 0.04141395249418869,
      "grad_norm": 1.8362210988998413,
      "learning_rate": 0.00019176822749625828,
      "loss": 2.4059,
      "step": 775
    },
    {
      "epoch": 0.041467389852245705,
      "grad_norm": 1.7280151844024658,
      "learning_rate": 0.00019175753688261706,
      "loss": 2.3578,
      "step": 776
    },
    {
      "epoch": 0.04152082721030272,
      "grad_norm": 2.2320806980133057,
      "learning_rate": 0.00019174684626897587,
      "loss": 2.2724,
      "step": 777
    },
    {
      "epoch": 0.04157426456835974,
      "grad_norm": 1.4135725498199463,
      "learning_rate": 0.00019173615565533462,
      "loss": 2.1877,
      "step": 778
    },
    {
      "epoch": 0.04162770192641676,
      "grad_norm": 1.0510939359664917,
      "learning_rate": 0.0001917254650416934,
      "loss": 2.1361,
      "step": 779
    },
    {
      "epoch": 0.04168113928447378,
      "grad_norm": 1.3365904092788696,
      "learning_rate": 0.00019171477442805218,
      "loss": 2.2069,
      "step": 780
    },
    {
      "epoch": 0.041734576642530795,
      "grad_norm": 1.6508723497390747,
      "learning_rate": 0.00019170408381441096,
      "loss": 2.3634,
      "step": 781
    },
    {
      "epoch": 0.04178801400058781,
      "grad_norm": 1.8848603963851929,
      "learning_rate": 0.00019169339320076974,
      "loss": 2.3596,
      "step": 782
    },
    {
      "epoch": 0.04184145135864483,
      "grad_norm": 1.788233757019043,
      "learning_rate": 0.00019168270258712852,
      "loss": 2.4034,
      "step": 783
    },
    {
      "epoch": 0.04189488871670185,
      "grad_norm": 2.407042980194092,
      "learning_rate": 0.0001916720119734873,
      "loss": 2.4255,
      "step": 784
    },
    {
      "epoch": 0.04194832607475887,
      "grad_norm": 1.8590139150619507,
      "learning_rate": 0.00019166132135984608,
      "loss": 2.2404,
      "step": 785
    },
    {
      "epoch": 0.042001763432815885,
      "grad_norm": 1.8157464265823364,
      "learning_rate": 0.00019165063074620483,
      "loss": 2.3647,
      "step": 786
    },
    {
      "epoch": 0.042055200790872896,
      "grad_norm": 1.4765375852584839,
      "learning_rate": 0.0001916399401325636,
      "loss": 2.2506,
      "step": 787
    },
    {
      "epoch": 0.042108638148929914,
      "grad_norm": 1.6450854539871216,
      "learning_rate": 0.00019162924951892241,
      "loss": 2.3603,
      "step": 788
    },
    {
      "epoch": 0.04216207550698693,
      "grad_norm": 1.5490843057632446,
      "learning_rate": 0.00019161855890528117,
      "loss": 2.1356,
      "step": 789
    },
    {
      "epoch": 0.04221551286504395,
      "grad_norm": 1.3291841745376587,
      "learning_rate": 0.00019160786829163995,
      "loss": 1.9431,
      "step": 790
    },
    {
      "epoch": 0.04226895022310097,
      "grad_norm": 1.686212182044983,
      "learning_rate": 0.00019159717767799873,
      "loss": 2.2573,
      "step": 791
    },
    {
      "epoch": 0.042322387581157986,
      "grad_norm": 1.4204866886138916,
      "learning_rate": 0.0001915864870643575,
      "loss": 1.9285,
      "step": 792
    },
    {
      "epoch": 0.042375824939215004,
      "grad_norm": 1.5252166986465454,
      "learning_rate": 0.00019157579645071629,
      "loss": 2.093,
      "step": 793
    },
    {
      "epoch": 0.04242926229727202,
      "grad_norm": 1.8324730396270752,
      "learning_rate": 0.00019156510583707507,
      "loss": 2.4512,
      "step": 794
    },
    {
      "epoch": 0.04248269965532904,
      "grad_norm": 1.4690618515014648,
      "learning_rate": 0.00019155441522343382,
      "loss": 2.2151,
      "step": 795
    },
    {
      "epoch": 0.04253613701338606,
      "grad_norm": 1.726744532585144,
      "learning_rate": 0.00019154372460979262,
      "loss": 2.2535,
      "step": 796
    },
    {
      "epoch": 0.042589574371443076,
      "grad_norm": 1.6687337160110474,
      "learning_rate": 0.00019153303399615138,
      "loss": 2.3831,
      "step": 797
    },
    {
      "epoch": 0.042643011729500094,
      "grad_norm": 1.2972661256790161,
      "learning_rate": 0.00019152234338251016,
      "loss": 2.1538,
      "step": 798
    },
    {
      "epoch": 0.04269644908755711,
      "grad_norm": 1.4154998064041138,
      "learning_rate": 0.00019151165276886894,
      "loss": 2.3319,
      "step": 799
    },
    {
      "epoch": 0.04274988644561413,
      "grad_norm": 1.2806435823440552,
      "learning_rate": 0.00019150096215522772,
      "loss": 2.0582,
      "step": 800
    },
    {
      "epoch": 0.04280332380367115,
      "grad_norm": 1.2283363342285156,
      "learning_rate": 0.0001914902715415865,
      "loss": 2.2156,
      "step": 801
    },
    {
      "epoch": 0.042856761161728166,
      "grad_norm": 1.7168383598327637,
      "learning_rate": 0.00019147958092794528,
      "loss": 2.2695,
      "step": 802
    },
    {
      "epoch": 0.042910198519785184,
      "grad_norm": 1.2420209646224976,
      "learning_rate": 0.00019146889031430406,
      "loss": 2.397,
      "step": 803
    },
    {
      "epoch": 0.0429636358778422,
      "grad_norm": 1.5471601486206055,
      "learning_rate": 0.00019145819970066283,
      "loss": 2.43,
      "step": 804
    },
    {
      "epoch": 0.04301707323589922,
      "grad_norm": 1.5725760459899902,
      "learning_rate": 0.00019144750908702161,
      "loss": 2.099,
      "step": 805
    },
    {
      "epoch": 0.04307051059395624,
      "grad_norm": 1.104733943939209,
      "learning_rate": 0.00019143681847338037,
      "loss": 2.1426,
      "step": 806
    },
    {
      "epoch": 0.043123947952013256,
      "grad_norm": 1.8186167478561401,
      "learning_rate": 0.00019142612785973917,
      "loss": 2.3944,
      "step": 807
    },
    {
      "epoch": 0.04317738531007027,
      "grad_norm": 1.355236291885376,
      "learning_rate": 0.00019141543724609793,
      "loss": 2.3784,
      "step": 808
    },
    {
      "epoch": 0.043230822668127285,
      "grad_norm": 2.256197690963745,
      "learning_rate": 0.0001914047466324567,
      "loss": 2.1467,
      "step": 809
    },
    {
      "epoch": 0.0432842600261843,
      "grad_norm": 1.4791826009750366,
      "learning_rate": 0.00019139405601881549,
      "loss": 2.4082,
      "step": 810
    },
    {
      "epoch": 0.04333769738424132,
      "grad_norm": 1.800547480583191,
      "learning_rate": 0.00019138336540517427,
      "loss": 2.2705,
      "step": 811
    },
    {
      "epoch": 0.04339113474229834,
      "grad_norm": 1.5285699367523193,
      "learning_rate": 0.00019137267479153305,
      "loss": 2.1457,
      "step": 812
    },
    {
      "epoch": 0.04344457210035536,
      "grad_norm": 1.6755547523498535,
      "learning_rate": 0.00019136198417789182,
      "loss": 2.0833,
      "step": 813
    },
    {
      "epoch": 0.043498009458412375,
      "grad_norm": 1.2386072874069214,
      "learning_rate": 0.00019135129356425058,
      "loss": 2.0075,
      "step": 814
    },
    {
      "epoch": 0.04355144681646939,
      "grad_norm": 1.4656533002853394,
      "learning_rate": 0.00019134060295060938,
      "loss": 2.2811,
      "step": 815
    },
    {
      "epoch": 0.04360488417452641,
      "grad_norm": 1.4681707620620728,
      "learning_rate": 0.00019132991233696816,
      "loss": 2.1466,
      "step": 816
    },
    {
      "epoch": 0.04365832153258343,
      "grad_norm": 0.9956331253051758,
      "learning_rate": 0.00019131922172332692,
      "loss": 2.105,
      "step": 817
    },
    {
      "epoch": 0.04371175889064045,
      "grad_norm": 1.176392674446106,
      "learning_rate": 0.0001913085311096857,
      "loss": 2.26,
      "step": 818
    },
    {
      "epoch": 0.043765196248697465,
      "grad_norm": 1.4165987968444824,
      "learning_rate": 0.0001912978404960445,
      "loss": 2.0563,
      "step": 819
    },
    {
      "epoch": 0.04381863360675448,
      "grad_norm": 1.5038751363754272,
      "learning_rate": 0.00019128714988240326,
      "loss": 2.1468,
      "step": 820
    },
    {
      "epoch": 0.0438720709648115,
      "grad_norm": 1.5201972723007202,
      "learning_rate": 0.00019127645926876203,
      "loss": 2.0273,
      "step": 821
    },
    {
      "epoch": 0.04392550832286852,
      "grad_norm": 1.5950446128845215,
      "learning_rate": 0.00019126576865512081,
      "loss": 2.2953,
      "step": 822
    },
    {
      "epoch": 0.04397894568092554,
      "grad_norm": 1.701784372329712,
      "learning_rate": 0.0001912550780414796,
      "loss": 2.1479,
      "step": 823
    },
    {
      "epoch": 0.044032383038982555,
      "grad_norm": 1.7777940034866333,
      "learning_rate": 0.00019124438742783837,
      "loss": 2.3859,
      "step": 824
    },
    {
      "epoch": 0.04408582039703957,
      "grad_norm": 1.4215927124023438,
      "learning_rate": 0.00019123369681419713,
      "loss": 2.1287,
      "step": 825
    },
    {
      "epoch": 0.04413925775509659,
      "grad_norm": 2.051622152328491,
      "learning_rate": 0.00019122300620055593,
      "loss": 2.2637,
      "step": 826
    },
    {
      "epoch": 0.04419269511315361,
      "grad_norm": 1.7996653318405151,
      "learning_rate": 0.0001912123155869147,
      "loss": 2.3269,
      "step": 827
    },
    {
      "epoch": 0.04424613247121063,
      "grad_norm": 1.6126220226287842,
      "learning_rate": 0.00019120162497327347,
      "loss": 2.0697,
      "step": 828
    },
    {
      "epoch": 0.04429956982926764,
      "grad_norm": 1.6810160875320435,
      "learning_rate": 0.00019119093435963224,
      "loss": 2.1977,
      "step": 829
    },
    {
      "epoch": 0.044353007187324656,
      "grad_norm": 2.4618430137634277,
      "learning_rate": 0.00019118024374599105,
      "loss": 2.43,
      "step": 830
    },
    {
      "epoch": 0.044406444545381674,
      "grad_norm": 1.3398199081420898,
      "learning_rate": 0.0001911695531323498,
      "loss": 2.1197,
      "step": 831
    },
    {
      "epoch": 0.04445988190343869,
      "grad_norm": 1.7551062107086182,
      "learning_rate": 0.00019115886251870858,
      "loss": 2.3633,
      "step": 832
    },
    {
      "epoch": 0.04451331926149571,
      "grad_norm": 1.1562623977661133,
      "learning_rate": 0.00019114817190506736,
      "loss": 2.0212,
      "step": 833
    },
    {
      "epoch": 0.04456675661955273,
      "grad_norm": 1.9004318714141846,
      "learning_rate": 0.00019113748129142614,
      "loss": 2.5187,
      "step": 834
    },
    {
      "epoch": 0.044620193977609746,
      "grad_norm": 1.6050034761428833,
      "learning_rate": 0.00019112679067778492,
      "loss": 2.4345,
      "step": 835
    },
    {
      "epoch": 0.044673631335666764,
      "grad_norm": 1.4816120862960815,
      "learning_rate": 0.00019111610006414368,
      "loss": 2.0139,
      "step": 836
    },
    {
      "epoch": 0.04472706869372378,
      "grad_norm": 1.6300297975540161,
      "learning_rate": 0.00019110540945050245,
      "loss": 2.2423,
      "step": 837
    },
    {
      "epoch": 0.0447805060517808,
      "grad_norm": 1.1008013486862183,
      "learning_rate": 0.00019109471883686126,
      "loss": 2.2093,
      "step": 838
    },
    {
      "epoch": 0.04483394340983782,
      "grad_norm": 1.3462824821472168,
      "learning_rate": 0.00019108402822322001,
      "loss": 2.0796,
      "step": 839
    },
    {
      "epoch": 0.044887380767894836,
      "grad_norm": 2.5925965309143066,
      "learning_rate": 0.0001910733376095788,
      "loss": 2.5868,
      "step": 840
    },
    {
      "epoch": 0.044940818125951854,
      "grad_norm": 1.2960865497589111,
      "learning_rate": 0.00019106264699593757,
      "loss": 2.1382,
      "step": 841
    },
    {
      "epoch": 0.04499425548400887,
      "grad_norm": 1.3488487005233765,
      "learning_rate": 0.00019105195638229635,
      "loss": 2.1382,
      "step": 842
    },
    {
      "epoch": 0.04504769284206589,
      "grad_norm": 2.005911111831665,
      "learning_rate": 0.00019104126576865513,
      "loss": 2.1171,
      "step": 843
    },
    {
      "epoch": 0.04510113020012291,
      "grad_norm": 1.9580823183059692,
      "learning_rate": 0.0001910305751550139,
      "loss": 2.3,
      "step": 844
    },
    {
      "epoch": 0.045154567558179926,
      "grad_norm": 1.1496927738189697,
      "learning_rate": 0.00019101988454137267,
      "loss": 2.1821,
      "step": 845
    },
    {
      "epoch": 0.045208004916236944,
      "grad_norm": 1.2058496475219727,
      "learning_rate": 0.00019100919392773147,
      "loss": 2.0817,
      "step": 846
    },
    {
      "epoch": 0.04526144227429396,
      "grad_norm": 1.889198660850525,
      "learning_rate": 0.00019099850331409025,
      "loss": 2.3753,
      "step": 847
    },
    {
      "epoch": 0.04531487963235098,
      "grad_norm": 1.375623345375061,
      "learning_rate": 0.000190987812700449,
      "loss": 2.3717,
      "step": 848
    },
    {
      "epoch": 0.045368316990408,
      "grad_norm": 1.0961283445358276,
      "learning_rate": 0.0001909771220868078,
      "loss": 2.0483,
      "step": 849
    },
    {
      "epoch": 0.04542175434846501,
      "grad_norm": 1.3222761154174805,
      "learning_rate": 0.00019096643147316656,
      "loss": 2.149,
      "step": 850
    },
    {
      "epoch": 0.045475191706522027,
      "grad_norm": 1.5250236988067627,
      "learning_rate": 0.00019095574085952534,
      "loss": 2.2894,
      "step": 851
    },
    {
      "epoch": 0.045528629064579045,
      "grad_norm": 1.4946156740188599,
      "learning_rate": 0.00019094505024588412,
      "loss": 2.1347,
      "step": 852
    },
    {
      "epoch": 0.04558206642263606,
      "grad_norm": 1.9192274808883667,
      "learning_rate": 0.0001909343596322429,
      "loss": 2.3343,
      "step": 853
    },
    {
      "epoch": 0.04563550378069308,
      "grad_norm": 2.398179054260254,
      "learning_rate": 0.00019092366901860168,
      "loss": 2.672,
      "step": 854
    },
    {
      "epoch": 0.0456889411387501,
      "grad_norm": 1.9572921991348267,
      "learning_rate": 0.00019091297840496046,
      "loss": 2.3854,
      "step": 855
    },
    {
      "epoch": 0.045742378496807116,
      "grad_norm": 1.7838544845581055,
      "learning_rate": 0.00019090228779131921,
      "loss": 2.0494,
      "step": 856
    },
    {
      "epoch": 0.045795815854864134,
      "grad_norm": 1.1935760974884033,
      "learning_rate": 0.00019089159717767802,
      "loss": 1.9563,
      "step": 857
    },
    {
      "epoch": 0.04584925321292115,
      "grad_norm": 1.9162477254867554,
      "learning_rate": 0.0001908809065640368,
      "loss": 2.2784,
      "step": 858
    },
    {
      "epoch": 0.04590269057097817,
      "grad_norm": 1.315930962562561,
      "learning_rate": 0.00019087021595039555,
      "loss": 2.0202,
      "step": 859
    },
    {
      "epoch": 0.04595612792903519,
      "grad_norm": 1.815403938293457,
      "learning_rate": 0.00019085952533675433,
      "loss": 2.1324,
      "step": 860
    },
    {
      "epoch": 0.046009565287092206,
      "grad_norm": 1.4251837730407715,
      "learning_rate": 0.0001908488347231131,
      "loss": 2.2503,
      "step": 861
    },
    {
      "epoch": 0.046063002645149224,
      "grad_norm": 1.2274397611618042,
      "learning_rate": 0.0001908381441094719,
      "loss": 2.0443,
      "step": 862
    },
    {
      "epoch": 0.04611644000320624,
      "grad_norm": 1.2769529819488525,
      "learning_rate": 0.00019082745349583067,
      "loss": 2.1343,
      "step": 863
    },
    {
      "epoch": 0.04616987736126326,
      "grad_norm": 1.333695888519287,
      "learning_rate": 0.00019081676288218942,
      "loss": 2.1262,
      "step": 864
    },
    {
      "epoch": 0.04622331471932028,
      "grad_norm": 1.4134503602981567,
      "learning_rate": 0.00019080607226854823,
      "loss": 2.2331,
      "step": 865
    },
    {
      "epoch": 0.046276752077377296,
      "grad_norm": 1.796800971031189,
      "learning_rate": 0.000190795381654907,
      "loss": 2.4915,
      "step": 866
    },
    {
      "epoch": 0.046330189435434314,
      "grad_norm": 1.3813637495040894,
      "learning_rate": 0.00019078469104126576,
      "loss": 1.9801,
      "step": 867
    },
    {
      "epoch": 0.04638362679349133,
      "grad_norm": 1.2202892303466797,
      "learning_rate": 0.00019077400042762454,
      "loss": 2.1912,
      "step": 868
    },
    {
      "epoch": 0.04643706415154835,
      "grad_norm": 1.403792142868042,
      "learning_rate": 0.00019076330981398335,
      "loss": 2.1194,
      "step": 869
    },
    {
      "epoch": 0.04649050150960537,
      "grad_norm": 1.3735758066177368,
      "learning_rate": 0.0001907526192003421,
      "loss": 2.1383,
      "step": 870
    },
    {
      "epoch": 0.04654393886766238,
      "grad_norm": 2.293522596359253,
      "learning_rate": 0.00019074192858670088,
      "loss": 2.462,
      "step": 871
    },
    {
      "epoch": 0.0465973762257194,
      "grad_norm": 1.4203075170516968,
      "learning_rate": 0.00019073123797305966,
      "loss": 2.0029,
      "step": 872
    },
    {
      "epoch": 0.046650813583776415,
      "grad_norm": 1.2935127019882202,
      "learning_rate": 0.00019072054735941844,
      "loss": 2.1424,
      "step": 873
    },
    {
      "epoch": 0.04670425094183343,
      "grad_norm": 1.0783452987670898,
      "learning_rate": 0.00019070985674577722,
      "loss": 2.1752,
      "step": 874
    },
    {
      "epoch": 0.04675768829989045,
      "grad_norm": 1.1208809614181519,
      "learning_rate": 0.000190699166132136,
      "loss": 2.186,
      "step": 875
    },
    {
      "epoch": 0.04681112565794747,
      "grad_norm": 1.4236620664596558,
      "learning_rate": 0.00019068847551849478,
      "loss": 2.3181,
      "step": 876
    },
    {
      "epoch": 0.04686456301600449,
      "grad_norm": 1.2951099872589111,
      "learning_rate": 0.00019067778490485356,
      "loss": 2.2349,
      "step": 877
    },
    {
      "epoch": 0.046918000374061505,
      "grad_norm": 1.5294564962387085,
      "learning_rate": 0.0001906670942912123,
      "loss": 2.1559,
      "step": 878
    },
    {
      "epoch": 0.04697143773211852,
      "grad_norm": 1.068179726600647,
      "learning_rate": 0.0001906564036775711,
      "loss": 2.1077,
      "step": 879
    },
    {
      "epoch": 0.04702487509017554,
      "grad_norm": 1.5224013328552246,
      "learning_rate": 0.0001906457130639299,
      "loss": 2.0669,
      "step": 880
    },
    {
      "epoch": 0.04707831244823256,
      "grad_norm": 1.5297578573226929,
      "learning_rate": 0.00019063502245028865,
      "loss": 2.4267,
      "step": 881
    },
    {
      "epoch": 0.04713174980628958,
      "grad_norm": 1.543073296546936,
      "learning_rate": 0.00019062433183664743,
      "loss": 2.1573,
      "step": 882
    },
    {
      "epoch": 0.047185187164346595,
      "grad_norm": 1.589132308959961,
      "learning_rate": 0.0001906136412230062,
      "loss": 1.978,
      "step": 883
    },
    {
      "epoch": 0.04723862452240361,
      "grad_norm": 1.6168893575668335,
      "learning_rate": 0.000190602950609365,
      "loss": 2.1949,
      "step": 884
    },
    {
      "epoch": 0.04729206188046063,
      "grad_norm": 1.5797832012176514,
      "learning_rate": 0.00019059225999572377,
      "loss": 2.295,
      "step": 885
    },
    {
      "epoch": 0.04734549923851765,
      "grad_norm": 1.7997578382492065,
      "learning_rate": 0.00019058156938208255,
      "loss": 2.6014,
      "step": 886
    },
    {
      "epoch": 0.04739893659657467,
      "grad_norm": 1.2988039255142212,
      "learning_rate": 0.0001905708787684413,
      "loss": 2.2444,
      "step": 887
    },
    {
      "epoch": 0.047452373954631685,
      "grad_norm": 1.3177063465118408,
      "learning_rate": 0.0001905601881548001,
      "loss": 2.122,
      "step": 888
    },
    {
      "epoch": 0.0475058113126887,
      "grad_norm": 1.5800199508666992,
      "learning_rate": 0.00019054949754115886,
      "loss": 2.403,
      "step": 889
    },
    {
      "epoch": 0.04755924867074572,
      "grad_norm": 1.0638456344604492,
      "learning_rate": 0.00019053880692751764,
      "loss": 2.1738,
      "step": 890
    },
    {
      "epoch": 0.04761268602880274,
      "grad_norm": 1.3262609243392944,
      "learning_rate": 0.00019052811631387642,
      "loss": 2.3202,
      "step": 891
    },
    {
      "epoch": 0.04766612338685975,
      "grad_norm": 2.5530269145965576,
      "learning_rate": 0.0001905174257002352,
      "loss": 2.2472,
      "step": 892
    },
    {
      "epoch": 0.04771956074491677,
      "grad_norm": 2.39853835105896,
      "learning_rate": 0.00019050673508659398,
      "loss": 2.3521,
      "step": 893
    },
    {
      "epoch": 0.047772998102973786,
      "grad_norm": 1.2057112455368042,
      "learning_rate": 0.00019049604447295276,
      "loss": 2.2149,
      "step": 894
    },
    {
      "epoch": 0.047826435461030804,
      "grad_norm": 1.4519708156585693,
      "learning_rate": 0.00019048535385931154,
      "loss": 2.3909,
      "step": 895
    },
    {
      "epoch": 0.04787987281908782,
      "grad_norm": 1.4575165510177612,
      "learning_rate": 0.00019047466324567032,
      "loss": 2.2249,
      "step": 896
    },
    {
      "epoch": 0.04793331017714484,
      "grad_norm": 1.3541306257247925,
      "learning_rate": 0.0001904639726320291,
      "loss": 2.2294,
      "step": 897
    },
    {
      "epoch": 0.04798674753520186,
      "grad_norm": 1.122869610786438,
      "learning_rate": 0.00019045328201838785,
      "loss": 2.348,
      "step": 898
    },
    {
      "epoch": 0.048040184893258876,
      "grad_norm": 1.561641812324524,
      "learning_rate": 0.00019044259140474666,
      "loss": 2.2005,
      "step": 899
    },
    {
      "epoch": 0.048093622251315894,
      "grad_norm": 1.4678305387496948,
      "learning_rate": 0.0001904319007911054,
      "loss": 2.0317,
      "step": 900
    },
    {
      "epoch": 0.04814705960937291,
      "grad_norm": 1.1428248882293701,
      "learning_rate": 0.0001904212101774642,
      "loss": 2.3086,
      "step": 901
    },
    {
      "epoch": 0.04820049696742993,
      "grad_norm": 1.4924651384353638,
      "learning_rate": 0.00019041051956382297,
      "loss": 2.3904,
      "step": 902
    },
    {
      "epoch": 0.04825393432548695,
      "grad_norm": 1.0865648984909058,
      "learning_rate": 0.00019039982895018175,
      "loss": 2.1222,
      "step": 903
    },
    {
      "epoch": 0.048307371683543966,
      "grad_norm": 1.629478096961975,
      "learning_rate": 0.00019038913833654053,
      "loss": 2.2441,
      "step": 904
    },
    {
      "epoch": 0.048360809041600984,
      "grad_norm": 2.0842390060424805,
      "learning_rate": 0.0001903784477228993,
      "loss": 2.3379,
      "step": 905
    },
    {
      "epoch": 0.048414246399658,
      "grad_norm": 2.0126280784606934,
      "learning_rate": 0.00019036775710925806,
      "loss": 2.5106,
      "step": 906
    },
    {
      "epoch": 0.04846768375771502,
      "grad_norm": 1.23736572265625,
      "learning_rate": 0.00019035706649561687,
      "loss": 2.2032,
      "step": 907
    },
    {
      "epoch": 0.04852112111577204,
      "grad_norm": 0.9739037156105042,
      "learning_rate": 0.00019034637588197565,
      "loss": 2.1335,
      "step": 908
    },
    {
      "epoch": 0.048574558473829056,
      "grad_norm": 1.4522392749786377,
      "learning_rate": 0.0001903356852683344,
      "loss": 2.1798,
      "step": 909
    },
    {
      "epoch": 0.048627995831886074,
      "grad_norm": 1.8891273736953735,
      "learning_rate": 0.00019032499465469318,
      "loss": 2.4007,
      "step": 910
    },
    {
      "epoch": 0.04868143318994309,
      "grad_norm": 1.5044276714324951,
      "learning_rate": 0.00019031430404105196,
      "loss": 2.2169,
      "step": 911
    },
    {
      "epoch": 0.04873487054800011,
      "grad_norm": 1.0390287637710571,
      "learning_rate": 0.00019030361342741074,
      "loss": 2.1694,
      "step": 912
    },
    {
      "epoch": 0.04878830790605712,
      "grad_norm": 2.0082037448883057,
      "learning_rate": 0.00019029292281376952,
      "loss": 2.4951,
      "step": 913
    },
    {
      "epoch": 0.04884174526411414,
      "grad_norm": 0.861765444278717,
      "learning_rate": 0.0001902822322001283,
      "loss": 2.0514,
      "step": 914
    },
    {
      "epoch": 0.04889518262217116,
      "grad_norm": 1.130778193473816,
      "learning_rate": 0.00019027154158648708,
      "loss": 2.1917,
      "step": 915
    },
    {
      "epoch": 0.048948619980228175,
      "grad_norm": 1.140794038772583,
      "learning_rate": 0.00019026085097284586,
      "loss": 2.165,
      "step": 916
    },
    {
      "epoch": 0.04900205733828519,
      "grad_norm": 1.0155681371688843,
      "learning_rate": 0.0001902501603592046,
      "loss": 2.2466,
      "step": 917
    },
    {
      "epoch": 0.04905549469634221,
      "grad_norm": 1.38925302028656,
      "learning_rate": 0.00019023946974556342,
      "loss": 2.2067,
      "step": 918
    },
    {
      "epoch": 0.04910893205439923,
      "grad_norm": 1.9756447076797485,
      "learning_rate": 0.0001902287791319222,
      "loss": 2.299,
      "step": 919
    },
    {
      "epoch": 0.04916236941245625,
      "grad_norm": 1.307783603668213,
      "learning_rate": 0.00019021808851828095,
      "loss": 2.1185,
      "step": 920
    },
    {
      "epoch": 0.049215806770513265,
      "grad_norm": 1.2512362003326416,
      "learning_rate": 0.00019020739790463973,
      "loss": 2.2039,
      "step": 921
    },
    {
      "epoch": 0.04926924412857028,
      "grad_norm": 1.4778058528900146,
      "learning_rate": 0.0001901967072909985,
      "loss": 2.2795,
      "step": 922
    },
    {
      "epoch": 0.0493226814866273,
      "grad_norm": 1.869733214378357,
      "learning_rate": 0.0001901860166773573,
      "loss": 2.2806,
      "step": 923
    },
    {
      "epoch": 0.04937611884468432,
      "grad_norm": 1.3221535682678223,
      "learning_rate": 0.00019017532606371607,
      "loss": 2.2731,
      "step": 924
    },
    {
      "epoch": 0.04942955620274134,
      "grad_norm": 1.1730527877807617,
      "learning_rate": 0.00019016463545007485,
      "loss": 2.0644,
      "step": 925
    },
    {
      "epoch": 0.049482993560798355,
      "grad_norm": 1.3170374631881714,
      "learning_rate": 0.00019015394483643363,
      "loss": 2.0416,
      "step": 926
    },
    {
      "epoch": 0.04953643091885537,
      "grad_norm": 1.3463069200515747,
      "learning_rate": 0.0001901432542227924,
      "loss": 2.1222,
      "step": 927
    },
    {
      "epoch": 0.04958986827691239,
      "grad_norm": 1.351737380027771,
      "learning_rate": 0.00019013256360915116,
      "loss": 2.1024,
      "step": 928
    },
    {
      "epoch": 0.04964330563496941,
      "grad_norm": 1.2718664407730103,
      "learning_rate": 0.00019012187299550994,
      "loss": 2.2029,
      "step": 929
    },
    {
      "epoch": 0.04969674299302643,
      "grad_norm": 1.3145381212234497,
      "learning_rate": 0.00019011118238186875,
      "loss": 2.2914,
      "step": 930
    },
    {
      "epoch": 0.049750180351083445,
      "grad_norm": 1.1492438316345215,
      "learning_rate": 0.0001901004917682275,
      "loss": 2.2288,
      "step": 931
    },
    {
      "epoch": 0.04980361770914046,
      "grad_norm": 1.2096220254898071,
      "learning_rate": 0.00019008980115458628,
      "loss": 2.0713,
      "step": 932
    },
    {
      "epoch": 0.04985705506719748,
      "grad_norm": 1.6046241521835327,
      "learning_rate": 0.00019007911054094506,
      "loss": 2.2336,
      "step": 933
    },
    {
      "epoch": 0.04991049242525449,
      "grad_norm": 2.5457961559295654,
      "learning_rate": 0.00019006841992730384,
      "loss": 2.1405,
      "step": 934
    },
    {
      "epoch": 0.04996392978331151,
      "grad_norm": 2.415942907333374,
      "learning_rate": 0.00019005772931366262,
      "loss": 2.2566,
      "step": 935
    },
    {
      "epoch": 0.05001736714136853,
      "grad_norm": 1.4180738925933838,
      "learning_rate": 0.0001900470387000214,
      "loss": 2.289,
      "step": 936
    },
    {
      "epoch": 0.050070804499425546,
      "grad_norm": 1.9068052768707275,
      "learning_rate": 0.00019003634808638018,
      "loss": 2.4201,
      "step": 937
    },
    {
      "epoch": 0.050124241857482564,
      "grad_norm": 1.5753716230392456,
      "learning_rate": 0.00019002565747273896,
      "loss": 2.382,
      "step": 938
    },
    {
      "epoch": 0.05017767921553958,
      "grad_norm": 1.9960376024246216,
      "learning_rate": 0.0001900149668590977,
      "loss": 2.4754,
      "step": 939
    },
    {
      "epoch": 0.0502311165735966,
      "grad_norm": 1.2534706592559814,
      "learning_rate": 0.0001900042762454565,
      "loss": 2.0853,
      "step": 940
    },
    {
      "epoch": 0.05028455393165362,
      "grad_norm": 1.2314977645874023,
      "learning_rate": 0.0001899935856318153,
      "loss": 2.0435,
      "step": 941
    },
    {
      "epoch": 0.050337991289710636,
      "grad_norm": 1.2997159957885742,
      "learning_rate": 0.00018998289501817405,
      "loss": 2.2131,
      "step": 942
    },
    {
      "epoch": 0.050391428647767654,
      "grad_norm": 1.1449689865112305,
      "learning_rate": 0.00018997220440453283,
      "loss": 2.1649,
      "step": 943
    },
    {
      "epoch": 0.05044486600582467,
      "grad_norm": 1.86053466796875,
      "learning_rate": 0.0001899615137908916,
      "loss": 2.4846,
      "step": 944
    },
    {
      "epoch": 0.05049830336388169,
      "grad_norm": 1.1910037994384766,
      "learning_rate": 0.00018995082317725039,
      "loss": 2.1436,
      "step": 945
    },
    {
      "epoch": 0.05055174072193871,
      "grad_norm": 1.1120388507843018,
      "learning_rate": 0.00018994013256360917,
      "loss": 2.1967,
      "step": 946
    },
    {
      "epoch": 0.050605178079995726,
      "grad_norm": 1.01145339012146,
      "learning_rate": 0.00018992944194996795,
      "loss": 2.0041,
      "step": 947
    },
    {
      "epoch": 0.050658615438052744,
      "grad_norm": 1.2396103143692017,
      "learning_rate": 0.0001899187513363267,
      "loss": 2.2688,
      "step": 948
    },
    {
      "epoch": 0.05071205279610976,
      "grad_norm": 1.2199912071228027,
      "learning_rate": 0.0001899080607226855,
      "loss": 2.1075,
      "step": 949
    },
    {
      "epoch": 0.05076549015416678,
      "grad_norm": 0.9256200790405273,
      "learning_rate": 0.00018989737010904426,
      "loss": 1.95,
      "step": 950
    },
    {
      "epoch": 0.0508189275122238,
      "grad_norm": 1.2445447444915771,
      "learning_rate": 0.00018988667949540304,
      "loss": 2.261,
      "step": 951
    },
    {
      "epoch": 0.050872364870280816,
      "grad_norm": 1.6769301891326904,
      "learning_rate": 0.00018987598888176182,
      "loss": 2.4662,
      "step": 952
    },
    {
      "epoch": 0.050925802228337834,
      "grad_norm": 1.7828115224838257,
      "learning_rate": 0.0001898652982681206,
      "loss": 2.2614,
      "step": 953
    },
    {
      "epoch": 0.05097923958639485,
      "grad_norm": 1.3922134637832642,
      "learning_rate": 0.00018985460765447938,
      "loss": 2.3383,
      "step": 954
    },
    {
      "epoch": 0.05103267694445186,
      "grad_norm": 1.1962181329727173,
      "learning_rate": 0.00018984391704083816,
      "loss": 2.1517,
      "step": 955
    },
    {
      "epoch": 0.05108611430250888,
      "grad_norm": 1.0479016304016113,
      "learning_rate": 0.0001898332264271969,
      "loss": 2.0086,
      "step": 956
    },
    {
      "epoch": 0.0511395516605659,
      "grad_norm": 1.563866138458252,
      "learning_rate": 0.00018982253581355571,
      "loss": 2.2482,
      "step": 957
    },
    {
      "epoch": 0.05119298901862292,
      "grad_norm": 1.3338490724563599,
      "learning_rate": 0.0001898118451999145,
      "loss": 2.0084,
      "step": 958
    },
    {
      "epoch": 0.051246426376679935,
      "grad_norm": 1.7132970094680786,
      "learning_rate": 0.00018980115458627325,
      "loss": 2.3382,
      "step": 959
    },
    {
      "epoch": 0.05129986373473695,
      "grad_norm": 1.190850019454956,
      "learning_rate": 0.00018979046397263205,
      "loss": 2.0981,
      "step": 960
    },
    {
      "epoch": 0.05135330109279397,
      "grad_norm": 0.9099943041801453,
      "learning_rate": 0.00018977977335899083,
      "loss": 2.2013,
      "step": 961
    },
    {
      "epoch": 0.05140673845085099,
      "grad_norm": 1.2725077867507935,
      "learning_rate": 0.00018976908274534959,
      "loss": 2.131,
      "step": 962
    },
    {
      "epoch": 0.05146017580890801,
      "grad_norm": 1.6348954439163208,
      "learning_rate": 0.00018975839213170837,
      "loss": 2.2426,
      "step": 963
    },
    {
      "epoch": 0.051513613166965025,
      "grad_norm": 1.4351247549057007,
      "learning_rate": 0.00018974770151806715,
      "loss": 2.1216,
      "step": 964
    },
    {
      "epoch": 0.05156705052502204,
      "grad_norm": 1.355481505393982,
      "learning_rate": 0.00018973701090442592,
      "loss": 2.2031,
      "step": 965
    },
    {
      "epoch": 0.05162048788307906,
      "grad_norm": 1.1219637393951416,
      "learning_rate": 0.0001897263202907847,
      "loss": 2.1489,
      "step": 966
    },
    {
      "epoch": 0.05167392524113608,
      "grad_norm": 2.0048775672912598,
      "learning_rate": 0.00018971562967714346,
      "loss": 2.247,
      "step": 967
    },
    {
      "epoch": 0.0517273625991931,
      "grad_norm": 1.3452367782592773,
      "learning_rate": 0.00018970493906350226,
      "loss": 2.0265,
      "step": 968
    },
    {
      "epoch": 0.051780799957250115,
      "grad_norm": 1.2537345886230469,
      "learning_rate": 0.00018969424844986104,
      "loss": 2.0041,
      "step": 969
    },
    {
      "epoch": 0.05183423731530713,
      "grad_norm": 1.3957139253616333,
      "learning_rate": 0.0001896835578362198,
      "loss": 2.164,
      "step": 970
    },
    {
      "epoch": 0.05188767467336415,
      "grad_norm": 1.7320055961608887,
      "learning_rate": 0.00018967286722257858,
      "loss": 2.3415,
      "step": 971
    },
    {
      "epoch": 0.05194111203142117,
      "grad_norm": 1.0724568367004395,
      "learning_rate": 0.00018966217660893738,
      "loss": 2.1319,
      "step": 972
    },
    {
      "epoch": 0.05199454938947819,
      "grad_norm": 1.2247984409332275,
      "learning_rate": 0.00018965148599529613,
      "loss": 2.2242,
      "step": 973
    },
    {
      "epoch": 0.052047986747535205,
      "grad_norm": 1.9695976972579956,
      "learning_rate": 0.00018964079538165491,
      "loss": 2.2507,
      "step": 974
    },
    {
      "epoch": 0.05210142410559222,
      "grad_norm": 1.5388609170913696,
      "learning_rate": 0.0001896301047680137,
      "loss": 2.0694,
      "step": 975
    },
    {
      "epoch": 0.052154861463649234,
      "grad_norm": 1.7017155885696411,
      "learning_rate": 0.00018961941415437247,
      "loss": 2.3479,
      "step": 976
    },
    {
      "epoch": 0.05220829882170625,
      "grad_norm": 1.7330573797225952,
      "learning_rate": 0.00018960872354073125,
      "loss": 2.2761,
      "step": 977
    },
    {
      "epoch": 0.05226173617976327,
      "grad_norm": 1.3766883611679077,
      "learning_rate": 0.00018959803292709,
      "loss": 2.3367,
      "step": 978
    },
    {
      "epoch": 0.05231517353782029,
      "grad_norm": 2.0134243965148926,
      "learning_rate": 0.00018958734231344879,
      "loss": 2.3577,
      "step": 979
    },
    {
      "epoch": 0.052368610895877306,
      "grad_norm": 1.287734031677246,
      "learning_rate": 0.0001895766516998076,
      "loss": 2.042,
      "step": 980
    },
    {
      "epoch": 0.052422048253934324,
      "grad_norm": 1.5339672565460205,
      "learning_rate": 0.00018956596108616634,
      "loss": 2.2083,
      "step": 981
    },
    {
      "epoch": 0.05247548561199134,
      "grad_norm": 1.4226977825164795,
      "learning_rate": 0.00018955527047252512,
      "loss": 2.3277,
      "step": 982
    },
    {
      "epoch": 0.05252892297004836,
      "grad_norm": 1.2751632928848267,
      "learning_rate": 0.00018954457985888393,
      "loss": 2.2829,
      "step": 983
    },
    {
      "epoch": 0.05258236032810538,
      "grad_norm": 1.3605647087097168,
      "learning_rate": 0.00018953388924524268,
      "loss": 2.2066,
      "step": 984
    },
    {
      "epoch": 0.052635797686162396,
      "grad_norm": 1.04835844039917,
      "learning_rate": 0.00018952319863160146,
      "loss": 2.0535,
      "step": 985
    },
    {
      "epoch": 0.052689235044219414,
      "grad_norm": 1.5190150737762451,
      "learning_rate": 0.00018951250801796024,
      "loss": 2.2382,
      "step": 986
    },
    {
      "epoch": 0.05274267240227643,
      "grad_norm": 1.898591160774231,
      "learning_rate": 0.00018950181740431902,
      "loss": 2.4098,
      "step": 987
    },
    {
      "epoch": 0.05279610976033345,
      "grad_norm": 1.688984751701355,
      "learning_rate": 0.0001894911267906778,
      "loss": 2.0854,
      "step": 988
    },
    {
      "epoch": 0.05284954711839047,
      "grad_norm": 1.1983951330184937,
      "learning_rate": 0.00018948043617703658,
      "loss": 2.1841,
      "step": 989
    },
    {
      "epoch": 0.052902984476447486,
      "grad_norm": 0.9932587146759033,
      "learning_rate": 0.00018946974556339533,
      "loss": 2.1194,
      "step": 990
    },
    {
      "epoch": 0.052956421834504504,
      "grad_norm": 1.463789939880371,
      "learning_rate": 0.00018945905494975414,
      "loss": 2.2294,
      "step": 991
    },
    {
      "epoch": 0.05300985919256152,
      "grad_norm": 1.8532174825668335,
      "learning_rate": 0.0001894483643361129,
      "loss": 2.3093,
      "step": 992
    },
    {
      "epoch": 0.05306329655061854,
      "grad_norm": 1.560888648033142,
      "learning_rate": 0.00018943767372247167,
      "loss": 2.2411,
      "step": 993
    },
    {
      "epoch": 0.05311673390867556,
      "grad_norm": 2.169429063796997,
      "learning_rate": 0.00018942698310883045,
      "loss": 2.2762,
      "step": 994
    },
    {
      "epoch": 0.053170171266732576,
      "grad_norm": 2.0678365230560303,
      "learning_rate": 0.00018941629249518923,
      "loss": 2.2352,
      "step": 995
    },
    {
      "epoch": 0.053223608624789594,
      "grad_norm": 1.2304105758666992,
      "learning_rate": 0.000189405601881548,
      "loss": 2.2647,
      "step": 996
    },
    {
      "epoch": 0.053277045982846605,
      "grad_norm": 1.1286602020263672,
      "learning_rate": 0.0001893949112679068,
      "loss": 2.2563,
      "step": 997
    },
    {
      "epoch": 0.05333048334090362,
      "grad_norm": 2.023955821990967,
      "learning_rate": 0.00018938422065426554,
      "loss": 2.266,
      "step": 998
    },
    {
      "epoch": 0.05338392069896064,
      "grad_norm": 1.25797438621521,
      "learning_rate": 0.00018937353004062435,
      "loss": 2.2712,
      "step": 999
    },
    {
      "epoch": 0.05343735805701766,
      "grad_norm": 1.1662591695785522,
      "learning_rate": 0.00018936283942698313,
      "loss": 2.08,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 18713,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1320534709171200.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
