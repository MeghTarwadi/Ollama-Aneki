{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.10687471611403532,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 5.343735805701766e-05,
      "grad_norm": 11.970237731933594,
      "learning_rate": 4e-05,
      "loss": 7.7476,
      "step": 1
    },
    {
      "epoch": 0.00010687471611403532,
      "grad_norm": 10.701306343078613,
      "learning_rate": 8e-05,
      "loss": 7.9686,
      "step": 2
    },
    {
      "epoch": 0.00016031207417105297,
      "grad_norm": 11.011762619018555,
      "learning_rate": 0.00012,
      "loss": 7.7773,
      "step": 3
    },
    {
      "epoch": 0.00021374943222807064,
      "grad_norm": 9.025511741638184,
      "learning_rate": 0.00016,
      "loss": 7.3149,
      "step": 4
    },
    {
      "epoch": 0.0002671867902850883,
      "grad_norm": 13.776436805725098,
      "learning_rate": 0.0002,
      "loss": 6.8264,
      "step": 5
    },
    {
      "epoch": 0.00032062414834210595,
      "grad_norm": 12.221830368041992,
      "learning_rate": 0.0001999893093863588,
      "loss": 5.7921,
      "step": 6
    },
    {
      "epoch": 0.0003740615063991236,
      "grad_norm": 9.13806438446045,
      "learning_rate": 0.00019997861877271757,
      "loss": 5.5478,
      "step": 7
    },
    {
      "epoch": 0.0004274988644561413,
      "grad_norm": 12.186979293823242,
      "learning_rate": 0.00019996792815907635,
      "loss": 4.7754,
      "step": 8
    },
    {
      "epoch": 0.00048093622251315894,
      "grad_norm": 12.555595397949219,
      "learning_rate": 0.00019995723754543513,
      "loss": 4.6827,
      "step": 9
    },
    {
      "epoch": 0.0005343735805701766,
      "grad_norm": 18.793758392333984,
      "learning_rate": 0.00019994654693179388,
      "loss": 4.0134,
      "step": 10
    },
    {
      "epoch": 0.0005878109386271942,
      "grad_norm": 14.1957368850708,
      "learning_rate": 0.0001999358563181527,
      "loss": 3.7564,
      "step": 11
    },
    {
      "epoch": 0.0006412482966842119,
      "grad_norm": 13.329666137695312,
      "learning_rate": 0.00019992516570451144,
      "loss": 3.9015,
      "step": 12
    },
    {
      "epoch": 0.0006946856547412296,
      "grad_norm": 22.324115753173828,
      "learning_rate": 0.00019991447509087022,
      "loss": 3.82,
      "step": 13
    },
    {
      "epoch": 0.0007481230127982472,
      "grad_norm": 11.132011413574219,
      "learning_rate": 0.000199903784477229,
      "loss": 3.1148,
      "step": 14
    },
    {
      "epoch": 0.0008015603708552649,
      "grad_norm": 13.737192153930664,
      "learning_rate": 0.00019989309386358778,
      "loss": 3.2672,
      "step": 15
    },
    {
      "epoch": 0.0008549977289122826,
      "grad_norm": 8.938203811645508,
      "learning_rate": 0.00019988240324994656,
      "loss": 3.1601,
      "step": 16
    },
    {
      "epoch": 0.0009084350869693002,
      "grad_norm": 7.181746959686279,
      "learning_rate": 0.00019987171263630534,
      "loss": 2.8089,
      "step": 17
    },
    {
      "epoch": 0.0009618724450263179,
      "grad_norm": 6.332230567932129,
      "learning_rate": 0.0001998610220226641,
      "loss": 2.9769,
      "step": 18
    },
    {
      "epoch": 0.0010153098030833356,
      "grad_norm": 9.867545127868652,
      "learning_rate": 0.0001998503314090229,
      "loss": 2.9206,
      "step": 19
    },
    {
      "epoch": 0.0010687471611403531,
      "grad_norm": 4.944268703460693,
      "learning_rate": 0.00019983964079538168,
      "loss": 2.878,
      "step": 20
    },
    {
      "epoch": 0.0011221845191973709,
      "grad_norm": 3.3289639949798584,
      "learning_rate": 0.00019982895018174043,
      "loss": 2.6061,
      "step": 21
    },
    {
      "epoch": 0.0011756218772543884,
      "grad_norm": 3.9966819286346436,
      "learning_rate": 0.0001998182595680992,
      "loss": 2.4531,
      "step": 22
    },
    {
      "epoch": 0.0012290592353114062,
      "grad_norm": 3.5843348503112793,
      "learning_rate": 0.000199807568954458,
      "loss": 2.5386,
      "step": 23
    },
    {
      "epoch": 0.0012824965933684238,
      "grad_norm": 3.203960418701172,
      "learning_rate": 0.00019979687834081677,
      "loss": 2.5833,
      "step": 24
    },
    {
      "epoch": 0.0013359339514254416,
      "grad_norm": 4.620530128479004,
      "learning_rate": 0.00019978618772717555,
      "loss": 2.7913,
      "step": 25
    },
    {
      "epoch": 0.0013893713094824591,
      "grad_norm": 3.376312732696533,
      "learning_rate": 0.00019977549711353433,
      "loss": 2.7973,
      "step": 26
    },
    {
      "epoch": 0.0014428086675394769,
      "grad_norm": 3.6215131282806396,
      "learning_rate": 0.0001997648064998931,
      "loss": 2.6454,
      "step": 27
    },
    {
      "epoch": 0.0014962460255964944,
      "grad_norm": 3.8309831619262695,
      "learning_rate": 0.0001997541158862519,
      "loss": 2.6092,
      "step": 28
    },
    {
      "epoch": 0.0015496833836535122,
      "grad_norm": 4.693153381347656,
      "learning_rate": 0.00019974342527261064,
      "loss": 2.5944,
      "step": 29
    },
    {
      "epoch": 0.0016031207417105298,
      "grad_norm": 3.486189365386963,
      "learning_rate": 0.00019973273465896945,
      "loss": 2.4438,
      "step": 30
    },
    {
      "epoch": 0.0016565580997675476,
      "grad_norm": 3.6962625980377197,
      "learning_rate": 0.00019972204404532823,
      "loss": 2.7958,
      "step": 31
    },
    {
      "epoch": 0.0017099954578245651,
      "grad_norm": 2.6351699829101562,
      "learning_rate": 0.00019971135343168698,
      "loss": 2.5024,
      "step": 32
    },
    {
      "epoch": 0.0017634328158815829,
      "grad_norm": 2.681365966796875,
      "learning_rate": 0.00019970066281804576,
      "loss": 2.5983,
      "step": 33
    },
    {
      "epoch": 0.0018168701739386004,
      "grad_norm": 2.860957384109497,
      "learning_rate": 0.00019968997220440457,
      "loss": 2.5358,
      "step": 34
    },
    {
      "epoch": 0.0018703075319956182,
      "grad_norm": 2.709834337234497,
      "learning_rate": 0.00019967928159076332,
      "loss": 2.4489,
      "step": 35
    },
    {
      "epoch": 0.0019237448900526358,
      "grad_norm": 2.413336753845215,
      "learning_rate": 0.0001996685909771221,
      "loss": 2.7474,
      "step": 36
    },
    {
      "epoch": 0.0019771822481096536,
      "grad_norm": 2.816228151321411,
      "learning_rate": 0.00019965790036348088,
      "loss": 2.6311,
      "step": 37
    },
    {
      "epoch": 0.002030619606166671,
      "grad_norm": 1.9335052967071533,
      "learning_rate": 0.00019964720974983966,
      "loss": 2.4506,
      "step": 38
    },
    {
      "epoch": 0.0020840569642236887,
      "grad_norm": 2.1110637187957764,
      "learning_rate": 0.00019963651913619844,
      "loss": 2.4318,
      "step": 39
    },
    {
      "epoch": 0.0021374943222807062,
      "grad_norm": 3.6771485805511475,
      "learning_rate": 0.0001996258285225572,
      "loss": 2.5202,
      "step": 40
    },
    {
      "epoch": 0.0021909316803377242,
      "grad_norm": 2.4947004318237305,
      "learning_rate": 0.00019961513790891597,
      "loss": 2.3713,
      "step": 41
    },
    {
      "epoch": 0.0022443690383947418,
      "grad_norm": 3.3227171897888184,
      "learning_rate": 0.00019960444729527478,
      "loss": 2.4246,
      "step": 42
    },
    {
      "epoch": 0.0022978063964517593,
      "grad_norm": 3.305040121078491,
      "learning_rate": 0.00019959375668163353,
      "loss": 2.3929,
      "step": 43
    },
    {
      "epoch": 0.002351243754508777,
      "grad_norm": 3.740335464477539,
      "learning_rate": 0.0001995830660679923,
      "loss": 2.4188,
      "step": 44
    },
    {
      "epoch": 0.002404681112565795,
      "grad_norm": 2.857349157333374,
      "learning_rate": 0.0001995723754543511,
      "loss": 2.4117,
      "step": 45
    },
    {
      "epoch": 0.0024581184706228124,
      "grad_norm": 3.1033103466033936,
      "learning_rate": 0.00019956168484070987,
      "loss": 2.6588,
      "step": 46
    },
    {
      "epoch": 0.00251155582867983,
      "grad_norm": 3.260343551635742,
      "learning_rate": 0.00019955099422706865,
      "loss": 2.5025,
      "step": 47
    },
    {
      "epoch": 0.0025649931867368476,
      "grad_norm": 3.641688346862793,
      "learning_rate": 0.00019954030361342743,
      "loss": 2.4434,
      "step": 48
    },
    {
      "epoch": 0.0026184305447938656,
      "grad_norm": 2.715123176574707,
      "learning_rate": 0.00019952961299978618,
      "loss": 2.3227,
      "step": 49
    },
    {
      "epoch": 0.002671867902850883,
      "grad_norm": 2.881333112716675,
      "learning_rate": 0.00019951892238614499,
      "loss": 2.3485,
      "step": 50
    },
    {
      "epoch": 0.0027253052609079007,
      "grad_norm": 3.0994696617126465,
      "learning_rate": 0.00019950823177250374,
      "loss": 2.4613,
      "step": 51
    },
    {
      "epoch": 0.0027787426189649182,
      "grad_norm": 2.9128899574279785,
      "learning_rate": 0.00019949754115886252,
      "loss": 2.2512,
      "step": 52
    },
    {
      "epoch": 0.0028321799770219362,
      "grad_norm": 3.4108967781066895,
      "learning_rate": 0.00019948685054522132,
      "loss": 2.7673,
      "step": 53
    },
    {
      "epoch": 0.0028856173350789538,
      "grad_norm": 3.5357625484466553,
      "learning_rate": 0.00019947615993158008,
      "loss": 2.2592,
      "step": 54
    },
    {
      "epoch": 0.0029390546931359713,
      "grad_norm": 2.1669394969940186,
      "learning_rate": 0.00019946546931793886,
      "loss": 2.2161,
      "step": 55
    },
    {
      "epoch": 0.002992492051192989,
      "grad_norm": 2.566709041595459,
      "learning_rate": 0.00019945477870429764,
      "loss": 2.3943,
      "step": 56
    },
    {
      "epoch": 0.003045929409250007,
      "grad_norm": 2.169023036956787,
      "learning_rate": 0.00019944408809065642,
      "loss": 2.4839,
      "step": 57
    },
    {
      "epoch": 0.0030993667673070244,
      "grad_norm": 2.2498955726623535,
      "learning_rate": 0.0001994333974770152,
      "loss": 2.1503,
      "step": 58
    },
    {
      "epoch": 0.003152804125364042,
      "grad_norm": 1.9470359086990356,
      "learning_rate": 0.00019942270686337398,
      "loss": 2.0655,
      "step": 59
    },
    {
      "epoch": 0.0032062414834210596,
      "grad_norm": 1.4817441701889038,
      "learning_rate": 0.00019941201624973273,
      "loss": 2.1665,
      "step": 60
    },
    {
      "epoch": 0.003259678841478077,
      "grad_norm": 2.762702703475952,
      "learning_rate": 0.00019940132563609153,
      "loss": 2.6132,
      "step": 61
    },
    {
      "epoch": 0.003313116199535095,
      "grad_norm": 1.7849444150924683,
      "learning_rate": 0.00019939063502245031,
      "loss": 2.1542,
      "step": 62
    },
    {
      "epoch": 0.0033665535575921127,
      "grad_norm": 2.055856466293335,
      "learning_rate": 0.00019937994440880907,
      "loss": 2.2057,
      "step": 63
    },
    {
      "epoch": 0.0034199909156491302,
      "grad_norm": 2.6720077991485596,
      "learning_rate": 0.00019936925379516785,
      "loss": 2.3751,
      "step": 64
    },
    {
      "epoch": 0.0034734282737061478,
      "grad_norm": 2.4066174030303955,
      "learning_rate": 0.00019935856318152663,
      "loss": 2.3271,
      "step": 65
    },
    {
      "epoch": 0.0035268656317631658,
      "grad_norm": 2.374605178833008,
      "learning_rate": 0.0001993478725678854,
      "loss": 2.3435,
      "step": 66
    },
    {
      "epoch": 0.0035803029898201833,
      "grad_norm": 2.8954312801361084,
      "learning_rate": 0.00019933718195424419,
      "loss": 2.404,
      "step": 67
    },
    {
      "epoch": 0.003633740347877201,
      "grad_norm": 4.533381938934326,
      "learning_rate": 0.00019932649134060294,
      "loss": 2.6031,
      "step": 68
    },
    {
      "epoch": 0.0036871777059342184,
      "grad_norm": 1.7986892461776733,
      "learning_rate": 0.00019931580072696174,
      "loss": 2.1513,
      "step": 69
    },
    {
      "epoch": 0.0037406150639912364,
      "grad_norm": 3.086799383163452,
      "learning_rate": 0.00019930511011332052,
      "loss": 2.695,
      "step": 70
    },
    {
      "epoch": 0.003794052422048254,
      "grad_norm": 2.4031572341918945,
      "learning_rate": 0.00019929441949967928,
      "loss": 2.4092,
      "step": 71
    },
    {
      "epoch": 0.0038474897801052716,
      "grad_norm": 2.7464113235473633,
      "learning_rate": 0.00019928372888603808,
      "loss": 2.2453,
      "step": 72
    },
    {
      "epoch": 0.003900927138162289,
      "grad_norm": 4.422228813171387,
      "learning_rate": 0.00019927303827239686,
      "loss": 2.3859,
      "step": 73
    },
    {
      "epoch": 0.003954364496219307,
      "grad_norm": 2.501262903213501,
      "learning_rate": 0.00019926234765875562,
      "loss": 2.2071,
      "step": 74
    },
    {
      "epoch": 0.004007801854276324,
      "grad_norm": 2.0280261039733887,
      "learning_rate": 0.0001992516570451144,
      "loss": 2.1383,
      "step": 75
    },
    {
      "epoch": 0.004061239212333342,
      "grad_norm": 1.853374719619751,
      "learning_rate": 0.00019924096643147317,
      "loss": 2.4608,
      "step": 76
    },
    {
      "epoch": 0.00411467657039036,
      "grad_norm": 1.887791395187378,
      "learning_rate": 0.00019923027581783195,
      "loss": 2.2432,
      "step": 77
    },
    {
      "epoch": 0.004168113928447377,
      "grad_norm": 2.1457149982452393,
      "learning_rate": 0.00019921958520419073,
      "loss": 2.44,
      "step": 78
    },
    {
      "epoch": 0.004221551286504395,
      "grad_norm": 2.7834243774414062,
      "learning_rate": 0.0001992088945905495,
      "loss": 2.3315,
      "step": 79
    },
    {
      "epoch": 0.0042749886445614125,
      "grad_norm": 2.035062074661255,
      "learning_rate": 0.0001991982039769083,
      "loss": 2.1417,
      "step": 80
    },
    {
      "epoch": 0.0043284260026184304,
      "grad_norm": 2.0203514099121094,
      "learning_rate": 0.00019918751336326707,
      "loss": 2.4737,
      "step": 81
    },
    {
      "epoch": 0.0043818633606754484,
      "grad_norm": 2.110635757446289,
      "learning_rate": 0.00019917682274962583,
      "loss": 2.1977,
      "step": 82
    },
    {
      "epoch": 0.0044353007187324656,
      "grad_norm": 2.089613199234009,
      "learning_rate": 0.0001991661321359846,
      "loss": 2.3423,
      "step": 83
    },
    {
      "epoch": 0.0044887380767894836,
      "grad_norm": 2.3507418632507324,
      "learning_rate": 0.0001991554415223434,
      "loss": 2.4084,
      "step": 84
    },
    {
      "epoch": 0.0045421754348465015,
      "grad_norm": 3.0323617458343506,
      "learning_rate": 0.00019914475090870216,
      "loss": 2.1922,
      "step": 85
    },
    {
      "epoch": 0.004595612792903519,
      "grad_norm": 2.4876773357391357,
      "learning_rate": 0.00019913406029506094,
      "loss": 2.3589,
      "step": 86
    },
    {
      "epoch": 0.004649050150960537,
      "grad_norm": 2.5492403507232666,
      "learning_rate": 0.00019912336968141972,
      "loss": 2.4409,
      "step": 87
    },
    {
      "epoch": 0.004702487509017554,
      "grad_norm": 2.1316730976104736,
      "learning_rate": 0.0001991126790677785,
      "loss": 2.2339,
      "step": 88
    },
    {
      "epoch": 0.004755924867074572,
      "grad_norm": 2.2884395122528076,
      "learning_rate": 0.00019910198845413728,
      "loss": 2.2245,
      "step": 89
    },
    {
      "epoch": 0.00480936222513159,
      "grad_norm": 2.0484306812286377,
      "learning_rate": 0.00019909129784049606,
      "loss": 2.5584,
      "step": 90
    },
    {
      "epoch": 0.004862799583188607,
      "grad_norm": 4.033507823944092,
      "learning_rate": 0.00019908060722685482,
      "loss": 2.5435,
      "step": 91
    },
    {
      "epoch": 0.004916236941245625,
      "grad_norm": 3.3569581508636475,
      "learning_rate": 0.00019906991661321362,
      "loss": 2.7566,
      "step": 92
    },
    {
      "epoch": 0.004969674299302643,
      "grad_norm": 1.7402949333190918,
      "learning_rate": 0.00019905922599957237,
      "loss": 1.9427,
      "step": 93
    },
    {
      "epoch": 0.00502311165735966,
      "grad_norm": 2.9608747959136963,
      "learning_rate": 0.00019904853538593115,
      "loss": 2.3295,
      "step": 94
    },
    {
      "epoch": 0.005076549015416678,
      "grad_norm": 2.370903491973877,
      "learning_rate": 0.00019903784477228996,
      "loss": 2.2999,
      "step": 95
    },
    {
      "epoch": 0.005129986373473695,
      "grad_norm": 2.3415687084198,
      "learning_rate": 0.00019902715415864871,
      "loss": 2.3111,
      "step": 96
    },
    {
      "epoch": 0.005183423731530713,
      "grad_norm": 1.7979177236557007,
      "learning_rate": 0.0001990164635450075,
      "loss": 2.3156,
      "step": 97
    },
    {
      "epoch": 0.005236861089587731,
      "grad_norm": 3.4317069053649902,
      "learning_rate": 0.00019900577293136627,
      "loss": 2.6324,
      "step": 98
    },
    {
      "epoch": 0.005290298447644748,
      "grad_norm": 2.148063898086548,
      "learning_rate": 0.00019899508231772505,
      "loss": 2.3485,
      "step": 99
    },
    {
      "epoch": 0.005343735805701766,
      "grad_norm": 3.534181833267212,
      "learning_rate": 0.00019898439170408383,
      "loss": 2.5574,
      "step": 100
    },
    {
      "epoch": 0.005397173163758783,
      "grad_norm": 2.7186295986175537,
      "learning_rate": 0.0001989737010904426,
      "loss": 2.2784,
      "step": 101
    },
    {
      "epoch": 0.005450610521815801,
      "grad_norm": 1.6162762641906738,
      "learning_rate": 0.00019896301047680136,
      "loss": 2.0946,
      "step": 102
    },
    {
      "epoch": 0.005504047879872819,
      "grad_norm": 1.6976977586746216,
      "learning_rate": 0.00019895231986316017,
      "loss": 2.2792,
      "step": 103
    },
    {
      "epoch": 0.0055574852379298365,
      "grad_norm": 1.8707302808761597,
      "learning_rate": 0.00019894162924951892,
      "loss": 2.3543,
      "step": 104
    },
    {
      "epoch": 0.0056109225959868544,
      "grad_norm": 2.0895769596099854,
      "learning_rate": 0.0001989309386358777,
      "loss": 2.3004,
      "step": 105
    },
    {
      "epoch": 0.0056643599540438724,
      "grad_norm": 2.0797436237335205,
      "learning_rate": 0.00019892024802223648,
      "loss": 2.2402,
      "step": 106
    },
    {
      "epoch": 0.0057177973121008896,
      "grad_norm": 2.4910085201263428,
      "learning_rate": 0.00019890955740859526,
      "loss": 2.3601,
      "step": 107
    },
    {
      "epoch": 0.0057712346701579076,
      "grad_norm": 2.357196807861328,
      "learning_rate": 0.00019889886679495404,
      "loss": 2.3339,
      "step": 108
    },
    {
      "epoch": 0.005824672028214925,
      "grad_norm": 1.7829447984695435,
      "learning_rate": 0.00019888817618131282,
      "loss": 2.2579,
      "step": 109
    },
    {
      "epoch": 0.005878109386271943,
      "grad_norm": 1.9549471139907837,
      "learning_rate": 0.00019887748556767157,
      "loss": 2.2803,
      "step": 110
    },
    {
      "epoch": 0.005931546744328961,
      "grad_norm": 2.221635341644287,
      "learning_rate": 0.00019886679495403038,
      "loss": 2.3409,
      "step": 111
    },
    {
      "epoch": 0.005984984102385978,
      "grad_norm": 2.5541090965270996,
      "learning_rate": 0.00019885610434038916,
      "loss": 2.4929,
      "step": 112
    },
    {
      "epoch": 0.006038421460442996,
      "grad_norm": 2.1521177291870117,
      "learning_rate": 0.0001988454137267479,
      "loss": 2.3489,
      "step": 113
    },
    {
      "epoch": 0.006091858818500014,
      "grad_norm": 2.678241014480591,
      "learning_rate": 0.0001988347231131067,
      "loss": 2.2437,
      "step": 114
    },
    {
      "epoch": 0.006145296176557031,
      "grad_norm": 2.028127908706665,
      "learning_rate": 0.00019882403249946547,
      "loss": 2.2929,
      "step": 115
    },
    {
      "epoch": 0.006198733534614049,
      "grad_norm": 2.691897392272949,
      "learning_rate": 0.00019881334188582425,
      "loss": 2.3493,
      "step": 116
    },
    {
      "epoch": 0.006252170892671066,
      "grad_norm": 2.0189437866210938,
      "learning_rate": 0.00019880265127218303,
      "loss": 2.2464,
      "step": 117
    },
    {
      "epoch": 0.006305608250728084,
      "grad_norm": 3.2763943672180176,
      "learning_rate": 0.0001987919606585418,
      "loss": 2.5909,
      "step": 118
    },
    {
      "epoch": 0.006359045608785102,
      "grad_norm": 2.2224647998809814,
      "learning_rate": 0.0001987812700449006,
      "loss": 2.4513,
      "step": 119
    },
    {
      "epoch": 0.006412482966842119,
      "grad_norm": 1.892526388168335,
      "learning_rate": 0.00019877057943125937,
      "loss": 2.3511,
      "step": 120
    },
    {
      "epoch": 0.006465920324899137,
      "grad_norm": 3.0634074211120605,
      "learning_rate": 0.00019875988881761812,
      "loss": 2.5005,
      "step": 121
    },
    {
      "epoch": 0.006519357682956154,
      "grad_norm": 3.1696016788482666,
      "learning_rate": 0.00019874919820397693,
      "loss": 2.5338,
      "step": 122
    },
    {
      "epoch": 0.006572795041013172,
      "grad_norm": 2.0189437866210938,
      "learning_rate": 0.0001987385075903357,
      "loss": 2.2577,
      "step": 123
    },
    {
      "epoch": 0.00662623239907019,
      "grad_norm": 2.5295908451080322,
      "learning_rate": 0.00019872781697669446,
      "loss": 2.4013,
      "step": 124
    },
    {
      "epoch": 0.006679669757127207,
      "grad_norm": 2.1718509197235107,
      "learning_rate": 0.00019871712636305324,
      "loss": 2.1857,
      "step": 125
    },
    {
      "epoch": 0.006733107115184225,
      "grad_norm": 2.6861414909362793,
      "learning_rate": 0.00019870643574941202,
      "loss": 2.6086,
      "step": 126
    },
    {
      "epoch": 0.006786544473241243,
      "grad_norm": 1.8559672832489014,
      "learning_rate": 0.0001986957451357708,
      "loss": 2.2779,
      "step": 127
    },
    {
      "epoch": 0.0068399818312982604,
      "grad_norm": 1.7880313396453857,
      "learning_rate": 0.00019868505452212958,
      "loss": 2.555,
      "step": 128
    },
    {
      "epoch": 0.0068934191893552784,
      "grad_norm": 2.1239988803863525,
      "learning_rate": 0.00019867436390848836,
      "loss": 2.31,
      "step": 129
    },
    {
      "epoch": 0.0069468565474122956,
      "grad_norm": 1.6436840295791626,
      "learning_rate": 0.00019866367329484714,
      "loss": 2.255,
      "step": 130
    },
    {
      "epoch": 0.0070002939054693136,
      "grad_norm": 1.931606411933899,
      "learning_rate": 0.00019865298268120592,
      "loss": 2.3587,
      "step": 131
    },
    {
      "epoch": 0.0070537312635263315,
      "grad_norm": 2.388674259185791,
      "learning_rate": 0.00019864229206756467,
      "loss": 2.4263,
      "step": 132
    },
    {
      "epoch": 0.007107168621583349,
      "grad_norm": 2.4609148502349854,
      "learning_rate": 0.00019863160145392345,
      "loss": 2.0797,
      "step": 133
    },
    {
      "epoch": 0.007160605979640367,
      "grad_norm": 1.9375379085540771,
      "learning_rate": 0.00019862091084028226,
      "loss": 2.4952,
      "step": 134
    },
    {
      "epoch": 0.007214043337697385,
      "grad_norm": 2.3791916370391846,
      "learning_rate": 0.000198610220226641,
      "loss": 2.4797,
      "step": 135
    },
    {
      "epoch": 0.007267480695754402,
      "grad_norm": 2.1189119815826416,
      "learning_rate": 0.0001985995296129998,
      "loss": 2.6165,
      "step": 136
    },
    {
      "epoch": 0.00732091805381142,
      "grad_norm": 2.1463654041290283,
      "learning_rate": 0.00019858883899935857,
      "loss": 2.2578,
      "step": 137
    },
    {
      "epoch": 0.007374355411868437,
      "grad_norm": 1.3656697273254395,
      "learning_rate": 0.00019857814838571735,
      "loss": 2.09,
      "step": 138
    },
    {
      "epoch": 0.007427792769925455,
      "grad_norm": 3.0640625953674316,
      "learning_rate": 0.00019856745777207613,
      "loss": 2.5065,
      "step": 139
    },
    {
      "epoch": 0.007481230127982473,
      "grad_norm": 1.8807659149169922,
      "learning_rate": 0.0001985567671584349,
      "loss": 2.4108,
      "step": 140
    },
    {
      "epoch": 0.00753466748603949,
      "grad_norm": 1.5126312971115112,
      "learning_rate": 0.0001985460765447937,
      "loss": 2.1425,
      "step": 141
    },
    {
      "epoch": 0.007588104844096508,
      "grad_norm": 1.6577357053756714,
      "learning_rate": 0.00019853538593115247,
      "loss": 2.4614,
      "step": 142
    },
    {
      "epoch": 0.007641542202153525,
      "grad_norm": 1.877882480621338,
      "learning_rate": 0.00019852469531751122,
      "loss": 2.2686,
      "step": 143
    },
    {
      "epoch": 0.007694979560210543,
      "grad_norm": 1.9157358407974243,
      "learning_rate": 0.00019851400470387,
      "loss": 2.0299,
      "step": 144
    },
    {
      "epoch": 0.007748416918267561,
      "grad_norm": 1.752771258354187,
      "learning_rate": 0.0001985033140902288,
      "loss": 2.3542,
      "step": 145
    },
    {
      "epoch": 0.007801854276324578,
      "grad_norm": 2.1627349853515625,
      "learning_rate": 0.00019849262347658756,
      "loss": 2.2514,
      "step": 146
    },
    {
      "epoch": 0.007855291634381596,
      "grad_norm": 1.1296221017837524,
      "learning_rate": 0.00019848193286294634,
      "loss": 2.0682,
      "step": 147
    },
    {
      "epoch": 0.007908728992438614,
      "grad_norm": 1.6073658466339111,
      "learning_rate": 0.00019847124224930512,
      "loss": 2.1776,
      "step": 148
    },
    {
      "epoch": 0.007962166350495632,
      "grad_norm": 1.6716794967651367,
      "learning_rate": 0.0001984605516356639,
      "loss": 2.4486,
      "step": 149
    },
    {
      "epoch": 0.008015603708552648,
      "grad_norm": 1.9482555389404297,
      "learning_rate": 0.00019844986102202268,
      "loss": 2.3839,
      "step": 150
    },
    {
      "epoch": 0.008069041066609666,
      "grad_norm": 1.4163340330123901,
      "learning_rate": 0.00019843917040838146,
      "loss": 2.1088,
      "step": 151
    },
    {
      "epoch": 0.008122478424666684,
      "grad_norm": 1.71939218044281,
      "learning_rate": 0.0001984284797947402,
      "loss": 2.2511,
      "step": 152
    },
    {
      "epoch": 0.008175915782723702,
      "grad_norm": 1.5992496013641357,
      "learning_rate": 0.00019841778918109902,
      "loss": 2.4163,
      "step": 153
    },
    {
      "epoch": 0.00822935314078072,
      "grad_norm": 1.8481218814849854,
      "learning_rate": 0.00019840709856745777,
      "loss": 2.271,
      "step": 154
    },
    {
      "epoch": 0.008282790498837737,
      "grad_norm": 1.4616034030914307,
      "learning_rate": 0.00019839640795381655,
      "loss": 2.1638,
      "step": 155
    },
    {
      "epoch": 0.008336227856894755,
      "grad_norm": 2.3104922771453857,
      "learning_rate": 0.00019838571734017533,
      "loss": 2.3733,
      "step": 156
    },
    {
      "epoch": 0.008389665214951773,
      "grad_norm": 1.8859935998916626,
      "learning_rate": 0.0001983750267265341,
      "loss": 2.0947,
      "step": 157
    },
    {
      "epoch": 0.00844310257300879,
      "grad_norm": 1.828004002571106,
      "learning_rate": 0.0001983643361128929,
      "loss": 2.0471,
      "step": 158
    },
    {
      "epoch": 0.008496539931065809,
      "grad_norm": 2.4341464042663574,
      "learning_rate": 0.00019835364549925167,
      "loss": 2.2798,
      "step": 159
    },
    {
      "epoch": 0.008549977289122825,
      "grad_norm": 2.6948931217193604,
      "learning_rate": 0.00019834295488561042,
      "loss": 2.4895,
      "step": 160
    },
    {
      "epoch": 0.008603414647179843,
      "grad_norm": 1.510308027267456,
      "learning_rate": 0.00019833226427196923,
      "loss": 2.2279,
      "step": 161
    },
    {
      "epoch": 0.008656852005236861,
      "grad_norm": 1.9699573516845703,
      "learning_rate": 0.000198321573658328,
      "loss": 2.2115,
      "step": 162
    },
    {
      "epoch": 0.008710289363293879,
      "grad_norm": 1.791363000869751,
      "learning_rate": 0.00019831088304468676,
      "loss": 2.0016,
      "step": 163
    },
    {
      "epoch": 0.008763726721350897,
      "grad_norm": 2.740567684173584,
      "learning_rate": 0.00019830019243104557,
      "loss": 2.2513,
      "step": 164
    },
    {
      "epoch": 0.008817164079407915,
      "grad_norm": 1.9829981327056885,
      "learning_rate": 0.00019828950181740432,
      "loss": 2.2649,
      "step": 165
    },
    {
      "epoch": 0.008870601437464931,
      "grad_norm": 2.47855544090271,
      "learning_rate": 0.0001982788112037631,
      "loss": 2.4758,
      "step": 166
    },
    {
      "epoch": 0.008924038795521949,
      "grad_norm": 2.771904706954956,
      "learning_rate": 0.00019826812059012188,
      "loss": 2.3403,
      "step": 167
    },
    {
      "epoch": 0.008977476153578967,
      "grad_norm": 1.563524842262268,
      "learning_rate": 0.00019825742997648066,
      "loss": 2.219,
      "step": 168
    },
    {
      "epoch": 0.009030913511635985,
      "grad_norm": 2.464757204055786,
      "learning_rate": 0.00019824673936283944,
      "loss": 2.3529,
      "step": 169
    },
    {
      "epoch": 0.009084350869693003,
      "grad_norm": 2.588552474975586,
      "learning_rate": 0.00019823604874919822,
      "loss": 2.5771,
      "step": 170
    },
    {
      "epoch": 0.00913778822775002,
      "grad_norm": 2.417567253112793,
      "learning_rate": 0.00019822535813555697,
      "loss": 2.0823,
      "step": 171
    },
    {
      "epoch": 0.009191225585807037,
      "grad_norm": 1.8058382272720337,
      "learning_rate": 0.00019821466752191578,
      "loss": 2.3618,
      "step": 172
    },
    {
      "epoch": 0.009244662943864055,
      "grad_norm": 2.4998278617858887,
      "learning_rate": 0.00019820397690827456,
      "loss": 2.6167,
      "step": 173
    },
    {
      "epoch": 0.009298100301921073,
      "grad_norm": 2.038851261138916,
      "learning_rate": 0.0001981932862946333,
      "loss": 2.3543,
      "step": 174
    },
    {
      "epoch": 0.009351537659978091,
      "grad_norm": 2.487422466278076,
      "learning_rate": 0.0001981825956809921,
      "loss": 2.068,
      "step": 175
    },
    {
      "epoch": 0.009404975018035108,
      "grad_norm": 1.7561699151992798,
      "learning_rate": 0.0001981719050673509,
      "loss": 2.0829,
      "step": 176
    },
    {
      "epoch": 0.009458412376092126,
      "grad_norm": 1.5048489570617676,
      "learning_rate": 0.00019816121445370965,
      "loss": 2.1128,
      "step": 177
    },
    {
      "epoch": 0.009511849734149144,
      "grad_norm": 1.6518044471740723,
      "learning_rate": 0.00019815052384006843,
      "loss": 2.2952,
      "step": 178
    },
    {
      "epoch": 0.009565287092206162,
      "grad_norm": 1.6791489124298096,
      "learning_rate": 0.0001981398332264272,
      "loss": 2.2058,
      "step": 179
    },
    {
      "epoch": 0.00961872445026318,
      "grad_norm": 1.806159496307373,
      "learning_rate": 0.000198129142612786,
      "loss": 2.2239,
      "step": 180
    },
    {
      "epoch": 0.009672161808320196,
      "grad_norm": 2.2939858436584473,
      "learning_rate": 0.00019811845199914477,
      "loss": 2.2677,
      "step": 181
    },
    {
      "epoch": 0.009725599166377214,
      "grad_norm": 1.5588476657867432,
      "learning_rate": 0.00019810776138550352,
      "loss": 2.2894,
      "step": 182
    },
    {
      "epoch": 0.009779036524434232,
      "grad_norm": 2.135014295578003,
      "learning_rate": 0.00019809707077186233,
      "loss": 2.3482,
      "step": 183
    },
    {
      "epoch": 0.00983247388249125,
      "grad_norm": 3.130028486251831,
      "learning_rate": 0.0001980863801582211,
      "loss": 2.4595,
      "step": 184
    },
    {
      "epoch": 0.009885911240548268,
      "grad_norm": 2.3617007732391357,
      "learning_rate": 0.00019807568954457986,
      "loss": 2.2736,
      "step": 185
    },
    {
      "epoch": 0.009939348598605286,
      "grad_norm": 1.951005220413208,
      "learning_rate": 0.00019806499893093864,
      "loss": 2.3125,
      "step": 186
    },
    {
      "epoch": 0.009992785956662302,
      "grad_norm": 1.8079955577850342,
      "learning_rate": 0.00019805430831729744,
      "loss": 2.2513,
      "step": 187
    },
    {
      "epoch": 0.01004622331471932,
      "grad_norm": 1.51261305809021,
      "learning_rate": 0.0001980436177036562,
      "loss": 2.1954,
      "step": 188
    },
    {
      "epoch": 0.010099660672776338,
      "grad_norm": 1.9357469081878662,
      "learning_rate": 0.00019803292709001498,
      "loss": 2.267,
      "step": 189
    },
    {
      "epoch": 0.010153098030833356,
      "grad_norm": 1.9593473672866821,
      "learning_rate": 0.00019802223647637376,
      "loss": 2.2414,
      "step": 190
    },
    {
      "epoch": 0.010206535388890374,
      "grad_norm": 1.5358587503433228,
      "learning_rate": 0.00019801154586273254,
      "loss": 2.2082,
      "step": 191
    },
    {
      "epoch": 0.01025997274694739,
      "grad_norm": 1.7860612869262695,
      "learning_rate": 0.00019800085524909132,
      "loss": 2.2528,
      "step": 192
    },
    {
      "epoch": 0.010313410105004408,
      "grad_norm": 1.4999393224716187,
      "learning_rate": 0.00019799016463545007,
      "loss": 2.1751,
      "step": 193
    },
    {
      "epoch": 0.010366847463061426,
      "grad_norm": 1.9045521020889282,
      "learning_rate": 0.00019797947402180885,
      "loss": 1.9488,
      "step": 194
    },
    {
      "epoch": 0.010420284821118444,
      "grad_norm": 1.627297282218933,
      "learning_rate": 0.00019796878340816766,
      "loss": 2.211,
      "step": 195
    },
    {
      "epoch": 0.010473722179175462,
      "grad_norm": 1.5917466878890991,
      "learning_rate": 0.0001979580927945264,
      "loss": 2.1858,
      "step": 196
    },
    {
      "epoch": 0.010527159537232478,
      "grad_norm": 1.4751759767532349,
      "learning_rate": 0.0001979474021808852,
      "loss": 2.0659,
      "step": 197
    },
    {
      "epoch": 0.010580596895289496,
      "grad_norm": 1.794964075088501,
      "learning_rate": 0.00019793671156724397,
      "loss": 2.1209,
      "step": 198
    },
    {
      "epoch": 0.010634034253346514,
      "grad_norm": 1.8415664434432983,
      "learning_rate": 0.00019792602095360275,
      "loss": 2.3496,
      "step": 199
    },
    {
      "epoch": 0.010687471611403532,
      "grad_norm": 1.5526354312896729,
      "learning_rate": 0.00019791533033996153,
      "loss": 2.0418,
      "step": 200
    },
    {
      "epoch": 0.01074090896946055,
      "grad_norm": 1.6219415664672852,
      "learning_rate": 0.0001979046397263203,
      "loss": 2.1485,
      "step": 201
    },
    {
      "epoch": 0.010794346327517567,
      "grad_norm": 2.909191846847534,
      "learning_rate": 0.00019789394911267906,
      "loss": 2.3218,
      "step": 202
    },
    {
      "epoch": 0.010847783685574585,
      "grad_norm": 1.991490364074707,
      "learning_rate": 0.00019788325849903787,
      "loss": 2.1348,
      "step": 203
    },
    {
      "epoch": 0.010901221043631603,
      "grad_norm": 1.8083890676498413,
      "learning_rate": 0.00019787256788539664,
      "loss": 2.2096,
      "step": 204
    },
    {
      "epoch": 0.01095465840168862,
      "grad_norm": 2.298269510269165,
      "learning_rate": 0.0001978618772717554,
      "loss": 2.0963,
      "step": 205
    },
    {
      "epoch": 0.011008095759745639,
      "grad_norm": 2.2239108085632324,
      "learning_rate": 0.0001978511866581142,
      "loss": 2.2159,
      "step": 206
    },
    {
      "epoch": 0.011061533117802657,
      "grad_norm": 2.3935747146606445,
      "learning_rate": 0.00019784049604447296,
      "loss": 2.4161,
      "step": 207
    },
    {
      "epoch": 0.011114970475859673,
      "grad_norm": 1.877626895904541,
      "learning_rate": 0.00019782980543083174,
      "loss": 2.3907,
      "step": 208
    },
    {
      "epoch": 0.011168407833916691,
      "grad_norm": 2.1585710048675537,
      "learning_rate": 0.00019781911481719052,
      "loss": 2.2906,
      "step": 209
    },
    {
      "epoch": 0.011221845191973709,
      "grad_norm": 2.2746617794036865,
      "learning_rate": 0.0001978084242035493,
      "loss": 2.5741,
      "step": 210
    },
    {
      "epoch": 0.011275282550030727,
      "grad_norm": 2.2482640743255615,
      "learning_rate": 0.00019779773358990808,
      "loss": 2.2525,
      "step": 211
    },
    {
      "epoch": 0.011328719908087745,
      "grad_norm": 2.741502285003662,
      "learning_rate": 0.00019778704297626685,
      "loss": 2.3312,
      "step": 212
    },
    {
      "epoch": 0.011382157266144761,
      "grad_norm": 1.865246295928955,
      "learning_rate": 0.0001977763523626256,
      "loss": 2.2199,
      "step": 213
    },
    {
      "epoch": 0.011435594624201779,
      "grad_norm": 1.6434811353683472,
      "learning_rate": 0.00019776566174898441,
      "loss": 2.0947,
      "step": 214
    },
    {
      "epoch": 0.011489031982258797,
      "grad_norm": 2.6523470878601074,
      "learning_rate": 0.0001977549711353432,
      "loss": 2.1767,
      "step": 215
    },
    {
      "epoch": 0.011542469340315815,
      "grad_norm": 1.6652511358261108,
      "learning_rate": 0.00019774428052170195,
      "loss": 2.2073,
      "step": 216
    },
    {
      "epoch": 0.011595906698372833,
      "grad_norm": 2.195754051208496,
      "learning_rate": 0.00019773358990806073,
      "loss": 2.3311,
      "step": 217
    },
    {
      "epoch": 0.01164934405642985,
      "grad_norm": 3.438298463821411,
      "learning_rate": 0.0001977228992944195,
      "loss": 2.5587,
      "step": 218
    },
    {
      "epoch": 0.011702781414486867,
      "grad_norm": 1.3485708236694336,
      "learning_rate": 0.00019771220868077829,
      "loss": 2.1136,
      "step": 219
    },
    {
      "epoch": 0.011756218772543885,
      "grad_norm": 3.2579479217529297,
      "learning_rate": 0.00019770151806713706,
      "loss": 2.2569,
      "step": 220
    },
    {
      "epoch": 0.011809656130600903,
      "grad_norm": 2.0421736240386963,
      "learning_rate": 0.00019769082745349582,
      "loss": 2.2771,
      "step": 221
    },
    {
      "epoch": 0.011863093488657921,
      "grad_norm": 2.9052767753601074,
      "learning_rate": 0.00019768013683985462,
      "loss": 2.5058,
      "step": 222
    },
    {
      "epoch": 0.011916530846714938,
      "grad_norm": 1.5456726551055908,
      "learning_rate": 0.0001976694462262134,
      "loss": 2.2721,
      "step": 223
    },
    {
      "epoch": 0.011969968204771956,
      "grad_norm": 1.706896185874939,
      "learning_rate": 0.00019765875561257216,
      "loss": 2.2572,
      "step": 224
    },
    {
      "epoch": 0.012023405562828974,
      "grad_norm": 1.8504962921142578,
      "learning_rate": 0.00019764806499893094,
      "loss": 2.1956,
      "step": 225
    },
    {
      "epoch": 0.012076842920885992,
      "grad_norm": 1.4890812635421753,
      "learning_rate": 0.00019763737438528974,
      "loss": 2.2131,
      "step": 226
    },
    {
      "epoch": 0.01213028027894301,
      "grad_norm": 1.6949875354766846,
      "learning_rate": 0.0001976266837716485,
      "loss": 2.1467,
      "step": 227
    },
    {
      "epoch": 0.012183717637000028,
      "grad_norm": 1.5813792943954468,
      "learning_rate": 0.00019761599315800728,
      "loss": 1.8115,
      "step": 228
    },
    {
      "epoch": 0.012237154995057044,
      "grad_norm": 2.0067131519317627,
      "learning_rate": 0.00019760530254436605,
      "loss": 2.3508,
      "step": 229
    },
    {
      "epoch": 0.012290592353114062,
      "grad_norm": 1.1246662139892578,
      "learning_rate": 0.00019759461193072483,
      "loss": 2.1677,
      "step": 230
    },
    {
      "epoch": 0.01234402971117108,
      "grad_norm": 1.7800709009170532,
      "learning_rate": 0.00019758392131708361,
      "loss": 2.3487,
      "step": 231
    },
    {
      "epoch": 0.012397467069228098,
      "grad_norm": 1.4524718523025513,
      "learning_rate": 0.0001975732307034424,
      "loss": 2.1984,
      "step": 232
    },
    {
      "epoch": 0.012450904427285116,
      "grad_norm": 2.1085140705108643,
      "learning_rate": 0.00019756254008980117,
      "loss": 2.4997,
      "step": 233
    },
    {
      "epoch": 0.012504341785342132,
      "grad_norm": 1.248300313949585,
      "learning_rate": 0.00019755184947615995,
      "loss": 2.2101,
      "step": 234
    },
    {
      "epoch": 0.01255777914339915,
      "grad_norm": 2.640291929244995,
      "learning_rate": 0.0001975411588625187,
      "loss": 2.6063,
      "step": 235
    },
    {
      "epoch": 0.012611216501456168,
      "grad_norm": 1.8572418689727783,
      "learning_rate": 0.00019753046824887749,
      "loss": 2.2879,
      "step": 236
    },
    {
      "epoch": 0.012664653859513186,
      "grad_norm": 1.9282485246658325,
      "learning_rate": 0.0001975197776352363,
      "loss": 2.0593,
      "step": 237
    },
    {
      "epoch": 0.012718091217570204,
      "grad_norm": 1.3157105445861816,
      "learning_rate": 0.00019750908702159504,
      "loss": 2.2296,
      "step": 238
    },
    {
      "epoch": 0.01277152857562722,
      "grad_norm": 1.6582422256469727,
      "learning_rate": 0.00019749839640795382,
      "loss": 2.407,
      "step": 239
    },
    {
      "epoch": 0.012824965933684238,
      "grad_norm": 1.5244755744934082,
      "learning_rate": 0.0001974877057943126,
      "loss": 2.2811,
      "step": 240
    },
    {
      "epoch": 0.012878403291741256,
      "grad_norm": 1.8624200820922852,
      "learning_rate": 0.00019747701518067138,
      "loss": 2.436,
      "step": 241
    },
    {
      "epoch": 0.012931840649798274,
      "grad_norm": 2.400510787963867,
      "learning_rate": 0.00019746632456703016,
      "loss": 2.4986,
      "step": 242
    },
    {
      "epoch": 0.012985278007855292,
      "grad_norm": 2.416327953338623,
      "learning_rate": 0.00019745563395338894,
      "loss": 2.3875,
      "step": 243
    },
    {
      "epoch": 0.013038715365912308,
      "grad_norm": 1.6085342168807983,
      "learning_rate": 0.0001974449433397477,
      "loss": 2.2275,
      "step": 244
    },
    {
      "epoch": 0.013092152723969326,
      "grad_norm": 1.516238808631897,
      "learning_rate": 0.0001974342527261065,
      "loss": 2.1047,
      "step": 245
    },
    {
      "epoch": 0.013145590082026344,
      "grad_norm": 1.732181191444397,
      "learning_rate": 0.00019742356211246525,
      "loss": 2.361,
      "step": 246
    },
    {
      "epoch": 0.013199027440083362,
      "grad_norm": 1.6561458110809326,
      "learning_rate": 0.00019741287149882403,
      "loss": 2.3346,
      "step": 247
    },
    {
      "epoch": 0.01325246479814038,
      "grad_norm": 1.286171317100525,
      "learning_rate": 0.00019740218088518281,
      "loss": 1.8738,
      "step": 248
    },
    {
      "epoch": 0.013305902156197398,
      "grad_norm": 1.586031198501587,
      "learning_rate": 0.0001973914902715416,
      "loss": 2.318,
      "step": 249
    },
    {
      "epoch": 0.013359339514254415,
      "grad_norm": 1.7317025661468506,
      "learning_rate": 0.00019738079965790037,
      "loss": 2.1821,
      "step": 250
    },
    {
      "epoch": 0.013412776872311433,
      "grad_norm": 6.731567859649658,
      "learning_rate": 0.00019737010904425915,
      "loss": 2.3858,
      "step": 251
    },
    {
      "epoch": 0.01346621423036845,
      "grad_norm": 1.4971562623977661,
      "learning_rate": 0.00019735941843061793,
      "loss": 2.1827,
      "step": 252
    },
    {
      "epoch": 0.013519651588425469,
      "grad_norm": 1.8602439165115356,
      "learning_rate": 0.0001973487278169767,
      "loss": 2.3221,
      "step": 253
    },
    {
      "epoch": 0.013573088946482487,
      "grad_norm": 2.0484297275543213,
      "learning_rate": 0.0001973380372033355,
      "loss": 2.3549,
      "step": 254
    },
    {
      "epoch": 0.013626526304539503,
      "grad_norm": 2.347074508666992,
      "learning_rate": 0.00019732734658969424,
      "loss": 2.262,
      "step": 255
    },
    {
      "epoch": 0.013679963662596521,
      "grad_norm": 2.5021588802337646,
      "learning_rate": 0.00019731665597605305,
      "loss": 2.3532,
      "step": 256
    },
    {
      "epoch": 0.013733401020653539,
      "grad_norm": 1.3558979034423828,
      "learning_rate": 0.0001973059653624118,
      "loss": 2.2396,
      "step": 257
    },
    {
      "epoch": 0.013786838378710557,
      "grad_norm": 1.9154551029205322,
      "learning_rate": 0.00019729527474877058,
      "loss": 2.228,
      "step": 258
    },
    {
      "epoch": 0.013840275736767575,
      "grad_norm": 3.1598494052886963,
      "learning_rate": 0.00019728458413512936,
      "loss": 2.0794,
      "step": 259
    },
    {
      "epoch": 0.013893713094824591,
      "grad_norm": 1.345278024673462,
      "learning_rate": 0.00019727389352148814,
      "loss": 2.3169,
      "step": 260
    },
    {
      "epoch": 0.013947150452881609,
      "grad_norm": 1.6322556734085083,
      "learning_rate": 0.00019726320290784692,
      "loss": 2.1137,
      "step": 261
    },
    {
      "epoch": 0.014000587810938627,
      "grad_norm": 1.275966763496399,
      "learning_rate": 0.0001972525122942057,
      "loss": 2.0456,
      "step": 262
    },
    {
      "epoch": 0.014054025168995645,
      "grad_norm": 1.6600183248519897,
      "learning_rate": 0.00019724182168056445,
      "loss": 2.2741,
      "step": 263
    },
    {
      "epoch": 0.014107462527052663,
      "grad_norm": 1.759407877922058,
      "learning_rate": 0.00019723113106692326,
      "loss": 2.364,
      "step": 264
    },
    {
      "epoch": 0.01416089988510968,
      "grad_norm": 1.834096074104309,
      "learning_rate": 0.00019722044045328204,
      "loss": 2.5592,
      "step": 265
    },
    {
      "epoch": 0.014214337243166697,
      "grad_norm": 1.7968651056289673,
      "learning_rate": 0.0001972097498396408,
      "loss": 2.3194,
      "step": 266
    },
    {
      "epoch": 0.014267774601223715,
      "grad_norm": 1.9356722831726074,
      "learning_rate": 0.00019719905922599957,
      "loss": 2.3197,
      "step": 267
    },
    {
      "epoch": 0.014321211959280733,
      "grad_norm": 1.4751859903335571,
      "learning_rate": 0.00019718836861235835,
      "loss": 2.3349,
      "step": 268
    },
    {
      "epoch": 0.014374649317337751,
      "grad_norm": 1.7063785791397095,
      "learning_rate": 0.00019717767799871713,
      "loss": 2.2312,
      "step": 269
    },
    {
      "epoch": 0.01442808667539477,
      "grad_norm": 2.111907720565796,
      "learning_rate": 0.0001971669873850759,
      "loss": 2.2744,
      "step": 270
    },
    {
      "epoch": 0.014481524033451786,
      "grad_norm": 1.9191499948501587,
      "learning_rate": 0.0001971562967714347,
      "loss": 2.2765,
      "step": 271
    },
    {
      "epoch": 0.014534961391508804,
      "grad_norm": 1.8256696462631226,
      "learning_rate": 0.00019714560615779347,
      "loss": 2.2111,
      "step": 272
    },
    {
      "epoch": 0.014588398749565822,
      "grad_norm": 2.2760839462280273,
      "learning_rate": 0.00019713491554415225,
      "loss": 2.3981,
      "step": 273
    },
    {
      "epoch": 0.01464183610762284,
      "grad_norm": 1.6807100772857666,
      "learning_rate": 0.000197124224930511,
      "loss": 2.2342,
      "step": 274
    },
    {
      "epoch": 0.014695273465679858,
      "grad_norm": 2.155566930770874,
      "learning_rate": 0.0001971135343168698,
      "loss": 2.463,
      "step": 275
    },
    {
      "epoch": 0.014748710823736874,
      "grad_norm": 1.4235788583755493,
      "learning_rate": 0.0001971028437032286,
      "loss": 2.3791,
      "step": 276
    },
    {
      "epoch": 0.014802148181793892,
      "grad_norm": 1.179872989654541,
      "learning_rate": 0.00019709215308958734,
      "loss": 2.0417,
      "step": 277
    },
    {
      "epoch": 0.01485558553985091,
      "grad_norm": 1.2439278364181519,
      "learning_rate": 0.00019708146247594612,
      "loss": 2.111,
      "step": 278
    },
    {
      "epoch": 0.014909022897907928,
      "grad_norm": 1.444427728652954,
      "learning_rate": 0.0001970707718623049,
      "loss": 2.2418,
      "step": 279
    },
    {
      "epoch": 0.014962460255964946,
      "grad_norm": 1.4565761089324951,
      "learning_rate": 0.00019706008124866368,
      "loss": 2.3222,
      "step": 280
    },
    {
      "epoch": 0.015015897614021962,
      "grad_norm": 1.575871229171753,
      "learning_rate": 0.00019704939063502246,
      "loss": 2.3932,
      "step": 281
    },
    {
      "epoch": 0.01506933497207898,
      "grad_norm": 1.6027491092681885,
      "learning_rate": 0.00019703870002138124,
      "loss": 2.2566,
      "step": 282
    },
    {
      "epoch": 0.015122772330135998,
      "grad_norm": 1.521506667137146,
      "learning_rate": 0.00019702800940774002,
      "loss": 2.0971,
      "step": 283
    },
    {
      "epoch": 0.015176209688193016,
      "grad_norm": 1.4854158163070679,
      "learning_rate": 0.0001970173187940988,
      "loss": 2.1723,
      "step": 284
    },
    {
      "epoch": 0.015229647046250034,
      "grad_norm": 1.490531325340271,
      "learning_rate": 0.00019700662818045755,
      "loss": 2.0702,
      "step": 285
    },
    {
      "epoch": 0.01528308440430705,
      "grad_norm": 1.8931411504745483,
      "learning_rate": 0.00019699593756681633,
      "loss": 2.4066,
      "step": 286
    },
    {
      "epoch": 0.015336521762364068,
      "grad_norm": 1.7550147771835327,
      "learning_rate": 0.00019698524695317514,
      "loss": 2.1709,
      "step": 287
    },
    {
      "epoch": 0.015389959120421086,
      "grad_norm": 1.12906813621521,
      "learning_rate": 0.0001969745563395339,
      "loss": 2.0291,
      "step": 288
    },
    {
      "epoch": 0.015443396478478104,
      "grad_norm": 2.241873025894165,
      "learning_rate": 0.00019696386572589267,
      "loss": 2.4844,
      "step": 289
    },
    {
      "epoch": 0.015496833836535122,
      "grad_norm": 2.7168161869049072,
      "learning_rate": 0.00019695317511225145,
      "loss": 2.4032,
      "step": 290
    },
    {
      "epoch": 0.01555027119459214,
      "grad_norm": 2.783116340637207,
      "learning_rate": 0.00019694248449861023,
      "loss": 2.4211,
      "step": 291
    },
    {
      "epoch": 0.015603708552649156,
      "grad_norm": 2.0058441162109375,
      "learning_rate": 0.000196931793884969,
      "loss": 2.4713,
      "step": 292
    },
    {
      "epoch": 0.015657145910706174,
      "grad_norm": 2.6176083087921143,
      "learning_rate": 0.0001969211032713278,
      "loss": 2.3305,
      "step": 293
    },
    {
      "epoch": 0.015710583268763192,
      "grad_norm": 1.164573073387146,
      "learning_rate": 0.00019691041265768657,
      "loss": 2.1476,
      "step": 294
    },
    {
      "epoch": 0.01576402062682021,
      "grad_norm": 1.9512273073196411,
      "learning_rate": 0.00019689972204404535,
      "loss": 2.2602,
      "step": 295
    },
    {
      "epoch": 0.01581745798487723,
      "grad_norm": 1.9704699516296387,
      "learning_rate": 0.0001968890314304041,
      "loss": 2.3751,
      "step": 296
    },
    {
      "epoch": 0.015870895342934246,
      "grad_norm": 1.685815691947937,
      "learning_rate": 0.00019687834081676288,
      "loss": 2.2835,
      "step": 297
    },
    {
      "epoch": 0.015924332700991264,
      "grad_norm": 1.6485916376113892,
      "learning_rate": 0.0001968676502031217,
      "loss": 2.3439,
      "step": 298
    },
    {
      "epoch": 0.01597777005904828,
      "grad_norm": 1.8568354845046997,
      "learning_rate": 0.00019685695958948044,
      "loss": 2.2396,
      "step": 299
    },
    {
      "epoch": 0.016031207417105297,
      "grad_norm": 1.53928804397583,
      "learning_rate": 0.00019684626897583922,
      "loss": 2.3245,
      "step": 300
    },
    {
      "epoch": 0.016084644775162315,
      "grad_norm": 2.2608256340026855,
      "learning_rate": 0.000196835578362198,
      "loss": 2.3091,
      "step": 301
    },
    {
      "epoch": 0.016138082133219333,
      "grad_norm": 1.516178011894226,
      "learning_rate": 0.00019682488774855678,
      "loss": 2.2187,
      "step": 302
    },
    {
      "epoch": 0.01619151949127635,
      "grad_norm": 2.634747266769409,
      "learning_rate": 0.00019681419713491556,
      "loss": 2.4919,
      "step": 303
    },
    {
      "epoch": 0.01624495684933337,
      "grad_norm": 1.651749849319458,
      "learning_rate": 0.00019680350652127434,
      "loss": 2.3897,
      "step": 304
    },
    {
      "epoch": 0.016298394207390387,
      "grad_norm": 1.9723024368286133,
      "learning_rate": 0.0001967928159076331,
      "loss": 2.1688,
      "step": 305
    },
    {
      "epoch": 0.016351831565447405,
      "grad_norm": 2.0453081130981445,
      "learning_rate": 0.0001967821252939919,
      "loss": 2.4991,
      "step": 306
    },
    {
      "epoch": 0.016405268923504423,
      "grad_norm": 1.9980307817459106,
      "learning_rate": 0.00019677143468035065,
      "loss": 2.2217,
      "step": 307
    },
    {
      "epoch": 0.01645870628156144,
      "grad_norm": 1.6497584581375122,
      "learning_rate": 0.00019676074406670943,
      "loss": 2.3419,
      "step": 308
    },
    {
      "epoch": 0.01651214363961846,
      "grad_norm": 1.9262864589691162,
      "learning_rate": 0.0001967500534530682,
      "loss": 2.2534,
      "step": 309
    },
    {
      "epoch": 0.016565580997675473,
      "grad_norm": 2.180208206176758,
      "learning_rate": 0.000196739362839427,
      "loss": 2.314,
      "step": 310
    },
    {
      "epoch": 0.01661901835573249,
      "grad_norm": 2.1183111667633057,
      "learning_rate": 0.00019672867222578577,
      "loss": 2.1948,
      "step": 311
    },
    {
      "epoch": 0.01667245571378951,
      "grad_norm": 1.4080612659454346,
      "learning_rate": 0.00019671798161214455,
      "loss": 2.1517,
      "step": 312
    },
    {
      "epoch": 0.016725893071846527,
      "grad_norm": 1.6375370025634766,
      "learning_rate": 0.0001967072909985033,
      "loss": 2.1603,
      "step": 313
    },
    {
      "epoch": 0.016779330429903545,
      "grad_norm": 1.4604445695877075,
      "learning_rate": 0.0001966966003848621,
      "loss": 2.2201,
      "step": 314
    },
    {
      "epoch": 0.016832767787960563,
      "grad_norm": 1.3064701557159424,
      "learning_rate": 0.0001966859097712209,
      "loss": 2.1033,
      "step": 315
    },
    {
      "epoch": 0.01688620514601758,
      "grad_norm": 1.5236225128173828,
      "learning_rate": 0.00019667521915757964,
      "loss": 2.0369,
      "step": 316
    },
    {
      "epoch": 0.0169396425040746,
      "grad_norm": 1.3869305849075317,
      "learning_rate": 0.00019666452854393845,
      "loss": 2.1949,
      "step": 317
    },
    {
      "epoch": 0.016993079862131617,
      "grad_norm": 1.2172577381134033,
      "learning_rate": 0.00019665383793029723,
      "loss": 2.0486,
      "step": 318
    },
    {
      "epoch": 0.017046517220188635,
      "grad_norm": 1.3590731620788574,
      "learning_rate": 0.00019664314731665598,
      "loss": 1.8919,
      "step": 319
    },
    {
      "epoch": 0.01709995457824565,
      "grad_norm": 1.4523439407348633,
      "learning_rate": 0.00019663245670301476,
      "loss": 2.1908,
      "step": 320
    },
    {
      "epoch": 0.017153391936302668,
      "grad_norm": 2.2986013889312744,
      "learning_rate": 0.00019662176608937354,
      "loss": 2.3291,
      "step": 321
    },
    {
      "epoch": 0.017206829294359686,
      "grad_norm": 1.6117777824401855,
      "learning_rate": 0.00019661107547573232,
      "loss": 2.2605,
      "step": 322
    },
    {
      "epoch": 0.017260266652416704,
      "grad_norm": 1.4682368040084839,
      "learning_rate": 0.0001966003848620911,
      "loss": 2.2054,
      "step": 323
    },
    {
      "epoch": 0.017313704010473722,
      "grad_norm": 1.5568195581436157,
      "learning_rate": 0.00019658969424844985,
      "loss": 2.1177,
      "step": 324
    },
    {
      "epoch": 0.01736714136853074,
      "grad_norm": 2.0581018924713135,
      "learning_rate": 0.00019657900363480866,
      "loss": 2.3674,
      "step": 325
    },
    {
      "epoch": 0.017420578726587758,
      "grad_norm": 1.8323602676391602,
      "learning_rate": 0.00019656831302116744,
      "loss": 2.1817,
      "step": 326
    },
    {
      "epoch": 0.017474016084644776,
      "grad_norm": 1.8304784297943115,
      "learning_rate": 0.0001965576224075262,
      "loss": 2.3724,
      "step": 327
    },
    {
      "epoch": 0.017527453442701794,
      "grad_norm": 1.864850401878357,
      "learning_rate": 0.00019654693179388497,
      "loss": 2.5782,
      "step": 328
    },
    {
      "epoch": 0.017580890800758812,
      "grad_norm": 1.7029331922531128,
      "learning_rate": 0.00019653624118024378,
      "loss": 2.1581,
      "step": 329
    },
    {
      "epoch": 0.01763432815881583,
      "grad_norm": 2.1743996143341064,
      "learning_rate": 0.00019652555056660253,
      "loss": 2.1713,
      "step": 330
    },
    {
      "epoch": 0.017687765516872844,
      "grad_norm": 3.396580457687378,
      "learning_rate": 0.0001965148599529613,
      "loss": 2.3627,
      "step": 331
    },
    {
      "epoch": 0.017741202874929862,
      "grad_norm": 2.4321463108062744,
      "learning_rate": 0.0001965041693393201,
      "loss": 2.2084,
      "step": 332
    },
    {
      "epoch": 0.01779464023298688,
      "grad_norm": 2.184349298477173,
      "learning_rate": 0.00019649347872567887,
      "loss": 1.9508,
      "step": 333
    },
    {
      "epoch": 0.017848077591043898,
      "grad_norm": 2.2527313232421875,
      "learning_rate": 0.00019648278811203765,
      "loss": 2.1596,
      "step": 334
    },
    {
      "epoch": 0.017901514949100916,
      "grad_norm": 1.2998868227005005,
      "learning_rate": 0.0001964720974983964,
      "loss": 1.9677,
      "step": 335
    },
    {
      "epoch": 0.017954952307157934,
      "grad_norm": 1.8274343013763428,
      "learning_rate": 0.00019646140688475518,
      "loss": 2.3404,
      "step": 336
    },
    {
      "epoch": 0.018008389665214952,
      "grad_norm": 1.9581352472305298,
      "learning_rate": 0.00019645071627111399,
      "loss": 2.0904,
      "step": 337
    },
    {
      "epoch": 0.01806182702327197,
      "grad_norm": 2.3672029972076416,
      "learning_rate": 0.00019644002565747274,
      "loss": 2.2664,
      "step": 338
    },
    {
      "epoch": 0.018115264381328988,
      "grad_norm": 1.6438535451889038,
      "learning_rate": 0.00019642933504383152,
      "loss": 2.2047,
      "step": 339
    },
    {
      "epoch": 0.018168701739386006,
      "grad_norm": 2.1392667293548584,
      "learning_rate": 0.00019641864443019032,
      "loss": 2.3227,
      "step": 340
    },
    {
      "epoch": 0.01822213909744302,
      "grad_norm": 2.389693260192871,
      "learning_rate": 0.00019640795381654908,
      "loss": 2.3894,
      "step": 341
    },
    {
      "epoch": 0.01827557645550004,
      "grad_norm": 1.3576246500015259,
      "learning_rate": 0.00019639726320290786,
      "loss": 1.9348,
      "step": 342
    },
    {
      "epoch": 0.018329013813557057,
      "grad_norm": 2.067734956741333,
      "learning_rate": 0.00019638657258926664,
      "loss": 2.4078,
      "step": 343
    },
    {
      "epoch": 0.018382451171614075,
      "grad_norm": 1.8153337240219116,
      "learning_rate": 0.00019637588197562542,
      "loss": 2.4434,
      "step": 344
    },
    {
      "epoch": 0.018435888529671093,
      "grad_norm": 1.6775426864624023,
      "learning_rate": 0.0001963651913619842,
      "loss": 2.3513,
      "step": 345
    },
    {
      "epoch": 0.01848932588772811,
      "grad_norm": 2.236999988555908,
      "learning_rate": 0.00019635450074834298,
      "loss": 2.3394,
      "step": 346
    },
    {
      "epoch": 0.01854276324578513,
      "grad_norm": 2.1314921379089355,
      "learning_rate": 0.00019634381013470173,
      "loss": 2.2935,
      "step": 347
    },
    {
      "epoch": 0.018596200603842147,
      "grad_norm": 1.52981698513031,
      "learning_rate": 0.00019633311952106053,
      "loss": 2.1449,
      "step": 348
    },
    {
      "epoch": 0.018649637961899165,
      "grad_norm": 1.2679015398025513,
      "learning_rate": 0.0001963224289074193,
      "loss": 2.0439,
      "step": 349
    },
    {
      "epoch": 0.018703075319956183,
      "grad_norm": 1.2990000247955322,
      "learning_rate": 0.00019631173829377807,
      "loss": 2.1577,
      "step": 350
    },
    {
      "epoch": 0.0187565126780132,
      "grad_norm": 1.254313588142395,
      "learning_rate": 0.00019630104768013685,
      "loss": 2.0955,
      "step": 351
    },
    {
      "epoch": 0.018809950036070215,
      "grad_norm": 1.9866148233413696,
      "learning_rate": 0.00019629035706649563,
      "loss": 2.1631,
      "step": 352
    },
    {
      "epoch": 0.018863387394127233,
      "grad_norm": 1.3959647417068481,
      "learning_rate": 0.0001962796664528544,
      "loss": 2.1566,
      "step": 353
    },
    {
      "epoch": 0.01891682475218425,
      "grad_norm": 1.6550424098968506,
      "learning_rate": 0.00019626897583921319,
      "loss": 2.2306,
      "step": 354
    },
    {
      "epoch": 0.01897026211024127,
      "grad_norm": 2.0562872886657715,
      "learning_rate": 0.00019625828522557194,
      "loss": 2.2609,
      "step": 355
    },
    {
      "epoch": 0.019023699468298287,
      "grad_norm": 1.5405733585357666,
      "learning_rate": 0.00019624759461193074,
      "loss": 2.0369,
      "step": 356
    },
    {
      "epoch": 0.019077136826355305,
      "grad_norm": 1.3865865468978882,
      "learning_rate": 0.00019623690399828952,
      "loss": 2.2382,
      "step": 357
    },
    {
      "epoch": 0.019130574184412323,
      "grad_norm": 2.133028268814087,
      "learning_rate": 0.00019622621338464828,
      "loss": 2.3701,
      "step": 358
    },
    {
      "epoch": 0.01918401154246934,
      "grad_norm": 1.4404391050338745,
      "learning_rate": 0.00019621552277100706,
      "loss": 2.1121,
      "step": 359
    },
    {
      "epoch": 0.01923744890052636,
      "grad_norm": 2.182229518890381,
      "learning_rate": 0.00019620483215736584,
      "loss": 2.1723,
      "step": 360
    },
    {
      "epoch": 0.019290886258583377,
      "grad_norm": 2.014420509338379,
      "learning_rate": 0.00019619414154372462,
      "loss": 2.3511,
      "step": 361
    },
    {
      "epoch": 0.01934432361664039,
      "grad_norm": 1.757541537284851,
      "learning_rate": 0.0001961834509300834,
      "loss": 2.1759,
      "step": 362
    },
    {
      "epoch": 0.01939776097469741,
      "grad_norm": 1.9685503244400024,
      "learning_rate": 0.00019617276031644218,
      "loss": 2.1858,
      "step": 363
    },
    {
      "epoch": 0.019451198332754428,
      "grad_norm": 1.4649550914764404,
      "learning_rate": 0.00019616206970280095,
      "loss": 2.2868,
      "step": 364
    },
    {
      "epoch": 0.019504635690811446,
      "grad_norm": 1.843182921409607,
      "learning_rate": 0.00019615137908915973,
      "loss": 2.3602,
      "step": 365
    },
    {
      "epoch": 0.019558073048868464,
      "grad_norm": 2.1367135047912598,
      "learning_rate": 0.0001961406884755185,
      "loss": 2.6718,
      "step": 366
    },
    {
      "epoch": 0.01961151040692548,
      "grad_norm": 2.3372647762298584,
      "learning_rate": 0.0001961299978618773,
      "loss": 2.3883,
      "step": 367
    },
    {
      "epoch": 0.0196649477649825,
      "grad_norm": 1.8020437955856323,
      "learning_rate": 0.00019611930724823607,
      "loss": 2.1933,
      "step": 368
    },
    {
      "epoch": 0.019718385123039518,
      "grad_norm": 1.6743323802947998,
      "learning_rate": 0.00019610861663459483,
      "loss": 1.9604,
      "step": 369
    },
    {
      "epoch": 0.019771822481096536,
      "grad_norm": 1.011966347694397,
      "learning_rate": 0.0001960979260209536,
      "loss": 2.0275,
      "step": 370
    },
    {
      "epoch": 0.019825259839153554,
      "grad_norm": 1.484629511833191,
      "learning_rate": 0.00019608723540731239,
      "loss": 2.3198,
      "step": 371
    },
    {
      "epoch": 0.01987869719721057,
      "grad_norm": 1.866796612739563,
      "learning_rate": 0.00019607654479367116,
      "loss": 2.0776,
      "step": 372
    },
    {
      "epoch": 0.019932134555267586,
      "grad_norm": 1.4470391273498535,
      "learning_rate": 0.00019606585418002994,
      "loss": 2.2892,
      "step": 373
    },
    {
      "epoch": 0.019985571913324604,
      "grad_norm": 1.2080490589141846,
      "learning_rate": 0.00019605516356638872,
      "loss": 2.0423,
      "step": 374
    },
    {
      "epoch": 0.020039009271381622,
      "grad_norm": 2.18241548538208,
      "learning_rate": 0.0001960444729527475,
      "loss": 2.2523,
      "step": 375
    },
    {
      "epoch": 0.02009244662943864,
      "grad_norm": 1.9202834367752075,
      "learning_rate": 0.00019603378233910628,
      "loss": 2.2496,
      "step": 376
    },
    {
      "epoch": 0.020145883987495658,
      "grad_norm": 1.365255355834961,
      "learning_rate": 0.00019602309172546504,
      "loss": 2.2767,
      "step": 377
    },
    {
      "epoch": 0.020199321345552676,
      "grad_norm": 1.7156450748443604,
      "learning_rate": 0.00019601240111182382,
      "loss": 2.3301,
      "step": 378
    },
    {
      "epoch": 0.020252758703609694,
      "grad_norm": 1.8139128684997559,
      "learning_rate": 0.00019600171049818262,
      "loss": 2.1138,
      "step": 379
    },
    {
      "epoch": 0.020306196061666712,
      "grad_norm": 2.0475480556488037,
      "learning_rate": 0.00019599101988454138,
      "loss": 2.2855,
      "step": 380
    },
    {
      "epoch": 0.02035963341972373,
      "grad_norm": 1.5645244121551514,
      "learning_rate": 0.00019598032927090015,
      "loss": 2.4369,
      "step": 381
    },
    {
      "epoch": 0.020413070777780748,
      "grad_norm": 1.7425687313079834,
      "learning_rate": 0.00019596963865725893,
      "loss": 2.4459,
      "step": 382
    },
    {
      "epoch": 0.020466508135837762,
      "grad_norm": 1.7663627862930298,
      "learning_rate": 0.00019595894804361771,
      "loss": 2.4063,
      "step": 383
    },
    {
      "epoch": 0.02051994549389478,
      "grad_norm": 1.4760801792144775,
      "learning_rate": 0.0001959482574299765,
      "loss": 2.3433,
      "step": 384
    },
    {
      "epoch": 0.0205733828519518,
      "grad_norm": 1.4966237545013428,
      "learning_rate": 0.00019593756681633527,
      "loss": 2.187,
      "step": 385
    },
    {
      "epoch": 0.020626820210008816,
      "grad_norm": 1.5994693040847778,
      "learning_rate": 0.00019592687620269405,
      "loss": 2.1935,
      "step": 386
    },
    {
      "epoch": 0.020680257568065834,
      "grad_norm": 1.582716464996338,
      "learning_rate": 0.00019591618558905283,
      "loss": 2.367,
      "step": 387
    },
    {
      "epoch": 0.020733694926122852,
      "grad_norm": 1.410301923751831,
      "learning_rate": 0.00019590549497541159,
      "loss": 2.2841,
      "step": 388
    },
    {
      "epoch": 0.02078713228417987,
      "grad_norm": 1.8106001615524292,
      "learning_rate": 0.00019589480436177036,
      "loss": 2.4835,
      "step": 389
    },
    {
      "epoch": 0.02084056964223689,
      "grad_norm": 1.8069345951080322,
      "learning_rate": 0.00019588411374812917,
      "loss": 2.3393,
      "step": 390
    },
    {
      "epoch": 0.020894007000293906,
      "grad_norm": 1.4058692455291748,
      "learning_rate": 0.00019587342313448792,
      "loss": 2.0817,
      "step": 391
    },
    {
      "epoch": 0.020947444358350924,
      "grad_norm": 1.6656426191329956,
      "learning_rate": 0.0001958627325208467,
      "loss": 2.2399,
      "step": 392
    },
    {
      "epoch": 0.021000881716407942,
      "grad_norm": 1.5856181383132935,
      "learning_rate": 0.00019585204190720548,
      "loss": 2.4157,
      "step": 393
    },
    {
      "epoch": 0.021054319074464957,
      "grad_norm": 2.1450612545013428,
      "learning_rate": 0.00019584135129356426,
      "loss": 2.1656,
      "step": 394
    },
    {
      "epoch": 0.021107756432521975,
      "grad_norm": 1.409704327583313,
      "learning_rate": 0.00019583066067992304,
      "loss": 2.2677,
      "step": 395
    },
    {
      "epoch": 0.021161193790578993,
      "grad_norm": 1.6665501594543457,
      "learning_rate": 0.00019581997006628182,
      "loss": 2.0794,
      "step": 396
    },
    {
      "epoch": 0.02121463114863601,
      "grad_norm": 1.0707180500030518,
      "learning_rate": 0.00019580927945264057,
      "loss": 2.1483,
      "step": 397
    },
    {
      "epoch": 0.02126806850669303,
      "grad_norm": 1.2828142642974854,
      "learning_rate": 0.00019579858883899938,
      "loss": 2.1464,
      "step": 398
    },
    {
      "epoch": 0.021321505864750047,
      "grad_norm": 2.5235960483551025,
      "learning_rate": 0.00019578789822535813,
      "loss": 2.2938,
      "step": 399
    },
    {
      "epoch": 0.021374943222807065,
      "grad_norm": 2.111114025115967,
      "learning_rate": 0.00019577720761171691,
      "loss": 2.241,
      "step": 400
    },
    {
      "epoch": 0.021428380580864083,
      "grad_norm": 1.5108869075775146,
      "learning_rate": 0.0001957665169980757,
      "loss": 2.0496,
      "step": 401
    },
    {
      "epoch": 0.0214818179389211,
      "grad_norm": 1.4925700426101685,
      "learning_rate": 0.00019575582638443447,
      "loss": 2.2275,
      "step": 402
    },
    {
      "epoch": 0.02153525529697812,
      "grad_norm": 1.7199853658676147,
      "learning_rate": 0.00019574513577079325,
      "loss": 2.2018,
      "step": 403
    },
    {
      "epoch": 0.021588692655035133,
      "grad_norm": 1.8864065408706665,
      "learning_rate": 0.00019573444515715203,
      "loss": 2.123,
      "step": 404
    },
    {
      "epoch": 0.02164213001309215,
      "grad_norm": 1.1504452228546143,
      "learning_rate": 0.0001957237545435108,
      "loss": 1.9894,
      "step": 405
    },
    {
      "epoch": 0.02169556737114917,
      "grad_norm": 1.6304064989089966,
      "learning_rate": 0.0001957130639298696,
      "loss": 2.2186,
      "step": 406
    },
    {
      "epoch": 0.021749004729206187,
      "grad_norm": 1.850245475769043,
      "learning_rate": 0.00019570237331622837,
      "loss": 2.2485,
      "step": 407
    },
    {
      "epoch": 0.021802442087263205,
      "grad_norm": 1.1990010738372803,
      "learning_rate": 0.00019569168270258712,
      "loss": 2.1127,
      "step": 408
    },
    {
      "epoch": 0.021855879445320223,
      "grad_norm": 2.3066883087158203,
      "learning_rate": 0.00019568099208894593,
      "loss": 2.3212,
      "step": 409
    },
    {
      "epoch": 0.02190931680337724,
      "grad_norm": 1.4544934034347534,
      "learning_rate": 0.00019567030147530468,
      "loss": 2.2913,
      "step": 410
    },
    {
      "epoch": 0.02196275416143426,
      "grad_norm": 1.6724469661712646,
      "learning_rate": 0.00019565961086166346,
      "loss": 2.304,
      "step": 411
    },
    {
      "epoch": 0.022016191519491277,
      "grad_norm": 1.6279677152633667,
      "learning_rate": 0.00019564892024802224,
      "loss": 2.3738,
      "step": 412
    },
    {
      "epoch": 0.022069628877548295,
      "grad_norm": 1.3418816328048706,
      "learning_rate": 0.00019563822963438102,
      "loss": 2.1854,
      "step": 413
    },
    {
      "epoch": 0.022123066235605313,
      "grad_norm": 1.5234036445617676,
      "learning_rate": 0.0001956275390207398,
      "loss": 2.2349,
      "step": 414
    },
    {
      "epoch": 0.022176503593662328,
      "grad_norm": 1.4872546195983887,
      "learning_rate": 0.00019561684840709858,
      "loss": 2.3764,
      "step": 415
    },
    {
      "epoch": 0.022229940951719346,
      "grad_norm": 1.5069973468780518,
      "learning_rate": 0.00019560615779345733,
      "loss": 2.2336,
      "step": 416
    },
    {
      "epoch": 0.022283378309776364,
      "grad_norm": 1.6426215171813965,
      "learning_rate": 0.00019559546717981614,
      "loss": 2.2119,
      "step": 417
    },
    {
      "epoch": 0.022336815667833382,
      "grad_norm": 2.135556697845459,
      "learning_rate": 0.00019558477656617492,
      "loss": 2.3547,
      "step": 418
    },
    {
      "epoch": 0.0223902530258904,
      "grad_norm": 2.499321699142456,
      "learning_rate": 0.00019557408595253367,
      "loss": 2.2448,
      "step": 419
    },
    {
      "epoch": 0.022443690383947418,
      "grad_norm": 1.5638816356658936,
      "learning_rate": 0.00019556339533889245,
      "loss": 2.0937,
      "step": 420
    },
    {
      "epoch": 0.022497127742004436,
      "grad_norm": 1.658165454864502,
      "learning_rate": 0.00019555270472525123,
      "loss": 2.177,
      "step": 421
    },
    {
      "epoch": 0.022550565100061454,
      "grad_norm": 1.5044136047363281,
      "learning_rate": 0.00019554201411161,
      "loss": 2.2475,
      "step": 422
    },
    {
      "epoch": 0.022604002458118472,
      "grad_norm": 1.7413270473480225,
      "learning_rate": 0.0001955313234979688,
      "loss": 2.1585,
      "step": 423
    },
    {
      "epoch": 0.02265743981617549,
      "grad_norm": 1.666377067565918,
      "learning_rate": 0.00019552063288432757,
      "loss": 2.3403,
      "step": 424
    },
    {
      "epoch": 0.022710877174232504,
      "grad_norm": 1.6102144718170166,
      "learning_rate": 0.00019550994227068635,
      "loss": 2.1854,
      "step": 425
    },
    {
      "epoch": 0.022764314532289522,
      "grad_norm": 1.4352996349334717,
      "learning_rate": 0.00019549925165704513,
      "loss": 2.1658,
      "step": 426
    },
    {
      "epoch": 0.02281775189034654,
      "grad_norm": 1.4771225452423096,
      "learning_rate": 0.00019548856104340388,
      "loss": 2.1162,
      "step": 427
    },
    {
      "epoch": 0.022871189248403558,
      "grad_norm": 1.5282219648361206,
      "learning_rate": 0.0001954778704297627,
      "loss": 2.1798,
      "step": 428
    },
    {
      "epoch": 0.022924626606460576,
      "grad_norm": 1.9245152473449707,
      "learning_rate": 0.00019546717981612147,
      "loss": 2.485,
      "step": 429
    },
    {
      "epoch": 0.022978063964517594,
      "grad_norm": 1.490512728691101,
      "learning_rate": 0.00019545648920248022,
      "loss": 2.0756,
      "step": 430
    },
    {
      "epoch": 0.023031501322574612,
      "grad_norm": 3.5692527294158936,
      "learning_rate": 0.000195445798588839,
      "loss": 2.6035,
      "step": 431
    },
    {
      "epoch": 0.02308493868063163,
      "grad_norm": 1.775394082069397,
      "learning_rate": 0.0001954351079751978,
      "loss": 2.3631,
      "step": 432
    },
    {
      "epoch": 0.023138376038688648,
      "grad_norm": 1.9869366884231567,
      "learning_rate": 0.00019542441736155656,
      "loss": 2.1336,
      "step": 433
    },
    {
      "epoch": 0.023191813396745666,
      "grad_norm": 1.7394651174545288,
      "learning_rate": 0.00019541372674791534,
      "loss": 2.3423,
      "step": 434
    },
    {
      "epoch": 0.023245250754802684,
      "grad_norm": 1.5531123876571655,
      "learning_rate": 0.00019540303613427412,
      "loss": 2.1749,
      "step": 435
    },
    {
      "epoch": 0.0232986881128597,
      "grad_norm": 1.1321598291397095,
      "learning_rate": 0.0001953923455206329,
      "loss": 2.1495,
      "step": 436
    },
    {
      "epoch": 0.023352125470916717,
      "grad_norm": 1.6347970962524414,
      "learning_rate": 0.00019538165490699168,
      "loss": 2.3563,
      "step": 437
    },
    {
      "epoch": 0.023405562828973735,
      "grad_norm": 1.37310791015625,
      "learning_rate": 0.00019537096429335043,
      "loss": 2.3454,
      "step": 438
    },
    {
      "epoch": 0.023459000187030753,
      "grad_norm": 2.4041125774383545,
      "learning_rate": 0.0001953602736797092,
      "loss": 2.4931,
      "step": 439
    },
    {
      "epoch": 0.02351243754508777,
      "grad_norm": 1.7578755617141724,
      "learning_rate": 0.00019534958306606802,
      "loss": 2.1209,
      "step": 440
    },
    {
      "epoch": 0.02356587490314479,
      "grad_norm": 1.4815069437026978,
      "learning_rate": 0.00019533889245242677,
      "loss": 2.3518,
      "step": 441
    },
    {
      "epoch": 0.023619312261201807,
      "grad_norm": 1.346793293952942,
      "learning_rate": 0.00019532820183878555,
      "loss": 2.1413,
      "step": 442
    },
    {
      "epoch": 0.023672749619258825,
      "grad_norm": 1.59746253490448,
      "learning_rate": 0.00019531751122514433,
      "loss": 2.2829,
      "step": 443
    },
    {
      "epoch": 0.023726186977315843,
      "grad_norm": 1.4369903802871704,
      "learning_rate": 0.0001953068206115031,
      "loss": 2.2007,
      "step": 444
    },
    {
      "epoch": 0.02377962433537286,
      "grad_norm": 1.640157699584961,
      "learning_rate": 0.0001952961299978619,
      "loss": 2.2621,
      "step": 445
    },
    {
      "epoch": 0.023833061693429875,
      "grad_norm": 1.7678275108337402,
      "learning_rate": 0.00019528543938422067,
      "loss": 2.1785,
      "step": 446
    },
    {
      "epoch": 0.023886499051486893,
      "grad_norm": 2.1613359451293945,
      "learning_rate": 0.00019527474877057942,
      "loss": 2.3874,
      "step": 447
    },
    {
      "epoch": 0.02393993640954391,
      "grad_norm": 1.2673709392547607,
      "learning_rate": 0.00019526405815693823,
      "loss": 2.0311,
      "step": 448
    },
    {
      "epoch": 0.02399337376760093,
      "grad_norm": 1.5374022722244263,
      "learning_rate": 0.00019525336754329698,
      "loss": 2.2203,
      "step": 449
    },
    {
      "epoch": 0.024046811125657947,
      "grad_norm": 1.5975531339645386,
      "learning_rate": 0.00019524267692965576,
      "loss": 2.2332,
      "step": 450
    },
    {
      "epoch": 0.024100248483714965,
      "grad_norm": 1.9423389434814453,
      "learning_rate": 0.00019523198631601457,
      "loss": 2.2466,
      "step": 451
    },
    {
      "epoch": 0.024153685841771983,
      "grad_norm": 1.4145885705947876,
      "learning_rate": 0.00019522129570237332,
      "loss": 2.2765,
      "step": 452
    },
    {
      "epoch": 0.024207123199829,
      "grad_norm": 1.6494556665420532,
      "learning_rate": 0.0001952106050887321,
      "loss": 2.3276,
      "step": 453
    },
    {
      "epoch": 0.02426056055788602,
      "grad_norm": 1.2665228843688965,
      "learning_rate": 0.00019519991447509088,
      "loss": 2.1897,
      "step": 454
    },
    {
      "epoch": 0.024313997915943037,
      "grad_norm": 1.881937026977539,
      "learning_rate": 0.00019518922386144966,
      "loss": 2.4594,
      "step": 455
    },
    {
      "epoch": 0.024367435274000055,
      "grad_norm": 1.4754122495651245,
      "learning_rate": 0.00019517853324780844,
      "loss": 2.3743,
      "step": 456
    },
    {
      "epoch": 0.02442087263205707,
      "grad_norm": 1.5660134553909302,
      "learning_rate": 0.00019516784263416722,
      "loss": 2.0857,
      "step": 457
    },
    {
      "epoch": 0.024474309990114088,
      "grad_norm": 1.8273221254348755,
      "learning_rate": 0.00019515715202052597,
      "loss": 2.3449,
      "step": 458
    },
    {
      "epoch": 0.024527747348171106,
      "grad_norm": 1.7010369300842285,
      "learning_rate": 0.00019514646140688478,
      "loss": 2.1599,
      "step": 459
    },
    {
      "epoch": 0.024581184706228124,
      "grad_norm": 1.334182858467102,
      "learning_rate": 0.00019513577079324356,
      "loss": 2.1056,
      "step": 460
    },
    {
      "epoch": 0.02463462206428514,
      "grad_norm": 1.033002495765686,
      "learning_rate": 0.0001951250801796023,
      "loss": 2.0707,
      "step": 461
    },
    {
      "epoch": 0.02468805942234216,
      "grad_norm": 1.2207106351852417,
      "learning_rate": 0.0001951143895659611,
      "loss": 2.0797,
      "step": 462
    },
    {
      "epoch": 0.024741496780399178,
      "grad_norm": 1.7076621055603027,
      "learning_rate": 0.00019510369895231987,
      "loss": 2.2944,
      "step": 463
    },
    {
      "epoch": 0.024794934138456196,
      "grad_norm": 1.6337462663650513,
      "learning_rate": 0.00019509300833867865,
      "loss": 2.2865,
      "step": 464
    },
    {
      "epoch": 0.024848371496513214,
      "grad_norm": 1.4570035934448242,
      "learning_rate": 0.00019508231772503743,
      "loss": 2.1124,
      "step": 465
    },
    {
      "epoch": 0.02490180885457023,
      "grad_norm": 1.6131442785263062,
      "learning_rate": 0.00019507162711139618,
      "loss": 2.0855,
      "step": 466
    },
    {
      "epoch": 0.024955246212627246,
      "grad_norm": 1.6882671117782593,
      "learning_rate": 0.000195060936497755,
      "loss": 2.3363,
      "step": 467
    },
    {
      "epoch": 0.025008683570684264,
      "grad_norm": 2.1365067958831787,
      "learning_rate": 0.00019505024588411377,
      "loss": 2.1806,
      "step": 468
    },
    {
      "epoch": 0.025062120928741282,
      "grad_norm": 2.0290260314941406,
      "learning_rate": 0.00019503955527047252,
      "loss": 2.1939,
      "step": 469
    },
    {
      "epoch": 0.0251155582867983,
      "grad_norm": 1.8123079538345337,
      "learning_rate": 0.0001950288646568313,
      "loss": 2.3007,
      "step": 470
    },
    {
      "epoch": 0.025168995644855318,
      "grad_norm": 1.6627124547958374,
      "learning_rate": 0.0001950181740431901,
      "loss": 2.2224,
      "step": 471
    },
    {
      "epoch": 0.025222433002912336,
      "grad_norm": 1.4242817163467407,
      "learning_rate": 0.00019500748342954886,
      "loss": 2.1412,
      "step": 472
    },
    {
      "epoch": 0.025275870360969354,
      "grad_norm": 1.7171142101287842,
      "learning_rate": 0.00019499679281590764,
      "loss": 2.2832,
      "step": 473
    },
    {
      "epoch": 0.025329307719026372,
      "grad_norm": 1.901180386543274,
      "learning_rate": 0.00019498610220226642,
      "loss": 2.391,
      "step": 474
    },
    {
      "epoch": 0.02538274507708339,
      "grad_norm": 2.2097671031951904,
      "learning_rate": 0.0001949754115886252,
      "loss": 2.2794,
      "step": 475
    },
    {
      "epoch": 0.025436182435140408,
      "grad_norm": 2.028759717941284,
      "learning_rate": 0.00019496472097498398,
      "loss": 2.3262,
      "step": 476
    },
    {
      "epoch": 0.025489619793197426,
      "grad_norm": 1.851659893989563,
      "learning_rate": 0.00019495403036134273,
      "loss": 2.2633,
      "step": 477
    },
    {
      "epoch": 0.02554305715125444,
      "grad_norm": 1.7000919580459595,
      "learning_rate": 0.00019494333974770154,
      "loss": 2.1925,
      "step": 478
    },
    {
      "epoch": 0.02559649450931146,
      "grad_norm": 1.4994149208068848,
      "learning_rate": 0.00019493264913406032,
      "loss": 2.0432,
      "step": 479
    },
    {
      "epoch": 0.025649931867368476,
      "grad_norm": 1.6292952299118042,
      "learning_rate": 0.00019492195852041907,
      "loss": 2.2275,
      "step": 480
    },
    {
      "epoch": 0.025703369225425494,
      "grad_norm": 1.2673231363296509,
      "learning_rate": 0.00019491126790677785,
      "loss": 2.2141,
      "step": 481
    },
    {
      "epoch": 0.025756806583482512,
      "grad_norm": 1.658721923828125,
      "learning_rate": 0.00019490057729313666,
      "loss": 2.0951,
      "step": 482
    },
    {
      "epoch": 0.02581024394153953,
      "grad_norm": 2.232370138168335,
      "learning_rate": 0.0001948898866794954,
      "loss": 2.2737,
      "step": 483
    },
    {
      "epoch": 0.02586368129959655,
      "grad_norm": 1.5627198219299316,
      "learning_rate": 0.0001948791960658542,
      "loss": 2.3156,
      "step": 484
    },
    {
      "epoch": 0.025917118657653566,
      "grad_norm": 1.6117973327636719,
      "learning_rate": 0.00019486850545221297,
      "loss": 2.3256,
      "step": 485
    },
    {
      "epoch": 0.025970556015710584,
      "grad_norm": 2.249568223953247,
      "learning_rate": 0.00019485781483857175,
      "loss": 2.2664,
      "step": 486
    },
    {
      "epoch": 0.026023993373767602,
      "grad_norm": 1.8979620933532715,
      "learning_rate": 0.00019484712422493053,
      "loss": 2.4969,
      "step": 487
    },
    {
      "epoch": 0.026077430731824617,
      "grad_norm": 2.128509759902954,
      "learning_rate": 0.0001948364336112893,
      "loss": 2.2922,
      "step": 488
    },
    {
      "epoch": 0.026130868089881635,
      "grad_norm": 1.6429524421691895,
      "learning_rate": 0.00019482574299764806,
      "loss": 2.1732,
      "step": 489
    },
    {
      "epoch": 0.026184305447938653,
      "grad_norm": 2.024879217147827,
      "learning_rate": 0.00019481505238400687,
      "loss": 2.143,
      "step": 490
    },
    {
      "epoch": 0.02623774280599567,
      "grad_norm": 1.5489376783370972,
      "learning_rate": 0.00019480436177036562,
      "loss": 2.1844,
      "step": 491
    },
    {
      "epoch": 0.02629118016405269,
      "grad_norm": 1.3861124515533447,
      "learning_rate": 0.0001947936711567244,
      "loss": 2.353,
      "step": 492
    },
    {
      "epoch": 0.026344617522109707,
      "grad_norm": 1.7259782552719116,
      "learning_rate": 0.0001947829805430832,
      "loss": 2.2519,
      "step": 493
    },
    {
      "epoch": 0.026398054880166725,
      "grad_norm": 1.6109014749526978,
      "learning_rate": 0.00019477228992944196,
      "loss": 2.3396,
      "step": 494
    },
    {
      "epoch": 0.026451492238223743,
      "grad_norm": 1.8184728622436523,
      "learning_rate": 0.00019476159931580074,
      "loss": 2.3335,
      "step": 495
    },
    {
      "epoch": 0.02650492959628076,
      "grad_norm": 1.2416975498199463,
      "learning_rate": 0.00019475090870215952,
      "loss": 2.2105,
      "step": 496
    },
    {
      "epoch": 0.02655836695433778,
      "grad_norm": 1.635772466659546,
      "learning_rate": 0.0001947402180885183,
      "loss": 2.0831,
      "step": 497
    },
    {
      "epoch": 0.026611804312394797,
      "grad_norm": 1.3396542072296143,
      "learning_rate": 0.00019472952747487708,
      "loss": 2.0149,
      "step": 498
    },
    {
      "epoch": 0.02666524167045181,
      "grad_norm": 1.4073172807693481,
      "learning_rate": 0.00019471883686123586,
      "loss": 2.2383,
      "step": 499
    },
    {
      "epoch": 0.02671867902850883,
      "grad_norm": 1.2398576736450195,
      "learning_rate": 0.0001947081462475946,
      "loss": 2.2385,
      "step": 500
    },
    {
      "epoch": 0.026772116386565847,
      "grad_norm": 1.3142000436782837,
      "learning_rate": 0.00019469745563395341,
      "loss": 2.1336,
      "step": 501
    },
    {
      "epoch": 0.026825553744622865,
      "grad_norm": 2.0256950855255127,
      "learning_rate": 0.00019468676502031217,
      "loss": 2.3916,
      "step": 502
    },
    {
      "epoch": 0.026878991102679883,
      "grad_norm": 2.338701009750366,
      "learning_rate": 0.00019467607440667095,
      "loss": 2.2554,
      "step": 503
    },
    {
      "epoch": 0.0269324284607369,
      "grad_norm": 1.0974456071853638,
      "learning_rate": 0.00019466538379302973,
      "loss": 2.1999,
      "step": 504
    },
    {
      "epoch": 0.02698586581879392,
      "grad_norm": 1.844046711921692,
      "learning_rate": 0.0001946546931793885,
      "loss": 2.4719,
      "step": 505
    },
    {
      "epoch": 0.027039303176850937,
      "grad_norm": 0.9773042798042297,
      "learning_rate": 0.00019464400256574729,
      "loss": 2.1225,
      "step": 506
    },
    {
      "epoch": 0.027092740534907955,
      "grad_norm": 1.511744737625122,
      "learning_rate": 0.00019463331195210607,
      "loss": 2.2554,
      "step": 507
    },
    {
      "epoch": 0.027146177892964973,
      "grad_norm": 1.4934674501419067,
      "learning_rate": 0.00019462262133846482,
      "loss": 2.2031,
      "step": 508
    },
    {
      "epoch": 0.027199615251021988,
      "grad_norm": 1.9313687086105347,
      "learning_rate": 0.00019461193072482362,
      "loss": 2.2365,
      "step": 509
    },
    {
      "epoch": 0.027253052609079006,
      "grad_norm": 1.7127453088760376,
      "learning_rate": 0.0001946012401111824,
      "loss": 2.0869,
      "step": 510
    },
    {
      "epoch": 0.027306489967136024,
      "grad_norm": 1.6346625089645386,
      "learning_rate": 0.00019459054949754116,
      "loss": 2.407,
      "step": 511
    },
    {
      "epoch": 0.027359927325193042,
      "grad_norm": 1.6943135261535645,
      "learning_rate": 0.00019457985888389994,
      "loss": 2.2212,
      "step": 512
    },
    {
      "epoch": 0.02741336468325006,
      "grad_norm": 1.9423191547393799,
      "learning_rate": 0.00019456916827025872,
      "loss": 2.2137,
      "step": 513
    },
    {
      "epoch": 0.027466802041307078,
      "grad_norm": 1.1779425144195557,
      "learning_rate": 0.0001945584776566175,
      "loss": 2.2278,
      "step": 514
    },
    {
      "epoch": 0.027520239399364096,
      "grad_norm": 1.723721981048584,
      "learning_rate": 0.00019454778704297628,
      "loss": 2.2462,
      "step": 515
    },
    {
      "epoch": 0.027573676757421114,
      "grad_norm": 1.3354990482330322,
      "learning_rate": 0.00019453709642933505,
      "loss": 2.156,
      "step": 516
    },
    {
      "epoch": 0.027627114115478132,
      "grad_norm": 2.3046646118164062,
      "learning_rate": 0.00019452640581569383,
      "loss": 2.2433,
      "step": 517
    },
    {
      "epoch": 0.02768055147353515,
      "grad_norm": 1.220600962638855,
      "learning_rate": 0.00019451571520205261,
      "loss": 2.0083,
      "step": 518
    },
    {
      "epoch": 0.027733988831592168,
      "grad_norm": 1.5465046167373657,
      "learning_rate": 0.00019450502458841137,
      "loss": 2.1271,
      "step": 519
    },
    {
      "epoch": 0.027787426189649182,
      "grad_norm": 1.1995384693145752,
      "learning_rate": 0.00019449433397477017,
      "loss": 2.1132,
      "step": 520
    },
    {
      "epoch": 0.0278408635477062,
      "grad_norm": 1.6440407037734985,
      "learning_rate": 0.00019448364336112895,
      "loss": 2.2017,
      "step": 521
    },
    {
      "epoch": 0.027894300905763218,
      "grad_norm": 1.2458263635635376,
      "learning_rate": 0.0001944729527474877,
      "loss": 2.138,
      "step": 522
    },
    {
      "epoch": 0.027947738263820236,
      "grad_norm": 1.9719293117523193,
      "learning_rate": 0.00019446226213384649,
      "loss": 2.1484,
      "step": 523
    },
    {
      "epoch": 0.028001175621877254,
      "grad_norm": 1.7744888067245483,
      "learning_rate": 0.00019445157152020527,
      "loss": 2.3131,
      "step": 524
    },
    {
      "epoch": 0.028054612979934272,
      "grad_norm": 1.3475861549377441,
      "learning_rate": 0.00019444088090656404,
      "loss": 2.2292,
      "step": 525
    },
    {
      "epoch": 0.02810805033799129,
      "grad_norm": 9.445395469665527,
      "learning_rate": 0.00019443019029292282,
      "loss": 2.6096,
      "step": 526
    },
    {
      "epoch": 0.028161487696048308,
      "grad_norm": 1.2377979755401611,
      "learning_rate": 0.0001944194996792816,
      "loss": 2.1588,
      "step": 527
    },
    {
      "epoch": 0.028214925054105326,
      "grad_norm": 1.9649357795715332,
      "learning_rate": 0.00019440880906564038,
      "loss": 2.5576,
      "step": 528
    },
    {
      "epoch": 0.028268362412162344,
      "grad_norm": 1.4124835729599,
      "learning_rate": 0.00019439811845199916,
      "loss": 2.0712,
      "step": 529
    },
    {
      "epoch": 0.02832179977021936,
      "grad_norm": 1.3399747610092163,
      "learning_rate": 0.00019438742783835792,
      "loss": 2.1809,
      "step": 530
    },
    {
      "epoch": 0.028375237128276377,
      "grad_norm": 1.6877853870391846,
      "learning_rate": 0.0001943767372247167,
      "loss": 2.1646,
      "step": 531
    },
    {
      "epoch": 0.028428674486333395,
      "grad_norm": 1.7558391094207764,
      "learning_rate": 0.0001943660466110755,
      "loss": 2.3478,
      "step": 532
    },
    {
      "epoch": 0.028482111844390413,
      "grad_norm": 1.1842668056488037,
      "learning_rate": 0.00019435535599743425,
      "loss": 2.1867,
      "step": 533
    },
    {
      "epoch": 0.02853554920244743,
      "grad_norm": 1.6987746953964233,
      "learning_rate": 0.00019434466538379303,
      "loss": 2.4122,
      "step": 534
    },
    {
      "epoch": 0.02858898656050445,
      "grad_norm": 1.4259535074234009,
      "learning_rate": 0.00019433397477015181,
      "loss": 2.0508,
      "step": 535
    },
    {
      "epoch": 0.028642423918561467,
      "grad_norm": 1.246730923652649,
      "learning_rate": 0.0001943232841565106,
      "loss": 2.2981,
      "step": 536
    },
    {
      "epoch": 0.028695861276618485,
      "grad_norm": 1.554824709892273,
      "learning_rate": 0.00019431259354286937,
      "loss": 2.236,
      "step": 537
    },
    {
      "epoch": 0.028749298634675503,
      "grad_norm": 2.014029026031494,
      "learning_rate": 0.00019430190292922815,
      "loss": 2.4227,
      "step": 538
    },
    {
      "epoch": 0.02880273599273252,
      "grad_norm": 1.3142985105514526,
      "learning_rate": 0.00019429121231558693,
      "loss": 2.115,
      "step": 539
    },
    {
      "epoch": 0.02885617335078954,
      "grad_norm": 1.3710026741027832,
      "learning_rate": 0.0001942805217019457,
      "loss": 2.2525,
      "step": 540
    },
    {
      "epoch": 0.028909610708846553,
      "grad_norm": 1.5977188348770142,
      "learning_rate": 0.00019426983108830446,
      "loss": 2.1867,
      "step": 541
    },
    {
      "epoch": 0.02896304806690357,
      "grad_norm": 2.108612060546875,
      "learning_rate": 0.00019425914047466324,
      "loss": 2.1683,
      "step": 542
    },
    {
      "epoch": 0.02901648542496059,
      "grad_norm": 1.406044840812683,
      "learning_rate": 0.00019424844986102205,
      "loss": 2.1909,
      "step": 543
    },
    {
      "epoch": 0.029069922783017607,
      "grad_norm": 1.338052749633789,
      "learning_rate": 0.0001942377592473808,
      "loss": 2.5381,
      "step": 544
    },
    {
      "epoch": 0.029123360141074625,
      "grad_norm": 1.2222813367843628,
      "learning_rate": 0.00019422706863373958,
      "loss": 2.0283,
      "step": 545
    },
    {
      "epoch": 0.029176797499131643,
      "grad_norm": 2.5835468769073486,
      "learning_rate": 0.00019421637802009836,
      "loss": 2.2773,
      "step": 546
    },
    {
      "epoch": 0.02923023485718866,
      "grad_norm": 1.7525159120559692,
      "learning_rate": 0.00019420568740645714,
      "loss": 2.2138,
      "step": 547
    },
    {
      "epoch": 0.02928367221524568,
      "grad_norm": 1.307668924331665,
      "learning_rate": 0.00019419499679281592,
      "loss": 2.1131,
      "step": 548
    },
    {
      "epoch": 0.029337109573302697,
      "grad_norm": 1.4153488874435425,
      "learning_rate": 0.0001941843061791747,
      "loss": 2.0879,
      "step": 549
    },
    {
      "epoch": 0.029390546931359715,
      "grad_norm": 1.7380988597869873,
      "learning_rate": 0.00019417361556553345,
      "loss": 2.1686,
      "step": 550
    },
    {
      "epoch": 0.02944398428941673,
      "grad_norm": 1.1158589124679565,
      "learning_rate": 0.00019416292495189226,
      "loss": 2.3333,
      "step": 551
    },
    {
      "epoch": 0.029497421647473748,
      "grad_norm": 2.5812857151031494,
      "learning_rate": 0.00019415223433825101,
      "loss": 2.3925,
      "step": 552
    },
    {
      "epoch": 0.029550859005530766,
      "grad_norm": 1.5314263105392456,
      "learning_rate": 0.0001941415437246098,
      "loss": 2.2688,
      "step": 553
    },
    {
      "epoch": 0.029604296363587784,
      "grad_norm": 1.3873002529144287,
      "learning_rate": 0.00019413085311096857,
      "loss": 2.3305,
      "step": 554
    },
    {
      "epoch": 0.0296577337216448,
      "grad_norm": 1.0984522104263306,
      "learning_rate": 0.00019412016249732735,
      "loss": 2.2066,
      "step": 555
    },
    {
      "epoch": 0.02971117107970182,
      "grad_norm": 1.1515185832977295,
      "learning_rate": 0.00019410947188368613,
      "loss": 2.0687,
      "step": 556
    },
    {
      "epoch": 0.029764608437758838,
      "grad_norm": 1.8756275177001953,
      "learning_rate": 0.0001940987812700449,
      "loss": 2.2873,
      "step": 557
    },
    {
      "epoch": 0.029818045795815856,
      "grad_norm": 1.4606049060821533,
      "learning_rate": 0.00019408809065640366,
      "loss": 2.1353,
      "step": 558
    },
    {
      "epoch": 0.029871483153872874,
      "grad_norm": 1.695835828781128,
      "learning_rate": 0.00019407740004276247,
      "loss": 2.1446,
      "step": 559
    },
    {
      "epoch": 0.02992492051192989,
      "grad_norm": 1.6043890714645386,
      "learning_rate": 0.00019406670942912125,
      "loss": 2.2021,
      "step": 560
    },
    {
      "epoch": 0.02997835786998691,
      "grad_norm": 1.3705354928970337,
      "learning_rate": 0.00019405601881548,
      "loss": 2.2624,
      "step": 561
    },
    {
      "epoch": 0.030031795228043924,
      "grad_norm": 1.1575859785079956,
      "learning_rate": 0.0001940453282018388,
      "loss": 2.2651,
      "step": 562
    },
    {
      "epoch": 0.030085232586100942,
      "grad_norm": 1.5600175857543945,
      "learning_rate": 0.00019403463758819756,
      "loss": 2.114,
      "step": 563
    },
    {
      "epoch": 0.03013866994415796,
      "grad_norm": 1.4908521175384521,
      "learning_rate": 0.00019402394697455634,
      "loss": 1.9556,
      "step": 564
    },
    {
      "epoch": 0.030192107302214978,
      "grad_norm": 1.4462400674819946,
      "learning_rate": 0.00019401325636091512,
      "loss": 2.0967,
      "step": 565
    },
    {
      "epoch": 0.030245544660271996,
      "grad_norm": 2.2806289196014404,
      "learning_rate": 0.0001940025657472739,
      "loss": 2.3221,
      "step": 566
    },
    {
      "epoch": 0.030298982018329014,
      "grad_norm": 1.2984694242477417,
      "learning_rate": 0.00019399187513363268,
      "loss": 2.3351,
      "step": 567
    },
    {
      "epoch": 0.030352419376386032,
      "grad_norm": 1.485783338546753,
      "learning_rate": 0.00019398118451999146,
      "loss": 2.1589,
      "step": 568
    },
    {
      "epoch": 0.03040585673444305,
      "grad_norm": 1.6622333526611328,
      "learning_rate": 0.00019397049390635021,
      "loss": 2.1915,
      "step": 569
    },
    {
      "epoch": 0.030459294092500068,
      "grad_norm": 1.6898832321166992,
      "learning_rate": 0.00019395980329270902,
      "loss": 2.3699,
      "step": 570
    },
    {
      "epoch": 0.030512731450557086,
      "grad_norm": 1.549136757850647,
      "learning_rate": 0.0001939491126790678,
      "loss": 2.0333,
      "step": 571
    },
    {
      "epoch": 0.0305661688086141,
      "grad_norm": 1.4768996238708496,
      "learning_rate": 0.00019393842206542655,
      "loss": 2.2611,
      "step": 572
    },
    {
      "epoch": 0.03061960616667112,
      "grad_norm": 1.8645981550216675,
      "learning_rate": 0.00019392773145178533,
      "loss": 2.5206,
      "step": 573
    },
    {
      "epoch": 0.030673043524728136,
      "grad_norm": 1.8403762578964233,
      "learning_rate": 0.00019391704083814414,
      "loss": 2.3616,
      "step": 574
    },
    {
      "epoch": 0.030726480882785154,
      "grad_norm": 1.6410651206970215,
      "learning_rate": 0.0001939063502245029,
      "loss": 2.0992,
      "step": 575
    },
    {
      "epoch": 0.030779918240842172,
      "grad_norm": 1.3858604431152344,
      "learning_rate": 0.00019389565961086167,
      "loss": 2.3054,
      "step": 576
    },
    {
      "epoch": 0.03083335559889919,
      "grad_norm": 1.4648406505584717,
      "learning_rate": 0.00019388496899722045,
      "loss": 2.1871,
      "step": 577
    },
    {
      "epoch": 0.03088679295695621,
      "grad_norm": 1.4571911096572876,
      "learning_rate": 0.00019387427838357923,
      "loss": 1.9865,
      "step": 578
    },
    {
      "epoch": 0.030940230315013226,
      "grad_norm": 2.2239553928375244,
      "learning_rate": 0.000193863587769938,
      "loss": 2.236,
      "step": 579
    },
    {
      "epoch": 0.030993667673070244,
      "grad_norm": 1.4853217601776123,
      "learning_rate": 0.00019385289715629676,
      "loss": 2.1657,
      "step": 580
    },
    {
      "epoch": 0.031047105031127262,
      "grad_norm": 1.3081672191619873,
      "learning_rate": 0.00019384220654265554,
      "loss": 2.4128,
      "step": 581
    },
    {
      "epoch": 0.03110054238918428,
      "grad_norm": 1.622836709022522,
      "learning_rate": 0.00019383151592901435,
      "loss": 2.045,
      "step": 582
    },
    {
      "epoch": 0.031153979747241295,
      "grad_norm": 1.4885605573654175,
      "learning_rate": 0.0001938208253153731,
      "loss": 2.1411,
      "step": 583
    },
    {
      "epoch": 0.031207417105298313,
      "grad_norm": 1.9271833896636963,
      "learning_rate": 0.00019381013470173188,
      "loss": 2.345,
      "step": 584
    },
    {
      "epoch": 0.03126085446335533,
      "grad_norm": 1.3285125494003296,
      "learning_rate": 0.0001937994440880907,
      "loss": 2.1092,
      "step": 585
    },
    {
      "epoch": 0.03131429182141235,
      "grad_norm": 1.3425664901733398,
      "learning_rate": 0.00019378875347444944,
      "loss": 2.0709,
      "step": 586
    },
    {
      "epoch": 0.03136772917946937,
      "grad_norm": 1.370505690574646,
      "learning_rate": 0.00019377806286080822,
      "loss": 2.1339,
      "step": 587
    },
    {
      "epoch": 0.031421166537526385,
      "grad_norm": 1.2551767826080322,
      "learning_rate": 0.000193767372247167,
      "loss": 2.2462,
      "step": 588
    },
    {
      "epoch": 0.0314746038955834,
      "grad_norm": 1.2221683263778687,
      "learning_rate": 0.00019375668163352578,
      "loss": 2.1341,
      "step": 589
    },
    {
      "epoch": 0.03152804125364042,
      "grad_norm": 1.7038935422897339,
      "learning_rate": 0.00019374599101988456,
      "loss": 2.4229,
      "step": 590
    },
    {
      "epoch": 0.03158147861169744,
      "grad_norm": 1.4849499464035034,
      "learning_rate": 0.0001937353004062433,
      "loss": 2.2407,
      "step": 591
    },
    {
      "epoch": 0.03163491596975446,
      "grad_norm": 1.0743727684020996,
      "learning_rate": 0.0001937246097926021,
      "loss": 2.1336,
      "step": 592
    },
    {
      "epoch": 0.031688353327811475,
      "grad_norm": 1.3986661434173584,
      "learning_rate": 0.0001937139191789609,
      "loss": 2.1598,
      "step": 593
    },
    {
      "epoch": 0.03174179068586849,
      "grad_norm": 1.4312304258346558,
      "learning_rate": 0.00019370322856531965,
      "loss": 2.2655,
      "step": 594
    },
    {
      "epoch": 0.03179522804392551,
      "grad_norm": 1.4784131050109863,
      "learning_rate": 0.00019369253795167843,
      "loss": 2.2395,
      "step": 595
    },
    {
      "epoch": 0.03184866540198253,
      "grad_norm": 1.690409541130066,
      "learning_rate": 0.0001936818473380372,
      "loss": 2.2402,
      "step": 596
    },
    {
      "epoch": 0.03190210276003955,
      "grad_norm": 1.606317400932312,
      "learning_rate": 0.000193671156724396,
      "loss": 2.4965,
      "step": 597
    },
    {
      "epoch": 0.03195554011809656,
      "grad_norm": 1.3836867809295654,
      "learning_rate": 0.00019366046611075477,
      "loss": 2.1531,
      "step": 598
    },
    {
      "epoch": 0.032008977476153576,
      "grad_norm": 1.539427399635315,
      "learning_rate": 0.00019364977549711355,
      "loss": 2.2521,
      "step": 599
    },
    {
      "epoch": 0.032062414834210594,
      "grad_norm": 1.425067663192749,
      "learning_rate": 0.0001936390848834723,
      "loss": 2.4025,
      "step": 600
    },
    {
      "epoch": 0.03211585219226761,
      "grad_norm": 1.8217179775238037,
      "learning_rate": 0.0001936283942698311,
      "loss": 2.2552,
      "step": 601
    },
    {
      "epoch": 0.03216928955032463,
      "grad_norm": 1.4867947101593018,
      "learning_rate": 0.0001936177036561899,
      "loss": 2.4704,
      "step": 602
    },
    {
      "epoch": 0.03222272690838165,
      "grad_norm": 1.1089394092559814,
      "learning_rate": 0.00019360701304254864,
      "loss": 2.042,
      "step": 603
    },
    {
      "epoch": 0.032276164266438666,
      "grad_norm": 1.111799955368042,
      "learning_rate": 0.00019359632242890745,
      "loss": 2.136,
      "step": 604
    },
    {
      "epoch": 0.032329601624495684,
      "grad_norm": 1.902008056640625,
      "learning_rate": 0.0001935856318152662,
      "loss": 2.1942,
      "step": 605
    },
    {
      "epoch": 0.0323830389825527,
      "grad_norm": 1.1513423919677734,
      "learning_rate": 0.00019357494120162498,
      "loss": 2.1654,
      "step": 606
    },
    {
      "epoch": 0.03243647634060972,
      "grad_norm": 1.652424693107605,
      "learning_rate": 0.00019356425058798376,
      "loss": 2.1924,
      "step": 607
    },
    {
      "epoch": 0.03248991369866674,
      "grad_norm": 1.7454652786254883,
      "learning_rate": 0.00019355355997434254,
      "loss": 2.3041,
      "step": 608
    },
    {
      "epoch": 0.032543351056723756,
      "grad_norm": 1.2717794179916382,
      "learning_rate": 0.00019354286936070132,
      "loss": 2.2265,
      "step": 609
    },
    {
      "epoch": 0.032596788414780774,
      "grad_norm": 1.3080018758773804,
      "learning_rate": 0.0001935321787470601,
      "loss": 2.2936,
      "step": 610
    },
    {
      "epoch": 0.03265022577283779,
      "grad_norm": 1.8591549396514893,
      "learning_rate": 0.00019352148813341885,
      "loss": 2.207,
      "step": 611
    },
    {
      "epoch": 0.03270366313089481,
      "grad_norm": 1.2872246503829956,
      "learning_rate": 0.00019351079751977766,
      "loss": 2.2347,
      "step": 612
    },
    {
      "epoch": 0.03275710048895183,
      "grad_norm": 1.4195513725280762,
      "learning_rate": 0.00019350010690613644,
      "loss": 2.2671,
      "step": 613
    },
    {
      "epoch": 0.032810537847008846,
      "grad_norm": 1.4187769889831543,
      "learning_rate": 0.0001934894162924952,
      "loss": 2.2547,
      "step": 614
    },
    {
      "epoch": 0.032863975205065864,
      "grad_norm": 1.4649474620819092,
      "learning_rate": 0.00019347872567885397,
      "loss": 2.1367,
      "step": 615
    },
    {
      "epoch": 0.03291741256312288,
      "grad_norm": 1.4283174276351929,
      "learning_rate": 0.00019346803506521275,
      "loss": 2.1856,
      "step": 616
    },
    {
      "epoch": 0.0329708499211799,
      "grad_norm": 1.2949408292770386,
      "learning_rate": 0.00019345734445157153,
      "loss": 2.2842,
      "step": 617
    },
    {
      "epoch": 0.03302428727923692,
      "grad_norm": 1.3126037120819092,
      "learning_rate": 0.0001934466538379303,
      "loss": 2.1253,
      "step": 618
    },
    {
      "epoch": 0.03307772463729393,
      "grad_norm": 1.140615463256836,
      "learning_rate": 0.00019343596322428906,
      "loss": 2.1382,
      "step": 619
    },
    {
      "epoch": 0.03313116199535095,
      "grad_norm": 1.429225206375122,
      "learning_rate": 0.00019342527261064787,
      "loss": 2.0907,
      "step": 620
    },
    {
      "epoch": 0.033184599353407965,
      "grad_norm": 1.5354669094085693,
      "learning_rate": 0.00019341458199700665,
      "loss": 2.1818,
      "step": 621
    },
    {
      "epoch": 0.03323803671146498,
      "grad_norm": 1.8341164588928223,
      "learning_rate": 0.0001934038913833654,
      "loss": 2.1624,
      "step": 622
    },
    {
      "epoch": 0.033291474069522,
      "grad_norm": 1.8855928182601929,
      "learning_rate": 0.00019339320076972418,
      "loss": 2.3149,
      "step": 623
    },
    {
      "epoch": 0.03334491142757902,
      "grad_norm": 1.4104275703430176,
      "learning_rate": 0.00019338251015608299,
      "loss": 2.1542,
      "step": 624
    },
    {
      "epoch": 0.03339834878563604,
      "grad_norm": 2.076389789581299,
      "learning_rate": 0.00019337181954244174,
      "loss": 2.1413,
      "step": 625
    },
    {
      "epoch": 0.033451786143693055,
      "grad_norm": 1.0135736465454102,
      "learning_rate": 0.00019336112892880052,
      "loss": 2.016,
      "step": 626
    },
    {
      "epoch": 0.03350522350175007,
      "grad_norm": 2.021084785461426,
      "learning_rate": 0.0001933504383151593,
      "loss": 2.3583,
      "step": 627
    },
    {
      "epoch": 0.03355866085980709,
      "grad_norm": 1.93429434299469,
      "learning_rate": 0.00019333974770151808,
      "loss": 2.1787,
      "step": 628
    },
    {
      "epoch": 0.03361209821786411,
      "grad_norm": 1.3149877786636353,
      "learning_rate": 0.00019332905708787686,
      "loss": 2.1245,
      "step": 629
    },
    {
      "epoch": 0.03366553557592113,
      "grad_norm": 1.6661688089370728,
      "learning_rate": 0.00019331836647423564,
      "loss": 2.2005,
      "step": 630
    },
    {
      "epoch": 0.033718972933978145,
      "grad_norm": 1.995787501335144,
      "learning_rate": 0.00019330767586059442,
      "loss": 2.2639,
      "step": 631
    },
    {
      "epoch": 0.03377241029203516,
      "grad_norm": 1.186039924621582,
      "learning_rate": 0.0001932969852469532,
      "loss": 2.1086,
      "step": 632
    },
    {
      "epoch": 0.03382584765009218,
      "grad_norm": 1.4031790494918823,
      "learning_rate": 0.00019328629463331195,
      "loss": 2.0754,
      "step": 633
    },
    {
      "epoch": 0.0338792850081492,
      "grad_norm": 1.5352020263671875,
      "learning_rate": 0.00019327560401967073,
      "loss": 2.2993,
      "step": 634
    },
    {
      "epoch": 0.03393272236620622,
      "grad_norm": 1.470697283744812,
      "learning_rate": 0.00019326491340602954,
      "loss": 2.1864,
      "step": 635
    },
    {
      "epoch": 0.033986159724263235,
      "grad_norm": 1.729833722114563,
      "learning_rate": 0.0001932542227923883,
      "loss": 2.1184,
      "step": 636
    },
    {
      "epoch": 0.03403959708232025,
      "grad_norm": 2.1026992797851562,
      "learning_rate": 0.00019324353217874707,
      "loss": 2.2772,
      "step": 637
    },
    {
      "epoch": 0.03409303444037727,
      "grad_norm": 1.7636154890060425,
      "learning_rate": 0.00019323284156510585,
      "loss": 2.1647,
      "step": 638
    },
    {
      "epoch": 0.03414647179843429,
      "grad_norm": 1.5170955657958984,
      "learning_rate": 0.00019322215095146463,
      "loss": 2.3846,
      "step": 639
    },
    {
      "epoch": 0.0341999091564913,
      "grad_norm": 1.5729316473007202,
      "learning_rate": 0.0001932114603378234,
      "loss": 2.2668,
      "step": 640
    },
    {
      "epoch": 0.03425334651454832,
      "grad_norm": 1.5550216436386108,
      "learning_rate": 0.00019320076972418219,
      "loss": 2.2269,
      "step": 641
    },
    {
      "epoch": 0.034306783872605336,
      "grad_norm": 1.9758868217468262,
      "learning_rate": 0.00019319007911054094,
      "loss": 2.4448,
      "step": 642
    },
    {
      "epoch": 0.034360221230662354,
      "grad_norm": 1.2465736865997314,
      "learning_rate": 0.00019317938849689975,
      "loss": 2.1765,
      "step": 643
    },
    {
      "epoch": 0.03441365858871937,
      "grad_norm": 1.9220821857452393,
      "learning_rate": 0.0001931686978832585,
      "loss": 2.3592,
      "step": 644
    },
    {
      "epoch": 0.03446709594677639,
      "grad_norm": 1.3518892526626587,
      "learning_rate": 0.00019315800726961728,
      "loss": 2.306,
      "step": 645
    },
    {
      "epoch": 0.03452053330483341,
      "grad_norm": 1.3050446510314941,
      "learning_rate": 0.00019314731665597606,
      "loss": 2.1905,
      "step": 646
    },
    {
      "epoch": 0.034573970662890426,
      "grad_norm": 1.6217588186264038,
      "learning_rate": 0.00019313662604233484,
      "loss": 2.3406,
      "step": 647
    },
    {
      "epoch": 0.034627408020947444,
      "grad_norm": 1.060975193977356,
      "learning_rate": 0.00019312593542869362,
      "loss": 2.1587,
      "step": 648
    },
    {
      "epoch": 0.03468084537900446,
      "grad_norm": 1.7444885969161987,
      "learning_rate": 0.0001931152448150524,
      "loss": 2.2488,
      "step": 649
    },
    {
      "epoch": 0.03473428273706148,
      "grad_norm": 1.7485138177871704,
      "learning_rate": 0.00019310455420141118,
      "loss": 2.4073,
      "step": 650
    },
    {
      "epoch": 0.0347877200951185,
      "grad_norm": 1.705076813697815,
      "learning_rate": 0.00019309386358776996,
      "loss": 2.5313,
      "step": 651
    },
    {
      "epoch": 0.034841157453175516,
      "grad_norm": 1.5759954452514648,
      "learning_rate": 0.00019308317297412873,
      "loss": 2.1433,
      "step": 652
    },
    {
      "epoch": 0.034894594811232534,
      "grad_norm": 1.3418350219726562,
      "learning_rate": 0.0001930724823604875,
      "loss": 2.3001,
      "step": 653
    },
    {
      "epoch": 0.03494803216928955,
      "grad_norm": 1.7868655920028687,
      "learning_rate": 0.0001930617917468463,
      "loss": 2.3823,
      "step": 654
    },
    {
      "epoch": 0.03500146952734657,
      "grad_norm": 1.2001113891601562,
      "learning_rate": 0.00019305110113320505,
      "loss": 2.238,
      "step": 655
    },
    {
      "epoch": 0.03505490688540359,
      "grad_norm": 1.2167582511901855,
      "learning_rate": 0.00019304041051956383,
      "loss": 2.3818,
      "step": 656
    },
    {
      "epoch": 0.035108344243460606,
      "grad_norm": 1.3483376502990723,
      "learning_rate": 0.0001930297199059226,
      "loss": 2.1862,
      "step": 657
    },
    {
      "epoch": 0.035161781601517623,
      "grad_norm": 1.822011113166809,
      "learning_rate": 0.00019301902929228139,
      "loss": 2.2824,
      "step": 658
    },
    {
      "epoch": 0.03521521895957464,
      "grad_norm": 1.8485299348831177,
      "learning_rate": 0.00019300833867864017,
      "loss": 2.3416,
      "step": 659
    },
    {
      "epoch": 0.03526865631763166,
      "grad_norm": 1.7511745691299438,
      "learning_rate": 0.00019299764806499894,
      "loss": 2.2131,
      "step": 660
    },
    {
      "epoch": 0.03532209367568867,
      "grad_norm": 1.2927852869033813,
      "learning_rate": 0.0001929869574513577,
      "loss": 2.084,
      "step": 661
    },
    {
      "epoch": 0.03537553103374569,
      "grad_norm": 1.4853521585464478,
      "learning_rate": 0.0001929762668377165,
      "loss": 2.3778,
      "step": 662
    },
    {
      "epoch": 0.035428968391802707,
      "grad_norm": 1.2401342391967773,
      "learning_rate": 0.00019296557622407528,
      "loss": 2.275,
      "step": 663
    },
    {
      "epoch": 0.035482405749859725,
      "grad_norm": 1.3119258880615234,
      "learning_rate": 0.00019295488561043404,
      "loss": 2.2896,
      "step": 664
    },
    {
      "epoch": 0.03553584310791674,
      "grad_norm": 1.374569296836853,
      "learning_rate": 0.00019294419499679282,
      "loss": 2.0259,
      "step": 665
    },
    {
      "epoch": 0.03558928046597376,
      "grad_norm": 1.3899859189987183,
      "learning_rate": 0.0001929335043831516,
      "loss": 2.1519,
      "step": 666
    },
    {
      "epoch": 0.03564271782403078,
      "grad_norm": 1.5045347213745117,
      "learning_rate": 0.00019292281376951038,
      "loss": 2.1474,
      "step": 667
    },
    {
      "epoch": 0.035696155182087796,
      "grad_norm": 2.090165138244629,
      "learning_rate": 0.00019291212315586916,
      "loss": 2.2972,
      "step": 668
    },
    {
      "epoch": 0.035749592540144814,
      "grad_norm": 1.476009726524353,
      "learning_rate": 0.00019290143254222793,
      "loss": 2.2511,
      "step": 669
    },
    {
      "epoch": 0.03580302989820183,
      "grad_norm": 1.694032073020935,
      "learning_rate": 0.00019289074192858671,
      "loss": 2.193,
      "step": 670
    },
    {
      "epoch": 0.03585646725625885,
      "grad_norm": 1.730031967163086,
      "learning_rate": 0.0001928800513149455,
      "loss": 2.2926,
      "step": 671
    },
    {
      "epoch": 0.03590990461431587,
      "grad_norm": 1.614108920097351,
      "learning_rate": 0.00019286936070130425,
      "loss": 2.4566,
      "step": 672
    },
    {
      "epoch": 0.035963341972372886,
      "grad_norm": 2.556645393371582,
      "learning_rate": 0.00019285867008766305,
      "loss": 2.3967,
      "step": 673
    },
    {
      "epoch": 0.036016779330429904,
      "grad_norm": 2.056448459625244,
      "learning_rate": 0.00019284797947402183,
      "loss": 2.4301,
      "step": 674
    },
    {
      "epoch": 0.03607021668848692,
      "grad_norm": 1.5918775796890259,
      "learning_rate": 0.00019283728886038059,
      "loss": 2.1932,
      "step": 675
    },
    {
      "epoch": 0.03612365404654394,
      "grad_norm": 1.3888092041015625,
      "learning_rate": 0.00019282659824673937,
      "loss": 2.0333,
      "step": 676
    },
    {
      "epoch": 0.03617709140460096,
      "grad_norm": 1.3957209587097168,
      "learning_rate": 0.00019281590763309817,
      "loss": 2.0205,
      "step": 677
    },
    {
      "epoch": 0.036230528762657976,
      "grad_norm": 1.5049580335617065,
      "learning_rate": 0.00019280521701945692,
      "loss": 2.2625,
      "step": 678
    },
    {
      "epoch": 0.036283966120714994,
      "grad_norm": 1.2437160015106201,
      "learning_rate": 0.0001927945264058157,
      "loss": 2.2045,
      "step": 679
    },
    {
      "epoch": 0.03633740347877201,
      "grad_norm": 1.089208960533142,
      "learning_rate": 0.00019278383579217448,
      "loss": 1.9988,
      "step": 680
    },
    {
      "epoch": 0.03639084083682903,
      "grad_norm": 1.801600456237793,
      "learning_rate": 0.00019277314517853326,
      "loss": 2.3234,
      "step": 681
    },
    {
      "epoch": 0.03644427819488604,
      "grad_norm": 1.5907106399536133,
      "learning_rate": 0.00019276245456489204,
      "loss": 2.1304,
      "step": 682
    },
    {
      "epoch": 0.03649771555294306,
      "grad_norm": 1.4452266693115234,
      "learning_rate": 0.0001927517639512508,
      "loss": 2.183,
      "step": 683
    },
    {
      "epoch": 0.03655115291100008,
      "grad_norm": 1.920575499534607,
      "learning_rate": 0.00019274107333760958,
      "loss": 2.2345,
      "step": 684
    },
    {
      "epoch": 0.036604590269057095,
      "grad_norm": 1.465707778930664,
      "learning_rate": 0.00019273038272396838,
      "loss": 2.4336,
      "step": 685
    },
    {
      "epoch": 0.03665802762711411,
      "grad_norm": 1.324928879737854,
      "learning_rate": 0.00019271969211032713,
      "loss": 2.0683,
      "step": 686
    },
    {
      "epoch": 0.03671146498517113,
      "grad_norm": 1.1896140575408936,
      "learning_rate": 0.00019270900149668591,
      "loss": 2.1509,
      "step": 687
    },
    {
      "epoch": 0.03676490234322815,
      "grad_norm": 1.3168636560440063,
      "learning_rate": 0.0001926983108830447,
      "loss": 2.1888,
      "step": 688
    },
    {
      "epoch": 0.03681833970128517,
      "grad_norm": 1.6046332120895386,
      "learning_rate": 0.00019268762026940347,
      "loss": 2.3692,
      "step": 689
    },
    {
      "epoch": 0.036871777059342185,
      "grad_norm": 1.7971361875534058,
      "learning_rate": 0.00019267692965576225,
      "loss": 2.3852,
      "step": 690
    },
    {
      "epoch": 0.0369252144173992,
      "grad_norm": 0.9796358942985535,
      "learning_rate": 0.00019266623904212103,
      "loss": 2.1712,
      "step": 691
    },
    {
      "epoch": 0.03697865177545622,
      "grad_norm": 0.7618374228477478,
      "learning_rate": 0.00019265554842847979,
      "loss": 1.8588,
      "step": 692
    },
    {
      "epoch": 0.03703208913351324,
      "grad_norm": 1.5379258394241333,
      "learning_rate": 0.0001926448578148386,
      "loss": 2.4374,
      "step": 693
    },
    {
      "epoch": 0.03708552649157026,
      "grad_norm": 1.6948246955871582,
      "learning_rate": 0.00019263416720119734,
      "loss": 2.2216,
      "step": 694
    },
    {
      "epoch": 0.037138963849627275,
      "grad_norm": 1.5116922855377197,
      "learning_rate": 0.00019262347658755612,
      "loss": 2.2677,
      "step": 695
    },
    {
      "epoch": 0.03719240120768429,
      "grad_norm": 1.3563711643218994,
      "learning_rate": 0.00019261278597391493,
      "loss": 2.0893,
      "step": 696
    },
    {
      "epoch": 0.03724583856574131,
      "grad_norm": 1.4171955585479736,
      "learning_rate": 0.00019260209536027368,
      "loss": 2.3012,
      "step": 697
    },
    {
      "epoch": 0.03729927592379833,
      "grad_norm": 1.3919068574905396,
      "learning_rate": 0.00019259140474663246,
      "loss": 2.2228,
      "step": 698
    },
    {
      "epoch": 0.03735271328185535,
      "grad_norm": 1.4732645750045776,
      "learning_rate": 0.00019258071413299124,
      "loss": 2.12,
      "step": 699
    },
    {
      "epoch": 0.037406150639912365,
      "grad_norm": 1.150924801826477,
      "learning_rate": 0.00019257002351935002,
      "loss": 2.0477,
      "step": 700
    },
    {
      "epoch": 0.03745958799796938,
      "grad_norm": 1.508605718612671,
      "learning_rate": 0.0001925593329057088,
      "loss": 2.3267,
      "step": 701
    },
    {
      "epoch": 0.0375130253560264,
      "grad_norm": 1.3584378957748413,
      "learning_rate": 0.00019254864229206758,
      "loss": 2.1193,
      "step": 702
    },
    {
      "epoch": 0.03756646271408341,
      "grad_norm": 1.12851083278656,
      "learning_rate": 0.00019253795167842633,
      "loss": 2.0581,
      "step": 703
    },
    {
      "epoch": 0.03761990007214043,
      "grad_norm": 1.6076037883758545,
      "learning_rate": 0.00019252726106478514,
      "loss": 2.2609,
      "step": 704
    },
    {
      "epoch": 0.03767333743019745,
      "grad_norm": 1.305631160736084,
      "learning_rate": 0.00019251657045114392,
      "loss": 2.1421,
      "step": 705
    },
    {
      "epoch": 0.037726774788254466,
      "grad_norm": 1.3302642107009888,
      "learning_rate": 0.00019250587983750267,
      "loss": 2.1032,
      "step": 706
    },
    {
      "epoch": 0.037780212146311484,
      "grad_norm": 1.2037323713302612,
      "learning_rate": 0.00019249518922386145,
      "loss": 2.1911,
      "step": 707
    },
    {
      "epoch": 0.0378336495043685,
      "grad_norm": 1.983379602432251,
      "learning_rate": 0.00019248449861022023,
      "loss": 2.4865,
      "step": 708
    },
    {
      "epoch": 0.03788708686242552,
      "grad_norm": 1.8941936492919922,
      "learning_rate": 0.000192473807996579,
      "loss": 2.2883,
      "step": 709
    },
    {
      "epoch": 0.03794052422048254,
      "grad_norm": 1.5908396244049072,
      "learning_rate": 0.0001924631173829378,
      "loss": 2.3647,
      "step": 710
    },
    {
      "epoch": 0.037993961578539556,
      "grad_norm": 1.6741591691970825,
      "learning_rate": 0.00019245242676929654,
      "loss": 2.3694,
      "step": 711
    },
    {
      "epoch": 0.038047398936596574,
      "grad_norm": 1.471516489982605,
      "learning_rate": 0.00019244173615565535,
      "loss": 2.1786,
      "step": 712
    },
    {
      "epoch": 0.03810083629465359,
      "grad_norm": 1.148306131362915,
      "learning_rate": 0.00019243104554201413,
      "loss": 1.9337,
      "step": 713
    },
    {
      "epoch": 0.03815427365271061,
      "grad_norm": 1.1475337743759155,
      "learning_rate": 0.00019242035492837288,
      "loss": 2.1267,
      "step": 714
    },
    {
      "epoch": 0.03820771101076763,
      "grad_norm": 1.7932006120681763,
      "learning_rate": 0.0001924096643147317,
      "loss": 2.4304,
      "step": 715
    },
    {
      "epoch": 0.038261148368824646,
      "grad_norm": 1.6503653526306152,
      "learning_rate": 0.00019239897370109047,
      "loss": 2.2993,
      "step": 716
    },
    {
      "epoch": 0.038314585726881664,
      "grad_norm": 1.4143157005310059,
      "learning_rate": 0.00019238828308744922,
      "loss": 2.1607,
      "step": 717
    },
    {
      "epoch": 0.03836802308493868,
      "grad_norm": 1.7323933839797974,
      "learning_rate": 0.000192377592473808,
      "loss": 2.4244,
      "step": 718
    },
    {
      "epoch": 0.0384214604429957,
      "grad_norm": 1.5399236679077148,
      "learning_rate": 0.00019236690186016678,
      "loss": 2.4355,
      "step": 719
    },
    {
      "epoch": 0.03847489780105272,
      "grad_norm": 1.2913134098052979,
      "learning_rate": 0.00019235621124652556,
      "loss": 2.0885,
      "step": 720
    },
    {
      "epoch": 0.038528335159109736,
      "grad_norm": 1.8611347675323486,
      "learning_rate": 0.00019234552063288434,
      "loss": 2.3302,
      "step": 721
    },
    {
      "epoch": 0.038581772517166754,
      "grad_norm": 1.376860499382019,
      "learning_rate": 0.0001923348300192431,
      "loss": 2.4387,
      "step": 722
    },
    {
      "epoch": 0.03863520987522377,
      "grad_norm": 1.3544220924377441,
      "learning_rate": 0.0001923241394056019,
      "loss": 2.3295,
      "step": 723
    },
    {
      "epoch": 0.03868864723328078,
      "grad_norm": 1.8229951858520508,
      "learning_rate": 0.00019231344879196068,
      "loss": 2.3541,
      "step": 724
    },
    {
      "epoch": 0.0387420845913378,
      "grad_norm": 0.8953661918640137,
      "learning_rate": 0.00019230275817831943,
      "loss": 2.1561,
      "step": 725
    },
    {
      "epoch": 0.03879552194939482,
      "grad_norm": 2.336698293685913,
      "learning_rate": 0.0001922920675646782,
      "loss": 2.2908,
      "step": 726
    },
    {
      "epoch": 0.03884895930745184,
      "grad_norm": 1.4190393686294556,
      "learning_rate": 0.00019228137695103702,
      "loss": 2.1746,
      "step": 727
    },
    {
      "epoch": 0.038902396665508855,
      "grad_norm": 1.3974239826202393,
      "learning_rate": 0.00019227068633739577,
      "loss": 2.3588,
      "step": 728
    },
    {
      "epoch": 0.03895583402356587,
      "grad_norm": 1.7201000452041626,
      "learning_rate": 0.00019225999572375455,
      "loss": 2.2868,
      "step": 729
    },
    {
      "epoch": 0.03900927138162289,
      "grad_norm": 1.2189627885818481,
      "learning_rate": 0.00019224930511011333,
      "loss": 2.1488,
      "step": 730
    },
    {
      "epoch": 0.03906270873967991,
      "grad_norm": 1.4673552513122559,
      "learning_rate": 0.0001922386144964721,
      "loss": 2.2888,
      "step": 731
    },
    {
      "epoch": 0.03911614609773693,
      "grad_norm": 1.4084268808364868,
      "learning_rate": 0.0001922279238828309,
      "loss": 1.9699,
      "step": 732
    },
    {
      "epoch": 0.039169583455793945,
      "grad_norm": 1.5589308738708496,
      "learning_rate": 0.00019221723326918964,
      "loss": 2.2258,
      "step": 733
    },
    {
      "epoch": 0.03922302081385096,
      "grad_norm": 1.6125773191452026,
      "learning_rate": 0.00019220654265554842,
      "loss": 2.302,
      "step": 734
    },
    {
      "epoch": 0.03927645817190798,
      "grad_norm": 1.2845243215560913,
      "learning_rate": 0.00019219585204190723,
      "loss": 2.2323,
      "step": 735
    },
    {
      "epoch": 0.039329895529965,
      "grad_norm": 1.1935590505599976,
      "learning_rate": 0.00019218516142826598,
      "loss": 2.1067,
      "step": 736
    },
    {
      "epoch": 0.03938333288802202,
      "grad_norm": 1.6254839897155762,
      "learning_rate": 0.00019217447081462476,
      "loss": 2.2107,
      "step": 737
    },
    {
      "epoch": 0.039436770246079035,
      "grad_norm": 1.056924819946289,
      "learning_rate": 0.00019216378020098357,
      "loss": 2.1193,
      "step": 738
    },
    {
      "epoch": 0.03949020760413605,
      "grad_norm": 1.818843960762024,
      "learning_rate": 0.00019215308958734232,
      "loss": 2.1516,
      "step": 739
    },
    {
      "epoch": 0.03954364496219307,
      "grad_norm": 1.5182384252548218,
      "learning_rate": 0.0001921423989737011,
      "loss": 2.1917,
      "step": 740
    },
    {
      "epoch": 0.03959708232025009,
      "grad_norm": 1.2093291282653809,
      "learning_rate": 0.00019213170836005988,
      "loss": 2.0492,
      "step": 741
    },
    {
      "epoch": 0.03965051967830711,
      "grad_norm": 1.693192958831787,
      "learning_rate": 0.00019212101774641866,
      "loss": 2.3007,
      "step": 742
    },
    {
      "epoch": 0.039703957036364125,
      "grad_norm": 1.4848439693450928,
      "learning_rate": 0.00019211032713277744,
      "loss": 2.0549,
      "step": 743
    },
    {
      "epoch": 0.03975739439442114,
      "grad_norm": 1.4457166194915771,
      "learning_rate": 0.00019209963651913622,
      "loss": 2.1888,
      "step": 744
    },
    {
      "epoch": 0.039810831752478154,
      "grad_norm": 1.3603407144546509,
      "learning_rate": 0.00019208894590549497,
      "loss": 2.1078,
      "step": 745
    },
    {
      "epoch": 0.03986426911053517,
      "grad_norm": 1.4642853736877441,
      "learning_rate": 0.00019207825529185378,
      "loss": 2.4174,
      "step": 746
    },
    {
      "epoch": 0.03991770646859219,
      "grad_norm": 1.656751036643982,
      "learning_rate": 0.00019206756467821253,
      "loss": 2.3446,
      "step": 747
    },
    {
      "epoch": 0.03997114382664921,
      "grad_norm": 1.841557264328003,
      "learning_rate": 0.0001920568740645713,
      "loss": 2.194,
      "step": 748
    },
    {
      "epoch": 0.040024581184706226,
      "grad_norm": 1.2078144550323486,
      "learning_rate": 0.0001920461834509301,
      "loss": 1.9999,
      "step": 749
    },
    {
      "epoch": 0.040078018542763244,
      "grad_norm": 1.5541423559188843,
      "learning_rate": 0.00019203549283728887,
      "loss": 2.2954,
      "step": 750
    },
    {
      "epoch": 0.04013145590082026,
      "grad_norm": 1.7217576503753662,
      "learning_rate": 0.00019202480222364765,
      "loss": 2.2379,
      "step": 751
    },
    {
      "epoch": 0.04018489325887728,
      "grad_norm": 1.4468308687210083,
      "learning_rate": 0.00019201411161000643,
      "loss": 2.1967,
      "step": 752
    },
    {
      "epoch": 0.0402383306169343,
      "grad_norm": 1.4045405387878418,
      "learning_rate": 0.00019200342099636518,
      "loss": 2.2211,
      "step": 753
    },
    {
      "epoch": 0.040291767974991316,
      "grad_norm": 1.4573277235031128,
      "learning_rate": 0.000191992730382724,
      "loss": 2.0269,
      "step": 754
    },
    {
      "epoch": 0.040345205333048334,
      "grad_norm": 1.2338197231292725,
      "learning_rate": 0.00019198203976908277,
      "loss": 2.0608,
      "step": 755
    },
    {
      "epoch": 0.04039864269110535,
      "grad_norm": 1.498403787612915,
      "learning_rate": 0.00019197134915544152,
      "loss": 2.1584,
      "step": 756
    },
    {
      "epoch": 0.04045208004916237,
      "grad_norm": 1.1624724864959717,
      "learning_rate": 0.0001919606585418003,
      "loss": 2.144,
      "step": 757
    },
    {
      "epoch": 0.04050551740721939,
      "grad_norm": 2.293793201446533,
      "learning_rate": 0.00019194996792815908,
      "loss": 2.4298,
      "step": 758
    },
    {
      "epoch": 0.040558954765276406,
      "grad_norm": 1.3676292896270752,
      "learning_rate": 0.00019193927731451786,
      "loss": 2.3048,
      "step": 759
    },
    {
      "epoch": 0.040612392123333424,
      "grad_norm": 1.639336347579956,
      "learning_rate": 0.00019192858670087664,
      "loss": 1.9894,
      "step": 760
    },
    {
      "epoch": 0.04066582948139044,
      "grad_norm": 1.1904668807983398,
      "learning_rate": 0.00019191789608723542,
      "loss": 2.1527,
      "step": 761
    },
    {
      "epoch": 0.04071926683944746,
      "grad_norm": 1.6730982065200806,
      "learning_rate": 0.0001919072054735942,
      "loss": 2.3292,
      "step": 762
    },
    {
      "epoch": 0.04077270419750448,
      "grad_norm": 1.425299048423767,
      "learning_rate": 0.00019189651485995298,
      "loss": 2.2435,
      "step": 763
    },
    {
      "epoch": 0.040826141555561496,
      "grad_norm": 1.3476836681365967,
      "learning_rate": 0.00019188582424631173,
      "loss": 2.1123,
      "step": 764
    },
    {
      "epoch": 0.040879578913618514,
      "grad_norm": 1.9108693599700928,
      "learning_rate": 0.00019187513363267054,
      "loss": 2.0987,
      "step": 765
    },
    {
      "epoch": 0.040933016271675525,
      "grad_norm": 1.2677574157714844,
      "learning_rate": 0.00019186444301902932,
      "loss": 2.2587,
      "step": 766
    },
    {
      "epoch": 0.04098645362973254,
      "grad_norm": 1.3521994352340698,
      "learning_rate": 0.00019185375240538807,
      "loss": 2.2967,
      "step": 767
    },
    {
      "epoch": 0.04103989098778956,
      "grad_norm": 1.1340030431747437,
      "learning_rate": 0.00019184306179174685,
      "loss": 1.9892,
      "step": 768
    },
    {
      "epoch": 0.04109332834584658,
      "grad_norm": 1.6534284353256226,
      "learning_rate": 0.00019183237117810563,
      "loss": 2.3923,
      "step": 769
    },
    {
      "epoch": 0.0411467657039036,
      "grad_norm": 1.613741397857666,
      "learning_rate": 0.0001918216805644644,
      "loss": 2.2854,
      "step": 770
    },
    {
      "epoch": 0.041200203061960615,
      "grad_norm": 1.1695085763931274,
      "learning_rate": 0.0001918109899508232,
      "loss": 2.2447,
      "step": 771
    },
    {
      "epoch": 0.04125364042001763,
      "grad_norm": 1.3475147485733032,
      "learning_rate": 0.00019180029933718197,
      "loss": 2.0667,
      "step": 772
    },
    {
      "epoch": 0.04130707777807465,
      "grad_norm": 1.9056130647659302,
      "learning_rate": 0.00019178960872354075,
      "loss": 2.293,
      "step": 773
    },
    {
      "epoch": 0.04136051513613167,
      "grad_norm": 1.151181936264038,
      "learning_rate": 0.00019177891810989953,
      "loss": 2.1148,
      "step": 774
    },
    {
      "epoch": 0.04141395249418869,
      "grad_norm": 1.8362210988998413,
      "learning_rate": 0.00019176822749625828,
      "loss": 2.4059,
      "step": 775
    },
    {
      "epoch": 0.041467389852245705,
      "grad_norm": 1.7280151844024658,
      "learning_rate": 0.00019175753688261706,
      "loss": 2.3578,
      "step": 776
    },
    {
      "epoch": 0.04152082721030272,
      "grad_norm": 2.2320806980133057,
      "learning_rate": 0.00019174684626897587,
      "loss": 2.2724,
      "step": 777
    },
    {
      "epoch": 0.04157426456835974,
      "grad_norm": 1.4135725498199463,
      "learning_rate": 0.00019173615565533462,
      "loss": 2.1877,
      "step": 778
    },
    {
      "epoch": 0.04162770192641676,
      "grad_norm": 1.0510939359664917,
      "learning_rate": 0.0001917254650416934,
      "loss": 2.1361,
      "step": 779
    },
    {
      "epoch": 0.04168113928447378,
      "grad_norm": 1.3365904092788696,
      "learning_rate": 0.00019171477442805218,
      "loss": 2.2069,
      "step": 780
    },
    {
      "epoch": 0.041734576642530795,
      "grad_norm": 1.6508723497390747,
      "learning_rate": 0.00019170408381441096,
      "loss": 2.3634,
      "step": 781
    },
    {
      "epoch": 0.04178801400058781,
      "grad_norm": 1.8848603963851929,
      "learning_rate": 0.00019169339320076974,
      "loss": 2.3596,
      "step": 782
    },
    {
      "epoch": 0.04184145135864483,
      "grad_norm": 1.788233757019043,
      "learning_rate": 0.00019168270258712852,
      "loss": 2.4034,
      "step": 783
    },
    {
      "epoch": 0.04189488871670185,
      "grad_norm": 2.407042980194092,
      "learning_rate": 0.0001916720119734873,
      "loss": 2.4255,
      "step": 784
    },
    {
      "epoch": 0.04194832607475887,
      "grad_norm": 1.8590139150619507,
      "learning_rate": 0.00019166132135984608,
      "loss": 2.2404,
      "step": 785
    },
    {
      "epoch": 0.042001763432815885,
      "grad_norm": 1.8157464265823364,
      "learning_rate": 0.00019165063074620483,
      "loss": 2.3647,
      "step": 786
    },
    {
      "epoch": 0.042055200790872896,
      "grad_norm": 1.4765375852584839,
      "learning_rate": 0.0001916399401325636,
      "loss": 2.2506,
      "step": 787
    },
    {
      "epoch": 0.042108638148929914,
      "grad_norm": 1.6450854539871216,
      "learning_rate": 0.00019162924951892241,
      "loss": 2.3603,
      "step": 788
    },
    {
      "epoch": 0.04216207550698693,
      "grad_norm": 1.5490843057632446,
      "learning_rate": 0.00019161855890528117,
      "loss": 2.1356,
      "step": 789
    },
    {
      "epoch": 0.04221551286504395,
      "grad_norm": 1.3291841745376587,
      "learning_rate": 0.00019160786829163995,
      "loss": 1.9431,
      "step": 790
    },
    {
      "epoch": 0.04226895022310097,
      "grad_norm": 1.686212182044983,
      "learning_rate": 0.00019159717767799873,
      "loss": 2.2573,
      "step": 791
    },
    {
      "epoch": 0.042322387581157986,
      "grad_norm": 1.4204866886138916,
      "learning_rate": 0.0001915864870643575,
      "loss": 1.9285,
      "step": 792
    },
    {
      "epoch": 0.042375824939215004,
      "grad_norm": 1.5252166986465454,
      "learning_rate": 0.00019157579645071629,
      "loss": 2.093,
      "step": 793
    },
    {
      "epoch": 0.04242926229727202,
      "grad_norm": 1.8324730396270752,
      "learning_rate": 0.00019156510583707507,
      "loss": 2.4512,
      "step": 794
    },
    {
      "epoch": 0.04248269965532904,
      "grad_norm": 1.4690618515014648,
      "learning_rate": 0.00019155441522343382,
      "loss": 2.2151,
      "step": 795
    },
    {
      "epoch": 0.04253613701338606,
      "grad_norm": 1.726744532585144,
      "learning_rate": 0.00019154372460979262,
      "loss": 2.2535,
      "step": 796
    },
    {
      "epoch": 0.042589574371443076,
      "grad_norm": 1.6687337160110474,
      "learning_rate": 0.00019153303399615138,
      "loss": 2.3831,
      "step": 797
    },
    {
      "epoch": 0.042643011729500094,
      "grad_norm": 1.2972661256790161,
      "learning_rate": 0.00019152234338251016,
      "loss": 2.1538,
      "step": 798
    },
    {
      "epoch": 0.04269644908755711,
      "grad_norm": 1.4154998064041138,
      "learning_rate": 0.00019151165276886894,
      "loss": 2.3319,
      "step": 799
    },
    {
      "epoch": 0.04274988644561413,
      "grad_norm": 1.2806435823440552,
      "learning_rate": 0.00019150096215522772,
      "loss": 2.0582,
      "step": 800
    },
    {
      "epoch": 0.04280332380367115,
      "grad_norm": 1.2283363342285156,
      "learning_rate": 0.0001914902715415865,
      "loss": 2.2156,
      "step": 801
    },
    {
      "epoch": 0.042856761161728166,
      "grad_norm": 1.7168383598327637,
      "learning_rate": 0.00019147958092794528,
      "loss": 2.2695,
      "step": 802
    },
    {
      "epoch": 0.042910198519785184,
      "grad_norm": 1.2420209646224976,
      "learning_rate": 0.00019146889031430406,
      "loss": 2.397,
      "step": 803
    },
    {
      "epoch": 0.0429636358778422,
      "grad_norm": 1.5471601486206055,
      "learning_rate": 0.00019145819970066283,
      "loss": 2.43,
      "step": 804
    },
    {
      "epoch": 0.04301707323589922,
      "grad_norm": 1.5725760459899902,
      "learning_rate": 0.00019144750908702161,
      "loss": 2.099,
      "step": 805
    },
    {
      "epoch": 0.04307051059395624,
      "grad_norm": 1.104733943939209,
      "learning_rate": 0.00019143681847338037,
      "loss": 2.1426,
      "step": 806
    },
    {
      "epoch": 0.043123947952013256,
      "grad_norm": 1.8186167478561401,
      "learning_rate": 0.00019142612785973917,
      "loss": 2.3944,
      "step": 807
    },
    {
      "epoch": 0.04317738531007027,
      "grad_norm": 1.355236291885376,
      "learning_rate": 0.00019141543724609793,
      "loss": 2.3784,
      "step": 808
    },
    {
      "epoch": 0.043230822668127285,
      "grad_norm": 2.256197690963745,
      "learning_rate": 0.0001914047466324567,
      "loss": 2.1467,
      "step": 809
    },
    {
      "epoch": 0.0432842600261843,
      "grad_norm": 1.4791826009750366,
      "learning_rate": 0.00019139405601881549,
      "loss": 2.4082,
      "step": 810
    },
    {
      "epoch": 0.04333769738424132,
      "grad_norm": 1.800547480583191,
      "learning_rate": 0.00019138336540517427,
      "loss": 2.2705,
      "step": 811
    },
    {
      "epoch": 0.04339113474229834,
      "grad_norm": 1.5285699367523193,
      "learning_rate": 0.00019137267479153305,
      "loss": 2.1457,
      "step": 812
    },
    {
      "epoch": 0.04344457210035536,
      "grad_norm": 1.6755547523498535,
      "learning_rate": 0.00019136198417789182,
      "loss": 2.0833,
      "step": 813
    },
    {
      "epoch": 0.043498009458412375,
      "grad_norm": 1.2386072874069214,
      "learning_rate": 0.00019135129356425058,
      "loss": 2.0075,
      "step": 814
    },
    {
      "epoch": 0.04355144681646939,
      "grad_norm": 1.4656533002853394,
      "learning_rate": 0.00019134060295060938,
      "loss": 2.2811,
      "step": 815
    },
    {
      "epoch": 0.04360488417452641,
      "grad_norm": 1.4681707620620728,
      "learning_rate": 0.00019132991233696816,
      "loss": 2.1466,
      "step": 816
    },
    {
      "epoch": 0.04365832153258343,
      "grad_norm": 0.9956331253051758,
      "learning_rate": 0.00019131922172332692,
      "loss": 2.105,
      "step": 817
    },
    {
      "epoch": 0.04371175889064045,
      "grad_norm": 1.176392674446106,
      "learning_rate": 0.0001913085311096857,
      "loss": 2.26,
      "step": 818
    },
    {
      "epoch": 0.043765196248697465,
      "grad_norm": 1.4165987968444824,
      "learning_rate": 0.0001912978404960445,
      "loss": 2.0563,
      "step": 819
    },
    {
      "epoch": 0.04381863360675448,
      "grad_norm": 1.5038751363754272,
      "learning_rate": 0.00019128714988240326,
      "loss": 2.1468,
      "step": 820
    },
    {
      "epoch": 0.0438720709648115,
      "grad_norm": 1.5201972723007202,
      "learning_rate": 0.00019127645926876203,
      "loss": 2.0273,
      "step": 821
    },
    {
      "epoch": 0.04392550832286852,
      "grad_norm": 1.5950446128845215,
      "learning_rate": 0.00019126576865512081,
      "loss": 2.2953,
      "step": 822
    },
    {
      "epoch": 0.04397894568092554,
      "grad_norm": 1.701784372329712,
      "learning_rate": 0.0001912550780414796,
      "loss": 2.1479,
      "step": 823
    },
    {
      "epoch": 0.044032383038982555,
      "grad_norm": 1.7777940034866333,
      "learning_rate": 0.00019124438742783837,
      "loss": 2.3859,
      "step": 824
    },
    {
      "epoch": 0.04408582039703957,
      "grad_norm": 1.4215927124023438,
      "learning_rate": 0.00019123369681419713,
      "loss": 2.1287,
      "step": 825
    },
    {
      "epoch": 0.04413925775509659,
      "grad_norm": 2.051622152328491,
      "learning_rate": 0.00019122300620055593,
      "loss": 2.2637,
      "step": 826
    },
    {
      "epoch": 0.04419269511315361,
      "grad_norm": 1.7996653318405151,
      "learning_rate": 0.0001912123155869147,
      "loss": 2.3269,
      "step": 827
    },
    {
      "epoch": 0.04424613247121063,
      "grad_norm": 1.6126220226287842,
      "learning_rate": 0.00019120162497327347,
      "loss": 2.0697,
      "step": 828
    },
    {
      "epoch": 0.04429956982926764,
      "grad_norm": 1.6810160875320435,
      "learning_rate": 0.00019119093435963224,
      "loss": 2.1977,
      "step": 829
    },
    {
      "epoch": 0.044353007187324656,
      "grad_norm": 2.4618430137634277,
      "learning_rate": 0.00019118024374599105,
      "loss": 2.43,
      "step": 830
    },
    {
      "epoch": 0.044406444545381674,
      "grad_norm": 1.3398199081420898,
      "learning_rate": 0.0001911695531323498,
      "loss": 2.1197,
      "step": 831
    },
    {
      "epoch": 0.04445988190343869,
      "grad_norm": 1.7551062107086182,
      "learning_rate": 0.00019115886251870858,
      "loss": 2.3633,
      "step": 832
    },
    {
      "epoch": 0.04451331926149571,
      "grad_norm": 1.1562623977661133,
      "learning_rate": 0.00019114817190506736,
      "loss": 2.0212,
      "step": 833
    },
    {
      "epoch": 0.04456675661955273,
      "grad_norm": 1.9004318714141846,
      "learning_rate": 0.00019113748129142614,
      "loss": 2.5187,
      "step": 834
    },
    {
      "epoch": 0.044620193977609746,
      "grad_norm": 1.6050034761428833,
      "learning_rate": 0.00019112679067778492,
      "loss": 2.4345,
      "step": 835
    },
    {
      "epoch": 0.044673631335666764,
      "grad_norm": 1.4816120862960815,
      "learning_rate": 0.00019111610006414368,
      "loss": 2.0139,
      "step": 836
    },
    {
      "epoch": 0.04472706869372378,
      "grad_norm": 1.6300297975540161,
      "learning_rate": 0.00019110540945050245,
      "loss": 2.2423,
      "step": 837
    },
    {
      "epoch": 0.0447805060517808,
      "grad_norm": 1.1008013486862183,
      "learning_rate": 0.00019109471883686126,
      "loss": 2.2093,
      "step": 838
    },
    {
      "epoch": 0.04483394340983782,
      "grad_norm": 1.3462824821472168,
      "learning_rate": 0.00019108402822322001,
      "loss": 2.0796,
      "step": 839
    },
    {
      "epoch": 0.044887380767894836,
      "grad_norm": 2.5925965309143066,
      "learning_rate": 0.0001910733376095788,
      "loss": 2.5868,
      "step": 840
    },
    {
      "epoch": 0.044940818125951854,
      "grad_norm": 1.2960865497589111,
      "learning_rate": 0.00019106264699593757,
      "loss": 2.1382,
      "step": 841
    },
    {
      "epoch": 0.04499425548400887,
      "grad_norm": 1.3488487005233765,
      "learning_rate": 0.00019105195638229635,
      "loss": 2.1382,
      "step": 842
    },
    {
      "epoch": 0.04504769284206589,
      "grad_norm": 2.005911111831665,
      "learning_rate": 0.00019104126576865513,
      "loss": 2.1171,
      "step": 843
    },
    {
      "epoch": 0.04510113020012291,
      "grad_norm": 1.9580823183059692,
      "learning_rate": 0.0001910305751550139,
      "loss": 2.3,
      "step": 844
    },
    {
      "epoch": 0.045154567558179926,
      "grad_norm": 1.1496927738189697,
      "learning_rate": 0.00019101988454137267,
      "loss": 2.1821,
      "step": 845
    },
    {
      "epoch": 0.045208004916236944,
      "grad_norm": 1.2058496475219727,
      "learning_rate": 0.00019100919392773147,
      "loss": 2.0817,
      "step": 846
    },
    {
      "epoch": 0.04526144227429396,
      "grad_norm": 1.889198660850525,
      "learning_rate": 0.00019099850331409025,
      "loss": 2.3753,
      "step": 847
    },
    {
      "epoch": 0.04531487963235098,
      "grad_norm": 1.375623345375061,
      "learning_rate": 0.000190987812700449,
      "loss": 2.3717,
      "step": 848
    },
    {
      "epoch": 0.045368316990408,
      "grad_norm": 1.0961283445358276,
      "learning_rate": 0.0001909771220868078,
      "loss": 2.0483,
      "step": 849
    },
    {
      "epoch": 0.04542175434846501,
      "grad_norm": 1.3222761154174805,
      "learning_rate": 0.00019096643147316656,
      "loss": 2.149,
      "step": 850
    },
    {
      "epoch": 0.045475191706522027,
      "grad_norm": 1.5250236988067627,
      "learning_rate": 0.00019095574085952534,
      "loss": 2.2894,
      "step": 851
    },
    {
      "epoch": 0.045528629064579045,
      "grad_norm": 1.4946156740188599,
      "learning_rate": 0.00019094505024588412,
      "loss": 2.1347,
      "step": 852
    },
    {
      "epoch": 0.04558206642263606,
      "grad_norm": 1.9192274808883667,
      "learning_rate": 0.0001909343596322429,
      "loss": 2.3343,
      "step": 853
    },
    {
      "epoch": 0.04563550378069308,
      "grad_norm": 2.398179054260254,
      "learning_rate": 0.00019092366901860168,
      "loss": 2.672,
      "step": 854
    },
    {
      "epoch": 0.0456889411387501,
      "grad_norm": 1.9572921991348267,
      "learning_rate": 0.00019091297840496046,
      "loss": 2.3854,
      "step": 855
    },
    {
      "epoch": 0.045742378496807116,
      "grad_norm": 1.7838544845581055,
      "learning_rate": 0.00019090228779131921,
      "loss": 2.0494,
      "step": 856
    },
    {
      "epoch": 0.045795815854864134,
      "grad_norm": 1.1935760974884033,
      "learning_rate": 0.00019089159717767802,
      "loss": 1.9563,
      "step": 857
    },
    {
      "epoch": 0.04584925321292115,
      "grad_norm": 1.9162477254867554,
      "learning_rate": 0.0001908809065640368,
      "loss": 2.2784,
      "step": 858
    },
    {
      "epoch": 0.04590269057097817,
      "grad_norm": 1.315930962562561,
      "learning_rate": 0.00019087021595039555,
      "loss": 2.0202,
      "step": 859
    },
    {
      "epoch": 0.04595612792903519,
      "grad_norm": 1.815403938293457,
      "learning_rate": 0.00019085952533675433,
      "loss": 2.1324,
      "step": 860
    },
    {
      "epoch": 0.046009565287092206,
      "grad_norm": 1.4251837730407715,
      "learning_rate": 0.0001908488347231131,
      "loss": 2.2503,
      "step": 861
    },
    {
      "epoch": 0.046063002645149224,
      "grad_norm": 1.2274397611618042,
      "learning_rate": 0.0001908381441094719,
      "loss": 2.0443,
      "step": 862
    },
    {
      "epoch": 0.04611644000320624,
      "grad_norm": 1.2769529819488525,
      "learning_rate": 0.00019082745349583067,
      "loss": 2.1343,
      "step": 863
    },
    {
      "epoch": 0.04616987736126326,
      "grad_norm": 1.333695888519287,
      "learning_rate": 0.00019081676288218942,
      "loss": 2.1262,
      "step": 864
    },
    {
      "epoch": 0.04622331471932028,
      "grad_norm": 1.4134503602981567,
      "learning_rate": 0.00019080607226854823,
      "loss": 2.2331,
      "step": 865
    },
    {
      "epoch": 0.046276752077377296,
      "grad_norm": 1.796800971031189,
      "learning_rate": 0.000190795381654907,
      "loss": 2.4915,
      "step": 866
    },
    {
      "epoch": 0.046330189435434314,
      "grad_norm": 1.3813637495040894,
      "learning_rate": 0.00019078469104126576,
      "loss": 1.9801,
      "step": 867
    },
    {
      "epoch": 0.04638362679349133,
      "grad_norm": 1.2202892303466797,
      "learning_rate": 0.00019077400042762454,
      "loss": 2.1912,
      "step": 868
    },
    {
      "epoch": 0.04643706415154835,
      "grad_norm": 1.403792142868042,
      "learning_rate": 0.00019076330981398335,
      "loss": 2.1194,
      "step": 869
    },
    {
      "epoch": 0.04649050150960537,
      "grad_norm": 1.3735758066177368,
      "learning_rate": 0.0001907526192003421,
      "loss": 2.1383,
      "step": 870
    },
    {
      "epoch": 0.04654393886766238,
      "grad_norm": 2.293522596359253,
      "learning_rate": 0.00019074192858670088,
      "loss": 2.462,
      "step": 871
    },
    {
      "epoch": 0.0465973762257194,
      "grad_norm": 1.4203075170516968,
      "learning_rate": 0.00019073123797305966,
      "loss": 2.0029,
      "step": 872
    },
    {
      "epoch": 0.046650813583776415,
      "grad_norm": 1.2935127019882202,
      "learning_rate": 0.00019072054735941844,
      "loss": 2.1424,
      "step": 873
    },
    {
      "epoch": 0.04670425094183343,
      "grad_norm": 1.0783452987670898,
      "learning_rate": 0.00019070985674577722,
      "loss": 2.1752,
      "step": 874
    },
    {
      "epoch": 0.04675768829989045,
      "grad_norm": 1.1208809614181519,
      "learning_rate": 0.000190699166132136,
      "loss": 2.186,
      "step": 875
    },
    {
      "epoch": 0.04681112565794747,
      "grad_norm": 1.4236620664596558,
      "learning_rate": 0.00019068847551849478,
      "loss": 2.3181,
      "step": 876
    },
    {
      "epoch": 0.04686456301600449,
      "grad_norm": 1.2951099872589111,
      "learning_rate": 0.00019067778490485356,
      "loss": 2.2349,
      "step": 877
    },
    {
      "epoch": 0.046918000374061505,
      "grad_norm": 1.5294564962387085,
      "learning_rate": 0.0001906670942912123,
      "loss": 2.1559,
      "step": 878
    },
    {
      "epoch": 0.04697143773211852,
      "grad_norm": 1.068179726600647,
      "learning_rate": 0.0001906564036775711,
      "loss": 2.1077,
      "step": 879
    },
    {
      "epoch": 0.04702487509017554,
      "grad_norm": 1.5224013328552246,
      "learning_rate": 0.0001906457130639299,
      "loss": 2.0669,
      "step": 880
    },
    {
      "epoch": 0.04707831244823256,
      "grad_norm": 1.5297578573226929,
      "learning_rate": 0.00019063502245028865,
      "loss": 2.4267,
      "step": 881
    },
    {
      "epoch": 0.04713174980628958,
      "grad_norm": 1.543073296546936,
      "learning_rate": 0.00019062433183664743,
      "loss": 2.1573,
      "step": 882
    },
    {
      "epoch": 0.047185187164346595,
      "grad_norm": 1.589132308959961,
      "learning_rate": 0.0001906136412230062,
      "loss": 1.978,
      "step": 883
    },
    {
      "epoch": 0.04723862452240361,
      "grad_norm": 1.6168893575668335,
      "learning_rate": 0.000190602950609365,
      "loss": 2.1949,
      "step": 884
    },
    {
      "epoch": 0.04729206188046063,
      "grad_norm": 1.5797832012176514,
      "learning_rate": 0.00019059225999572377,
      "loss": 2.295,
      "step": 885
    },
    {
      "epoch": 0.04734549923851765,
      "grad_norm": 1.7997578382492065,
      "learning_rate": 0.00019058156938208255,
      "loss": 2.6014,
      "step": 886
    },
    {
      "epoch": 0.04739893659657467,
      "grad_norm": 1.2988039255142212,
      "learning_rate": 0.0001905708787684413,
      "loss": 2.2444,
      "step": 887
    },
    {
      "epoch": 0.047452373954631685,
      "grad_norm": 1.3177063465118408,
      "learning_rate": 0.0001905601881548001,
      "loss": 2.122,
      "step": 888
    },
    {
      "epoch": 0.0475058113126887,
      "grad_norm": 1.5800199508666992,
      "learning_rate": 0.00019054949754115886,
      "loss": 2.403,
      "step": 889
    },
    {
      "epoch": 0.04755924867074572,
      "grad_norm": 1.0638456344604492,
      "learning_rate": 0.00019053880692751764,
      "loss": 2.1738,
      "step": 890
    },
    {
      "epoch": 0.04761268602880274,
      "grad_norm": 1.3262609243392944,
      "learning_rate": 0.00019052811631387642,
      "loss": 2.3202,
      "step": 891
    },
    {
      "epoch": 0.04766612338685975,
      "grad_norm": 2.5530269145965576,
      "learning_rate": 0.0001905174257002352,
      "loss": 2.2472,
      "step": 892
    },
    {
      "epoch": 0.04771956074491677,
      "grad_norm": 2.39853835105896,
      "learning_rate": 0.00019050673508659398,
      "loss": 2.3521,
      "step": 893
    },
    {
      "epoch": 0.047772998102973786,
      "grad_norm": 1.2057112455368042,
      "learning_rate": 0.00019049604447295276,
      "loss": 2.2149,
      "step": 894
    },
    {
      "epoch": 0.047826435461030804,
      "grad_norm": 1.4519708156585693,
      "learning_rate": 0.00019048535385931154,
      "loss": 2.3909,
      "step": 895
    },
    {
      "epoch": 0.04787987281908782,
      "grad_norm": 1.4575165510177612,
      "learning_rate": 0.00019047466324567032,
      "loss": 2.2249,
      "step": 896
    },
    {
      "epoch": 0.04793331017714484,
      "grad_norm": 1.3541306257247925,
      "learning_rate": 0.0001904639726320291,
      "loss": 2.2294,
      "step": 897
    },
    {
      "epoch": 0.04798674753520186,
      "grad_norm": 1.122869610786438,
      "learning_rate": 0.00019045328201838785,
      "loss": 2.348,
      "step": 898
    },
    {
      "epoch": 0.048040184893258876,
      "grad_norm": 1.561641812324524,
      "learning_rate": 0.00019044259140474666,
      "loss": 2.2005,
      "step": 899
    },
    {
      "epoch": 0.048093622251315894,
      "grad_norm": 1.4678305387496948,
      "learning_rate": 0.0001904319007911054,
      "loss": 2.0317,
      "step": 900
    },
    {
      "epoch": 0.04814705960937291,
      "grad_norm": 1.1428248882293701,
      "learning_rate": 0.0001904212101774642,
      "loss": 2.3086,
      "step": 901
    },
    {
      "epoch": 0.04820049696742993,
      "grad_norm": 1.4924651384353638,
      "learning_rate": 0.00019041051956382297,
      "loss": 2.3904,
      "step": 902
    },
    {
      "epoch": 0.04825393432548695,
      "grad_norm": 1.0865648984909058,
      "learning_rate": 0.00019039982895018175,
      "loss": 2.1222,
      "step": 903
    },
    {
      "epoch": 0.048307371683543966,
      "grad_norm": 1.629478096961975,
      "learning_rate": 0.00019038913833654053,
      "loss": 2.2441,
      "step": 904
    },
    {
      "epoch": 0.048360809041600984,
      "grad_norm": 2.0842390060424805,
      "learning_rate": 0.0001903784477228993,
      "loss": 2.3379,
      "step": 905
    },
    {
      "epoch": 0.048414246399658,
      "grad_norm": 2.0126280784606934,
      "learning_rate": 0.00019036775710925806,
      "loss": 2.5106,
      "step": 906
    },
    {
      "epoch": 0.04846768375771502,
      "grad_norm": 1.23736572265625,
      "learning_rate": 0.00019035706649561687,
      "loss": 2.2032,
      "step": 907
    },
    {
      "epoch": 0.04852112111577204,
      "grad_norm": 0.9739037156105042,
      "learning_rate": 0.00019034637588197565,
      "loss": 2.1335,
      "step": 908
    },
    {
      "epoch": 0.048574558473829056,
      "grad_norm": 1.4522392749786377,
      "learning_rate": 0.0001903356852683344,
      "loss": 2.1798,
      "step": 909
    },
    {
      "epoch": 0.048627995831886074,
      "grad_norm": 1.8891273736953735,
      "learning_rate": 0.00019032499465469318,
      "loss": 2.4007,
      "step": 910
    },
    {
      "epoch": 0.04868143318994309,
      "grad_norm": 1.5044276714324951,
      "learning_rate": 0.00019031430404105196,
      "loss": 2.2169,
      "step": 911
    },
    {
      "epoch": 0.04873487054800011,
      "grad_norm": 1.0390287637710571,
      "learning_rate": 0.00019030361342741074,
      "loss": 2.1694,
      "step": 912
    },
    {
      "epoch": 0.04878830790605712,
      "grad_norm": 2.0082037448883057,
      "learning_rate": 0.00019029292281376952,
      "loss": 2.4951,
      "step": 913
    },
    {
      "epoch": 0.04884174526411414,
      "grad_norm": 0.861765444278717,
      "learning_rate": 0.0001902822322001283,
      "loss": 2.0514,
      "step": 914
    },
    {
      "epoch": 0.04889518262217116,
      "grad_norm": 1.130778193473816,
      "learning_rate": 0.00019027154158648708,
      "loss": 2.1917,
      "step": 915
    },
    {
      "epoch": 0.048948619980228175,
      "grad_norm": 1.140794038772583,
      "learning_rate": 0.00019026085097284586,
      "loss": 2.165,
      "step": 916
    },
    {
      "epoch": 0.04900205733828519,
      "grad_norm": 1.0155681371688843,
      "learning_rate": 0.0001902501603592046,
      "loss": 2.2466,
      "step": 917
    },
    {
      "epoch": 0.04905549469634221,
      "grad_norm": 1.38925302028656,
      "learning_rate": 0.00019023946974556342,
      "loss": 2.2067,
      "step": 918
    },
    {
      "epoch": 0.04910893205439923,
      "grad_norm": 1.9756447076797485,
      "learning_rate": 0.0001902287791319222,
      "loss": 2.299,
      "step": 919
    },
    {
      "epoch": 0.04916236941245625,
      "grad_norm": 1.307783603668213,
      "learning_rate": 0.00019021808851828095,
      "loss": 2.1185,
      "step": 920
    },
    {
      "epoch": 0.049215806770513265,
      "grad_norm": 1.2512362003326416,
      "learning_rate": 0.00019020739790463973,
      "loss": 2.2039,
      "step": 921
    },
    {
      "epoch": 0.04926924412857028,
      "grad_norm": 1.4778058528900146,
      "learning_rate": 0.0001901967072909985,
      "loss": 2.2795,
      "step": 922
    },
    {
      "epoch": 0.0493226814866273,
      "grad_norm": 1.869733214378357,
      "learning_rate": 0.0001901860166773573,
      "loss": 2.2806,
      "step": 923
    },
    {
      "epoch": 0.04937611884468432,
      "grad_norm": 1.3221535682678223,
      "learning_rate": 0.00019017532606371607,
      "loss": 2.2731,
      "step": 924
    },
    {
      "epoch": 0.04942955620274134,
      "grad_norm": 1.1730527877807617,
      "learning_rate": 0.00019016463545007485,
      "loss": 2.0644,
      "step": 925
    },
    {
      "epoch": 0.049482993560798355,
      "grad_norm": 1.3170374631881714,
      "learning_rate": 0.00019015394483643363,
      "loss": 2.0416,
      "step": 926
    },
    {
      "epoch": 0.04953643091885537,
      "grad_norm": 1.3463069200515747,
      "learning_rate": 0.0001901432542227924,
      "loss": 2.1222,
      "step": 927
    },
    {
      "epoch": 0.04958986827691239,
      "grad_norm": 1.351737380027771,
      "learning_rate": 0.00019013256360915116,
      "loss": 2.1024,
      "step": 928
    },
    {
      "epoch": 0.04964330563496941,
      "grad_norm": 1.2718664407730103,
      "learning_rate": 0.00019012187299550994,
      "loss": 2.2029,
      "step": 929
    },
    {
      "epoch": 0.04969674299302643,
      "grad_norm": 1.3145381212234497,
      "learning_rate": 0.00019011118238186875,
      "loss": 2.2914,
      "step": 930
    },
    {
      "epoch": 0.049750180351083445,
      "grad_norm": 1.1492438316345215,
      "learning_rate": 0.0001901004917682275,
      "loss": 2.2288,
      "step": 931
    },
    {
      "epoch": 0.04980361770914046,
      "grad_norm": 1.2096220254898071,
      "learning_rate": 0.00019008980115458628,
      "loss": 2.0713,
      "step": 932
    },
    {
      "epoch": 0.04985705506719748,
      "grad_norm": 1.6046241521835327,
      "learning_rate": 0.00019007911054094506,
      "loss": 2.2336,
      "step": 933
    },
    {
      "epoch": 0.04991049242525449,
      "grad_norm": 2.5457961559295654,
      "learning_rate": 0.00019006841992730384,
      "loss": 2.1405,
      "step": 934
    },
    {
      "epoch": 0.04996392978331151,
      "grad_norm": 2.415942907333374,
      "learning_rate": 0.00019005772931366262,
      "loss": 2.2566,
      "step": 935
    },
    {
      "epoch": 0.05001736714136853,
      "grad_norm": 1.4180738925933838,
      "learning_rate": 0.0001900470387000214,
      "loss": 2.289,
      "step": 936
    },
    {
      "epoch": 0.050070804499425546,
      "grad_norm": 1.9068052768707275,
      "learning_rate": 0.00019003634808638018,
      "loss": 2.4201,
      "step": 937
    },
    {
      "epoch": 0.050124241857482564,
      "grad_norm": 1.5753716230392456,
      "learning_rate": 0.00019002565747273896,
      "loss": 2.382,
      "step": 938
    },
    {
      "epoch": 0.05017767921553958,
      "grad_norm": 1.9960376024246216,
      "learning_rate": 0.0001900149668590977,
      "loss": 2.4754,
      "step": 939
    },
    {
      "epoch": 0.0502311165735966,
      "grad_norm": 1.2534706592559814,
      "learning_rate": 0.0001900042762454565,
      "loss": 2.0853,
      "step": 940
    },
    {
      "epoch": 0.05028455393165362,
      "grad_norm": 1.2314977645874023,
      "learning_rate": 0.0001899935856318153,
      "loss": 2.0435,
      "step": 941
    },
    {
      "epoch": 0.050337991289710636,
      "grad_norm": 1.2997159957885742,
      "learning_rate": 0.00018998289501817405,
      "loss": 2.2131,
      "step": 942
    },
    {
      "epoch": 0.050391428647767654,
      "grad_norm": 1.1449689865112305,
      "learning_rate": 0.00018997220440453283,
      "loss": 2.1649,
      "step": 943
    },
    {
      "epoch": 0.05044486600582467,
      "grad_norm": 1.86053466796875,
      "learning_rate": 0.0001899615137908916,
      "loss": 2.4846,
      "step": 944
    },
    {
      "epoch": 0.05049830336388169,
      "grad_norm": 1.1910037994384766,
      "learning_rate": 0.00018995082317725039,
      "loss": 2.1436,
      "step": 945
    },
    {
      "epoch": 0.05055174072193871,
      "grad_norm": 1.1120388507843018,
      "learning_rate": 0.00018994013256360917,
      "loss": 2.1967,
      "step": 946
    },
    {
      "epoch": 0.050605178079995726,
      "grad_norm": 1.01145339012146,
      "learning_rate": 0.00018992944194996795,
      "loss": 2.0041,
      "step": 947
    },
    {
      "epoch": 0.050658615438052744,
      "grad_norm": 1.2396103143692017,
      "learning_rate": 0.0001899187513363267,
      "loss": 2.2688,
      "step": 948
    },
    {
      "epoch": 0.05071205279610976,
      "grad_norm": 1.2199912071228027,
      "learning_rate": 0.0001899080607226855,
      "loss": 2.1075,
      "step": 949
    },
    {
      "epoch": 0.05076549015416678,
      "grad_norm": 0.9256200790405273,
      "learning_rate": 0.00018989737010904426,
      "loss": 1.95,
      "step": 950
    },
    {
      "epoch": 0.0508189275122238,
      "grad_norm": 1.2445447444915771,
      "learning_rate": 0.00018988667949540304,
      "loss": 2.261,
      "step": 951
    },
    {
      "epoch": 0.050872364870280816,
      "grad_norm": 1.6769301891326904,
      "learning_rate": 0.00018987598888176182,
      "loss": 2.4662,
      "step": 952
    },
    {
      "epoch": 0.050925802228337834,
      "grad_norm": 1.7828115224838257,
      "learning_rate": 0.0001898652982681206,
      "loss": 2.2614,
      "step": 953
    },
    {
      "epoch": 0.05097923958639485,
      "grad_norm": 1.3922134637832642,
      "learning_rate": 0.00018985460765447938,
      "loss": 2.3383,
      "step": 954
    },
    {
      "epoch": 0.05103267694445186,
      "grad_norm": 1.1962181329727173,
      "learning_rate": 0.00018984391704083816,
      "loss": 2.1517,
      "step": 955
    },
    {
      "epoch": 0.05108611430250888,
      "grad_norm": 1.0479016304016113,
      "learning_rate": 0.0001898332264271969,
      "loss": 2.0086,
      "step": 956
    },
    {
      "epoch": 0.0511395516605659,
      "grad_norm": 1.563866138458252,
      "learning_rate": 0.00018982253581355571,
      "loss": 2.2482,
      "step": 957
    },
    {
      "epoch": 0.05119298901862292,
      "grad_norm": 1.3338490724563599,
      "learning_rate": 0.0001898118451999145,
      "loss": 2.0084,
      "step": 958
    },
    {
      "epoch": 0.051246426376679935,
      "grad_norm": 1.7132970094680786,
      "learning_rate": 0.00018980115458627325,
      "loss": 2.3382,
      "step": 959
    },
    {
      "epoch": 0.05129986373473695,
      "grad_norm": 1.190850019454956,
      "learning_rate": 0.00018979046397263205,
      "loss": 2.0981,
      "step": 960
    },
    {
      "epoch": 0.05135330109279397,
      "grad_norm": 0.9099943041801453,
      "learning_rate": 0.00018977977335899083,
      "loss": 2.2013,
      "step": 961
    },
    {
      "epoch": 0.05140673845085099,
      "grad_norm": 1.2725077867507935,
      "learning_rate": 0.00018976908274534959,
      "loss": 2.131,
      "step": 962
    },
    {
      "epoch": 0.05146017580890801,
      "grad_norm": 1.6348954439163208,
      "learning_rate": 0.00018975839213170837,
      "loss": 2.2426,
      "step": 963
    },
    {
      "epoch": 0.051513613166965025,
      "grad_norm": 1.4351247549057007,
      "learning_rate": 0.00018974770151806715,
      "loss": 2.1216,
      "step": 964
    },
    {
      "epoch": 0.05156705052502204,
      "grad_norm": 1.355481505393982,
      "learning_rate": 0.00018973701090442592,
      "loss": 2.2031,
      "step": 965
    },
    {
      "epoch": 0.05162048788307906,
      "grad_norm": 1.1219637393951416,
      "learning_rate": 0.0001897263202907847,
      "loss": 2.1489,
      "step": 966
    },
    {
      "epoch": 0.05167392524113608,
      "grad_norm": 2.0048775672912598,
      "learning_rate": 0.00018971562967714346,
      "loss": 2.247,
      "step": 967
    },
    {
      "epoch": 0.0517273625991931,
      "grad_norm": 1.3452367782592773,
      "learning_rate": 0.00018970493906350226,
      "loss": 2.0265,
      "step": 968
    },
    {
      "epoch": 0.051780799957250115,
      "grad_norm": 1.2537345886230469,
      "learning_rate": 0.00018969424844986104,
      "loss": 2.0041,
      "step": 969
    },
    {
      "epoch": 0.05183423731530713,
      "grad_norm": 1.3957139253616333,
      "learning_rate": 0.0001896835578362198,
      "loss": 2.164,
      "step": 970
    },
    {
      "epoch": 0.05188767467336415,
      "grad_norm": 1.7320055961608887,
      "learning_rate": 0.00018967286722257858,
      "loss": 2.3415,
      "step": 971
    },
    {
      "epoch": 0.05194111203142117,
      "grad_norm": 1.0724568367004395,
      "learning_rate": 0.00018966217660893738,
      "loss": 2.1319,
      "step": 972
    },
    {
      "epoch": 0.05199454938947819,
      "grad_norm": 1.2247984409332275,
      "learning_rate": 0.00018965148599529613,
      "loss": 2.2242,
      "step": 973
    },
    {
      "epoch": 0.052047986747535205,
      "grad_norm": 1.9695976972579956,
      "learning_rate": 0.00018964079538165491,
      "loss": 2.2507,
      "step": 974
    },
    {
      "epoch": 0.05210142410559222,
      "grad_norm": 1.5388609170913696,
      "learning_rate": 0.0001896301047680137,
      "loss": 2.0694,
      "step": 975
    },
    {
      "epoch": 0.052154861463649234,
      "grad_norm": 1.7017155885696411,
      "learning_rate": 0.00018961941415437247,
      "loss": 2.3479,
      "step": 976
    },
    {
      "epoch": 0.05220829882170625,
      "grad_norm": 1.7330573797225952,
      "learning_rate": 0.00018960872354073125,
      "loss": 2.2761,
      "step": 977
    },
    {
      "epoch": 0.05226173617976327,
      "grad_norm": 1.3766883611679077,
      "learning_rate": 0.00018959803292709,
      "loss": 2.3367,
      "step": 978
    },
    {
      "epoch": 0.05231517353782029,
      "grad_norm": 2.0134243965148926,
      "learning_rate": 0.00018958734231344879,
      "loss": 2.3577,
      "step": 979
    },
    {
      "epoch": 0.052368610895877306,
      "grad_norm": 1.287734031677246,
      "learning_rate": 0.0001895766516998076,
      "loss": 2.042,
      "step": 980
    },
    {
      "epoch": 0.052422048253934324,
      "grad_norm": 1.5339672565460205,
      "learning_rate": 0.00018956596108616634,
      "loss": 2.2083,
      "step": 981
    },
    {
      "epoch": 0.05247548561199134,
      "grad_norm": 1.4226977825164795,
      "learning_rate": 0.00018955527047252512,
      "loss": 2.3277,
      "step": 982
    },
    {
      "epoch": 0.05252892297004836,
      "grad_norm": 1.2751632928848267,
      "learning_rate": 0.00018954457985888393,
      "loss": 2.2829,
      "step": 983
    },
    {
      "epoch": 0.05258236032810538,
      "grad_norm": 1.3605647087097168,
      "learning_rate": 0.00018953388924524268,
      "loss": 2.2066,
      "step": 984
    },
    {
      "epoch": 0.052635797686162396,
      "grad_norm": 1.04835844039917,
      "learning_rate": 0.00018952319863160146,
      "loss": 2.0535,
      "step": 985
    },
    {
      "epoch": 0.052689235044219414,
      "grad_norm": 1.5190150737762451,
      "learning_rate": 0.00018951250801796024,
      "loss": 2.2382,
      "step": 986
    },
    {
      "epoch": 0.05274267240227643,
      "grad_norm": 1.898591160774231,
      "learning_rate": 0.00018950181740431902,
      "loss": 2.4098,
      "step": 987
    },
    {
      "epoch": 0.05279610976033345,
      "grad_norm": 1.688984751701355,
      "learning_rate": 0.0001894911267906778,
      "loss": 2.0854,
      "step": 988
    },
    {
      "epoch": 0.05284954711839047,
      "grad_norm": 1.1983951330184937,
      "learning_rate": 0.00018948043617703658,
      "loss": 2.1841,
      "step": 989
    },
    {
      "epoch": 0.052902984476447486,
      "grad_norm": 0.9932587146759033,
      "learning_rate": 0.00018946974556339533,
      "loss": 2.1194,
      "step": 990
    },
    {
      "epoch": 0.052956421834504504,
      "grad_norm": 1.463789939880371,
      "learning_rate": 0.00018945905494975414,
      "loss": 2.2294,
      "step": 991
    },
    {
      "epoch": 0.05300985919256152,
      "grad_norm": 1.8532174825668335,
      "learning_rate": 0.0001894483643361129,
      "loss": 2.3093,
      "step": 992
    },
    {
      "epoch": 0.05306329655061854,
      "grad_norm": 1.560888648033142,
      "learning_rate": 0.00018943767372247167,
      "loss": 2.2411,
      "step": 993
    },
    {
      "epoch": 0.05311673390867556,
      "grad_norm": 2.169429063796997,
      "learning_rate": 0.00018942698310883045,
      "loss": 2.2762,
      "step": 994
    },
    {
      "epoch": 0.053170171266732576,
      "grad_norm": 2.0678365230560303,
      "learning_rate": 0.00018941629249518923,
      "loss": 2.2352,
      "step": 995
    },
    {
      "epoch": 0.053223608624789594,
      "grad_norm": 1.2304105758666992,
      "learning_rate": 0.000189405601881548,
      "loss": 2.2647,
      "step": 996
    },
    {
      "epoch": 0.053277045982846605,
      "grad_norm": 1.1286602020263672,
      "learning_rate": 0.0001893949112679068,
      "loss": 2.2563,
      "step": 997
    },
    {
      "epoch": 0.05333048334090362,
      "grad_norm": 2.023955821990967,
      "learning_rate": 0.00018938422065426554,
      "loss": 2.266,
      "step": 998
    },
    {
      "epoch": 0.05338392069896064,
      "grad_norm": 1.25797438621521,
      "learning_rate": 0.00018937353004062435,
      "loss": 2.2712,
      "step": 999
    },
    {
      "epoch": 0.05343735805701766,
      "grad_norm": 1.1662591695785522,
      "learning_rate": 0.00018936283942698313,
      "loss": 2.08,
      "step": 1000
    },
    {
      "epoch": 0.05349079541507468,
      "grad_norm": 1.3028292655944824,
      "learning_rate": 0.00018935214881334188,
      "loss": 2.0832,
      "step": 1001
    },
    {
      "epoch": 0.053544232773131695,
      "grad_norm": 1.8228563070297241,
      "learning_rate": 0.00018934145819970066,
      "loss": 2.3987,
      "step": 1002
    },
    {
      "epoch": 0.05359767013118871,
      "grad_norm": 1.2306971549987793,
      "learning_rate": 0.00018933076758605944,
      "loss": 2.1038,
      "step": 1003
    },
    {
      "epoch": 0.05365110748924573,
      "grad_norm": 1.6891781091690063,
      "learning_rate": 0.00018932007697241822,
      "loss": 2.0829,
      "step": 1004
    },
    {
      "epoch": 0.05370454484730275,
      "grad_norm": 1.0811512470245361,
      "learning_rate": 0.000189309386358777,
      "loss": 2.0517,
      "step": 1005
    },
    {
      "epoch": 0.05375798220535977,
      "grad_norm": 1.4734020233154297,
      "learning_rate": 0.00018929869574513578,
      "loss": 2.2632,
      "step": 1006
    },
    {
      "epoch": 0.053811419563416785,
      "grad_norm": 1.38963782787323,
      "learning_rate": 0.00018928800513149456,
      "loss": 2.0724,
      "step": 1007
    },
    {
      "epoch": 0.0538648569214738,
      "grad_norm": 1.2481670379638672,
      "learning_rate": 0.00018927731451785334,
      "loss": 2.1783,
      "step": 1008
    },
    {
      "epoch": 0.05391829427953082,
      "grad_norm": 1.5819883346557617,
      "learning_rate": 0.0001892666239042121,
      "loss": 2.2482,
      "step": 1009
    },
    {
      "epoch": 0.05397173163758784,
      "grad_norm": 1.2967729568481445,
      "learning_rate": 0.0001892559332905709,
      "loss": 2.1959,
      "step": 1010
    },
    {
      "epoch": 0.05402516899564486,
      "grad_norm": 1.4423342943191528,
      "learning_rate": 0.00018924524267692968,
      "loss": 2.2834,
      "step": 1011
    },
    {
      "epoch": 0.054078606353701875,
      "grad_norm": 1.2895318269729614,
      "learning_rate": 0.00018923455206328843,
      "loss": 2.1446,
      "step": 1012
    },
    {
      "epoch": 0.05413204371175889,
      "grad_norm": 1.9920551776885986,
      "learning_rate": 0.0001892238614496472,
      "loss": 2.432,
      "step": 1013
    },
    {
      "epoch": 0.05418548106981591,
      "grad_norm": 2.0321900844573975,
      "learning_rate": 0.000189213170836006,
      "loss": 2.5326,
      "step": 1014
    },
    {
      "epoch": 0.05423891842787293,
      "grad_norm": 1.3229732513427734,
      "learning_rate": 0.00018920248022236477,
      "loss": 2.0781,
      "step": 1015
    },
    {
      "epoch": 0.05429235578592995,
      "grad_norm": 1.3339474201202393,
      "learning_rate": 0.00018919178960872355,
      "loss": 2.211,
      "step": 1016
    },
    {
      "epoch": 0.054345793143986965,
      "grad_norm": 1.5720396041870117,
      "learning_rate": 0.00018918109899508233,
      "loss": 2.2811,
      "step": 1017
    },
    {
      "epoch": 0.054399230502043976,
      "grad_norm": 2.4236629009246826,
      "learning_rate": 0.0001891704083814411,
      "loss": 2.3921,
      "step": 1018
    },
    {
      "epoch": 0.054452667860100994,
      "grad_norm": 1.2298576831817627,
      "learning_rate": 0.0001891597177677999,
      "loss": 2.0955,
      "step": 1019
    },
    {
      "epoch": 0.05450610521815801,
      "grad_norm": 0.9350174069404602,
      "learning_rate": 0.00018914902715415864,
      "loss": 1.9161,
      "step": 1020
    },
    {
      "epoch": 0.05455954257621503,
      "grad_norm": 1.7232798337936401,
      "learning_rate": 0.00018913833654051742,
      "loss": 2.3866,
      "step": 1021
    },
    {
      "epoch": 0.05461297993427205,
      "grad_norm": 1.0443817377090454,
      "learning_rate": 0.00018912764592687623,
      "loss": 2.0457,
      "step": 1022
    },
    {
      "epoch": 0.054666417292329066,
      "grad_norm": 1.179816484451294,
      "learning_rate": 0.00018911695531323498,
      "loss": 2.0884,
      "step": 1023
    },
    {
      "epoch": 0.054719854650386084,
      "grad_norm": 1.342194676399231,
      "learning_rate": 0.00018910626469959376,
      "loss": 2.0753,
      "step": 1024
    },
    {
      "epoch": 0.0547732920084431,
      "grad_norm": 1.1786834001541138,
      "learning_rate": 0.00018909557408595254,
      "loss": 2.3108,
      "step": 1025
    },
    {
      "epoch": 0.05482672936650012,
      "grad_norm": 1.2406463623046875,
      "learning_rate": 0.00018908488347231132,
      "loss": 2.0457,
      "step": 1026
    },
    {
      "epoch": 0.05488016672455714,
      "grad_norm": 1.548630714416504,
      "learning_rate": 0.0001890741928586701,
      "loss": 2.1883,
      "step": 1027
    },
    {
      "epoch": 0.054933604082614156,
      "grad_norm": 0.752561628818512,
      "learning_rate": 0.00018906350224502888,
      "loss": 2.1063,
      "step": 1028
    },
    {
      "epoch": 0.054987041440671174,
      "grad_norm": 1.2597285509109497,
      "learning_rate": 0.00018905281163138766,
      "loss": 2.0581,
      "step": 1029
    },
    {
      "epoch": 0.05504047879872819,
      "grad_norm": 1.91995108127594,
      "learning_rate": 0.00018904212101774644,
      "loss": 2.4503,
      "step": 1030
    },
    {
      "epoch": 0.05509391615678521,
      "grad_norm": 0.8244147300720215,
      "learning_rate": 0.0001890314304041052,
      "loss": 1.9404,
      "step": 1031
    },
    {
      "epoch": 0.05514735351484223,
      "grad_norm": 1.2527105808258057,
      "learning_rate": 0.00018902073979046397,
      "loss": 2.1456,
      "step": 1032
    },
    {
      "epoch": 0.055200790872899246,
      "grad_norm": 1.4510611295700073,
      "learning_rate": 0.00018901004917682278,
      "loss": 2.0485,
      "step": 1033
    },
    {
      "epoch": 0.055254228230956264,
      "grad_norm": 1.2936227321624756,
      "learning_rate": 0.00018899935856318153,
      "loss": 2.0978,
      "step": 1034
    },
    {
      "epoch": 0.05530766558901328,
      "grad_norm": 1.6187281608581543,
      "learning_rate": 0.0001889886679495403,
      "loss": 2.3614,
      "step": 1035
    },
    {
      "epoch": 0.0553611029470703,
      "grad_norm": 1.6474411487579346,
      "learning_rate": 0.0001889779773358991,
      "loss": 2.2751,
      "step": 1036
    },
    {
      "epoch": 0.05541454030512732,
      "grad_norm": 0.9586048722267151,
      "learning_rate": 0.00018896728672225787,
      "loss": 2.2577,
      "step": 1037
    },
    {
      "epoch": 0.055467977663184335,
      "grad_norm": 1.4278606176376343,
      "learning_rate": 0.00018895659610861665,
      "loss": 2.1838,
      "step": 1038
    },
    {
      "epoch": 0.05552141502124135,
      "grad_norm": 1.5650602579116821,
      "learning_rate": 0.00018894590549497543,
      "loss": 2.1845,
      "step": 1039
    },
    {
      "epoch": 0.055574852379298365,
      "grad_norm": 1.2678059339523315,
      "learning_rate": 0.00018893521488133418,
      "loss": 2.196,
      "step": 1040
    },
    {
      "epoch": 0.05562828973735538,
      "grad_norm": 1.1515253782272339,
      "learning_rate": 0.000188924524267693,
      "loss": 2.0913,
      "step": 1041
    },
    {
      "epoch": 0.0556817270954124,
      "grad_norm": 1.0353114604949951,
      "learning_rate": 0.00018891383365405174,
      "loss": 2.1688,
      "step": 1042
    },
    {
      "epoch": 0.05573516445346942,
      "grad_norm": 1.4305317401885986,
      "learning_rate": 0.00018890314304041052,
      "loss": 2.2551,
      "step": 1043
    },
    {
      "epoch": 0.055788601811526436,
      "grad_norm": 2.160407543182373,
      "learning_rate": 0.0001888924524267693,
      "loss": 2.2091,
      "step": 1044
    },
    {
      "epoch": 0.055842039169583454,
      "grad_norm": 1.755277156829834,
      "learning_rate": 0.00018888176181312808,
      "loss": 2.0589,
      "step": 1045
    },
    {
      "epoch": 0.05589547652764047,
      "grad_norm": 1.1490364074707031,
      "learning_rate": 0.00018887107119948686,
      "loss": 2.2011,
      "step": 1046
    },
    {
      "epoch": 0.05594891388569749,
      "grad_norm": 1.259576678276062,
      "learning_rate": 0.00018886038058584564,
      "loss": 2.1737,
      "step": 1047
    },
    {
      "epoch": 0.05600235124375451,
      "grad_norm": 1.2591842412948608,
      "learning_rate": 0.00018884968997220442,
      "loss": 2.0784,
      "step": 1048
    },
    {
      "epoch": 0.056055788601811526,
      "grad_norm": 1.0516865253448486,
      "learning_rate": 0.0001888389993585632,
      "loss": 1.9799,
      "step": 1049
    },
    {
      "epoch": 0.056109225959868544,
      "grad_norm": 1.0075680017471313,
      "learning_rate": 0.00018882830874492198,
      "loss": 2.0257,
      "step": 1050
    },
    {
      "epoch": 0.05616266331792556,
      "grad_norm": 1.194820761680603,
      "learning_rate": 0.00018881761813128073,
      "loss": 1.9633,
      "step": 1051
    },
    {
      "epoch": 0.05621610067598258,
      "grad_norm": 1.7550089359283447,
      "learning_rate": 0.00018880692751763954,
      "loss": 2.2649,
      "step": 1052
    },
    {
      "epoch": 0.0562695380340396,
      "grad_norm": 1.08489990234375,
      "learning_rate": 0.0001887962369039983,
      "loss": 1.9842,
      "step": 1053
    },
    {
      "epoch": 0.056322975392096616,
      "grad_norm": 2.1051390171051025,
      "learning_rate": 0.00018878554629035707,
      "loss": 2.4973,
      "step": 1054
    },
    {
      "epoch": 0.056376412750153634,
      "grad_norm": 1.7644857168197632,
      "learning_rate": 0.00018877485567671585,
      "loss": 2.3049,
      "step": 1055
    },
    {
      "epoch": 0.05642985010821065,
      "grad_norm": 1.6021666526794434,
      "learning_rate": 0.00018876416506307463,
      "loss": 2.4343,
      "step": 1056
    },
    {
      "epoch": 0.05648328746626767,
      "grad_norm": 1.0336542129516602,
      "learning_rate": 0.0001887534744494334,
      "loss": 2.0721,
      "step": 1057
    },
    {
      "epoch": 0.05653672482432469,
      "grad_norm": 1.3581039905548096,
      "learning_rate": 0.0001887427838357922,
      "loss": 2.0974,
      "step": 1058
    },
    {
      "epoch": 0.056590162182381706,
      "grad_norm": 1.2288419008255005,
      "learning_rate": 0.00018873209322215094,
      "loss": 2.2484,
      "step": 1059
    },
    {
      "epoch": 0.05664359954043872,
      "grad_norm": 1.4683698415756226,
      "learning_rate": 0.00018872140260850975,
      "loss": 2.4,
      "step": 1060
    },
    {
      "epoch": 0.056697036898495735,
      "grad_norm": 1.0562084913253784,
      "learning_rate": 0.00018871071199486853,
      "loss": 2.0033,
      "step": 1061
    },
    {
      "epoch": 0.05675047425655275,
      "grad_norm": 1.3657398223876953,
      "learning_rate": 0.00018870002138122728,
      "loss": 2.1459,
      "step": 1062
    },
    {
      "epoch": 0.05680391161460977,
      "grad_norm": 1.1485264301300049,
      "learning_rate": 0.00018868933076758606,
      "loss": 2.2631,
      "step": 1063
    },
    {
      "epoch": 0.05685734897266679,
      "grad_norm": 1.263022780418396,
      "learning_rate": 0.00018867864015394484,
      "loss": 2.0217,
      "step": 1064
    },
    {
      "epoch": 0.05691078633072381,
      "grad_norm": 1.3827261924743652,
      "learning_rate": 0.00018866794954030362,
      "loss": 2.332,
      "step": 1065
    },
    {
      "epoch": 0.056964223688780825,
      "grad_norm": 1.293238639831543,
      "learning_rate": 0.0001886572589266624,
      "loss": 2.1765,
      "step": 1066
    },
    {
      "epoch": 0.05701766104683784,
      "grad_norm": 1.0014264583587646,
      "learning_rate": 0.00018864656831302118,
      "loss": 2.0106,
      "step": 1067
    },
    {
      "epoch": 0.05707109840489486,
      "grad_norm": 1.5071957111358643,
      "learning_rate": 0.00018863587769937996,
      "loss": 2.2911,
      "step": 1068
    },
    {
      "epoch": 0.05712453576295188,
      "grad_norm": 1.1304893493652344,
      "learning_rate": 0.00018862518708573874,
      "loss": 2.0182,
      "step": 1069
    },
    {
      "epoch": 0.0571779731210089,
      "grad_norm": 1.5643976926803589,
      "learning_rate": 0.0001886144964720975,
      "loss": 2.3446,
      "step": 1070
    },
    {
      "epoch": 0.057231410479065915,
      "grad_norm": 1.1741507053375244,
      "learning_rate": 0.0001886038058584563,
      "loss": 2.2947,
      "step": 1071
    },
    {
      "epoch": 0.05728484783712293,
      "grad_norm": 1.4268094301223755,
      "learning_rate": 0.00018859311524481508,
      "loss": 2.4074,
      "step": 1072
    },
    {
      "epoch": 0.05733828519517995,
      "grad_norm": 1.8990641832351685,
      "learning_rate": 0.00018858242463117383,
      "loss": 2.4336,
      "step": 1073
    },
    {
      "epoch": 0.05739172255323697,
      "grad_norm": 1.3159130811691284,
      "learning_rate": 0.0001885717340175326,
      "loss": 2.1778,
      "step": 1074
    },
    {
      "epoch": 0.05744515991129399,
      "grad_norm": 1.6528925895690918,
      "learning_rate": 0.00018856104340389142,
      "loss": 2.404,
      "step": 1075
    },
    {
      "epoch": 0.057498597269351005,
      "grad_norm": 1.5929213762283325,
      "learning_rate": 0.00018855035279025017,
      "loss": 2.3504,
      "step": 1076
    },
    {
      "epoch": 0.05755203462740802,
      "grad_norm": 1.2476723194122314,
      "learning_rate": 0.00018853966217660895,
      "loss": 2.1495,
      "step": 1077
    },
    {
      "epoch": 0.05760547198546504,
      "grad_norm": 1.075948715209961,
      "learning_rate": 0.00018852897156296773,
      "loss": 2.2474,
      "step": 1078
    },
    {
      "epoch": 0.05765890934352206,
      "grad_norm": 1.847332239151001,
      "learning_rate": 0.0001885182809493265,
      "loss": 2.2364,
      "step": 1079
    },
    {
      "epoch": 0.05771234670157908,
      "grad_norm": 1.8135323524475098,
      "learning_rate": 0.00018850759033568529,
      "loss": 2.211,
      "step": 1080
    },
    {
      "epoch": 0.05776578405963609,
      "grad_norm": 1.8156994581222534,
      "learning_rate": 0.00018849689972204404,
      "loss": 2.4722,
      "step": 1081
    },
    {
      "epoch": 0.057819221417693106,
      "grad_norm": 1.7902354001998901,
      "learning_rate": 0.00018848620910840282,
      "loss": 2.2417,
      "step": 1082
    },
    {
      "epoch": 0.057872658775750124,
      "grad_norm": 1.1293127536773682,
      "learning_rate": 0.00018847551849476163,
      "loss": 2.1342,
      "step": 1083
    },
    {
      "epoch": 0.05792609613380714,
      "grad_norm": 1.305681586265564,
      "learning_rate": 0.00018846482788112038,
      "loss": 2.3094,
      "step": 1084
    },
    {
      "epoch": 0.05797953349186416,
      "grad_norm": 1.4615336656570435,
      "learning_rate": 0.00018845413726747916,
      "loss": 2.1433,
      "step": 1085
    },
    {
      "epoch": 0.05803297084992118,
      "grad_norm": 1.0389463901519775,
      "learning_rate": 0.00018844344665383794,
      "loss": 2.073,
      "step": 1086
    },
    {
      "epoch": 0.058086408207978196,
      "grad_norm": 1.2150118350982666,
      "learning_rate": 0.00018843275604019672,
      "loss": 2.0259,
      "step": 1087
    },
    {
      "epoch": 0.058139845566035214,
      "grad_norm": 1.1759049892425537,
      "learning_rate": 0.0001884220654265555,
      "loss": 2.2285,
      "step": 1088
    },
    {
      "epoch": 0.05819328292409223,
      "grad_norm": 1.3207422494888306,
      "learning_rate": 0.00018841137481291428,
      "loss": 2.0512,
      "step": 1089
    },
    {
      "epoch": 0.05824672028214925,
      "grad_norm": 1.5429790019989014,
      "learning_rate": 0.00018840068419927303,
      "loss": 2.1308,
      "step": 1090
    },
    {
      "epoch": 0.05830015764020627,
      "grad_norm": 1.6126154661178589,
      "learning_rate": 0.00018838999358563184,
      "loss": 2.2988,
      "step": 1091
    },
    {
      "epoch": 0.058353594998263286,
      "grad_norm": 1.6705214977264404,
      "learning_rate": 0.0001883793029719906,
      "loss": 2.282,
      "step": 1092
    },
    {
      "epoch": 0.058407032356320304,
      "grad_norm": 1.2586854696273804,
      "learning_rate": 0.00018836861235834937,
      "loss": 2.2137,
      "step": 1093
    },
    {
      "epoch": 0.05846046971437732,
      "grad_norm": 1.9682615995407104,
      "learning_rate": 0.00018835792174470817,
      "loss": 2.368,
      "step": 1094
    },
    {
      "epoch": 0.05851390707243434,
      "grad_norm": 1.4632529020309448,
      "learning_rate": 0.00018834723113106693,
      "loss": 2.065,
      "step": 1095
    },
    {
      "epoch": 0.05856734443049136,
      "grad_norm": 1.4343078136444092,
      "learning_rate": 0.0001883365405174257,
      "loss": 2.1424,
      "step": 1096
    },
    {
      "epoch": 0.058620781788548376,
      "grad_norm": 1.467854380607605,
      "learning_rate": 0.00018832584990378449,
      "loss": 2.2922,
      "step": 1097
    },
    {
      "epoch": 0.058674219146605394,
      "grad_norm": 1.0842610597610474,
      "learning_rate": 0.00018831515929014327,
      "loss": 2.0662,
      "step": 1098
    },
    {
      "epoch": 0.05872765650466241,
      "grad_norm": 1.402072787284851,
      "learning_rate": 0.00018830446867650205,
      "loss": 2.0496,
      "step": 1099
    },
    {
      "epoch": 0.05878109386271943,
      "grad_norm": 1.115738034248352,
      "learning_rate": 0.00018829377806286083,
      "loss": 2.2119,
      "step": 1100
    },
    {
      "epoch": 0.05883453122077645,
      "grad_norm": 1.0916880369186401,
      "learning_rate": 0.00018828308744921958,
      "loss": 1.9314,
      "step": 1101
    },
    {
      "epoch": 0.05888796857883346,
      "grad_norm": 1.5227444171905518,
      "learning_rate": 0.00018827239683557838,
      "loss": 1.969,
      "step": 1102
    },
    {
      "epoch": 0.05894140593689048,
      "grad_norm": 1.2868092060089111,
      "learning_rate": 0.00018826170622193716,
      "loss": 2.1207,
      "step": 1103
    },
    {
      "epoch": 0.058994843294947495,
      "grad_norm": 1.6436491012573242,
      "learning_rate": 0.00018825101560829592,
      "loss": 2.0849,
      "step": 1104
    },
    {
      "epoch": 0.05904828065300451,
      "grad_norm": 1.2718703746795654,
      "learning_rate": 0.0001882403249946547,
      "loss": 2.3292,
      "step": 1105
    },
    {
      "epoch": 0.05910171801106153,
      "grad_norm": 1.0146564245224,
      "learning_rate": 0.00018822963438101348,
      "loss": 2.1389,
      "step": 1106
    },
    {
      "epoch": 0.05915515536911855,
      "grad_norm": 1.7721754312515259,
      "learning_rate": 0.00018821894376737226,
      "loss": 2.1764,
      "step": 1107
    },
    {
      "epoch": 0.05920859272717557,
      "grad_norm": 1.7673814296722412,
      "learning_rate": 0.00018820825315373104,
      "loss": 2.3818,
      "step": 1108
    },
    {
      "epoch": 0.059262030085232585,
      "grad_norm": 1.870714545249939,
      "learning_rate": 0.0001881975625400898,
      "loss": 2.2728,
      "step": 1109
    },
    {
      "epoch": 0.0593154674432896,
      "grad_norm": 1.6804448366165161,
      "learning_rate": 0.0001881868719264486,
      "loss": 2.1829,
      "step": 1110
    },
    {
      "epoch": 0.05936890480134662,
      "grad_norm": 1.790594220161438,
      "learning_rate": 0.00018817618131280737,
      "loss": 2.3341,
      "step": 1111
    },
    {
      "epoch": 0.05942234215940364,
      "grad_norm": 1.2198123931884766,
      "learning_rate": 0.00018816549069916613,
      "loss": 2.1688,
      "step": 1112
    },
    {
      "epoch": 0.05947577951746066,
      "grad_norm": 1.3018478155136108,
      "learning_rate": 0.0001881548000855249,
      "loss": 2.1643,
      "step": 1113
    },
    {
      "epoch": 0.059529216875517675,
      "grad_norm": 1.1036429405212402,
      "learning_rate": 0.0001881441094718837,
      "loss": 2.1639,
      "step": 1114
    },
    {
      "epoch": 0.05958265423357469,
      "grad_norm": 1.5281481742858887,
      "learning_rate": 0.00018813341885824247,
      "loss": 2.1795,
      "step": 1115
    },
    {
      "epoch": 0.05963609159163171,
      "grad_norm": 1.882413625717163,
      "learning_rate": 0.00018812272824460125,
      "loss": 2.4475,
      "step": 1116
    },
    {
      "epoch": 0.05968952894968873,
      "grad_norm": 1.4367362260818481,
      "learning_rate": 0.00018811203763096002,
      "loss": 2.0815,
      "step": 1117
    },
    {
      "epoch": 0.05974296630774575,
      "grad_norm": 1.054714560508728,
      "learning_rate": 0.0001881013470173188,
      "loss": 2.1557,
      "step": 1118
    },
    {
      "epoch": 0.059796403665802765,
      "grad_norm": 1.2007906436920166,
      "learning_rate": 0.00018809065640367758,
      "loss": 1.9986,
      "step": 1119
    },
    {
      "epoch": 0.05984984102385978,
      "grad_norm": 1.1284635066986084,
      "learning_rate": 0.00018807996579003634,
      "loss": 2.0134,
      "step": 1120
    },
    {
      "epoch": 0.0599032783819168,
      "grad_norm": 1.764559030532837,
      "learning_rate": 0.00018806927517639514,
      "loss": 2.2802,
      "step": 1121
    },
    {
      "epoch": 0.05995671573997382,
      "grad_norm": 1.301396131515503,
      "learning_rate": 0.00018805858456275392,
      "loss": 1.9721,
      "step": 1122
    },
    {
      "epoch": 0.06001015309803083,
      "grad_norm": 1.7121474742889404,
      "learning_rate": 0.00018804789394911268,
      "loss": 2.4185,
      "step": 1123
    },
    {
      "epoch": 0.06006359045608785,
      "grad_norm": 1.8480522632598877,
      "learning_rate": 0.00018803720333547146,
      "loss": 2.2302,
      "step": 1124
    },
    {
      "epoch": 0.060117027814144866,
      "grad_norm": 1.1085023880004883,
      "learning_rate": 0.00018802651272183026,
      "loss": 2.1098,
      "step": 1125
    },
    {
      "epoch": 0.060170465172201884,
      "grad_norm": 1.5350450277328491,
      "learning_rate": 0.00018801582210818901,
      "loss": 2.344,
      "step": 1126
    },
    {
      "epoch": 0.0602239025302589,
      "grad_norm": 1.394731044769287,
      "learning_rate": 0.0001880051314945478,
      "loss": 2.201,
      "step": 1127
    },
    {
      "epoch": 0.06027733988831592,
      "grad_norm": 1.5920689105987549,
      "learning_rate": 0.00018799444088090657,
      "loss": 2.463,
      "step": 1128
    },
    {
      "epoch": 0.06033077724637294,
      "grad_norm": 1.469049334526062,
      "learning_rate": 0.00018798375026726535,
      "loss": 2.051,
      "step": 1129
    },
    {
      "epoch": 0.060384214604429956,
      "grad_norm": 1.2362217903137207,
      "learning_rate": 0.00018797305965362413,
      "loss": 2.1281,
      "step": 1130
    },
    {
      "epoch": 0.060437651962486974,
      "grad_norm": 1.5063806772232056,
      "learning_rate": 0.0001879623690399829,
      "loss": 2.2824,
      "step": 1131
    },
    {
      "epoch": 0.06049108932054399,
      "grad_norm": 1.4699442386627197,
      "learning_rate": 0.00018795167842634167,
      "loss": 2.1238,
      "step": 1132
    },
    {
      "epoch": 0.06054452667860101,
      "grad_norm": 1.7500851154327393,
      "learning_rate": 0.00018794098781270047,
      "loss": 2.1964,
      "step": 1133
    },
    {
      "epoch": 0.06059796403665803,
      "grad_norm": 1.7013607025146484,
      "learning_rate": 0.00018793029719905922,
      "loss": 2.4372,
      "step": 1134
    },
    {
      "epoch": 0.060651401394715046,
      "grad_norm": 1.244396448135376,
      "learning_rate": 0.000187919606585418,
      "loss": 1.9574,
      "step": 1135
    },
    {
      "epoch": 0.060704838752772064,
      "grad_norm": 1.1468948125839233,
      "learning_rate": 0.0001879089159717768,
      "loss": 1.9165,
      "step": 1136
    },
    {
      "epoch": 0.06075827611082908,
      "grad_norm": 1.3668115139007568,
      "learning_rate": 0.00018789822535813556,
      "loss": 1.998,
      "step": 1137
    },
    {
      "epoch": 0.0608117134688861,
      "grad_norm": 1.0103230476379395,
      "learning_rate": 0.00018788753474449434,
      "loss": 2.1915,
      "step": 1138
    },
    {
      "epoch": 0.06086515082694312,
      "grad_norm": 1.234073519706726,
      "learning_rate": 0.00018787684413085312,
      "loss": 2.0633,
      "step": 1139
    },
    {
      "epoch": 0.060918588185000136,
      "grad_norm": 1.590462327003479,
      "learning_rate": 0.0001878661535172119,
      "loss": 2.2531,
      "step": 1140
    },
    {
      "epoch": 0.060972025543057154,
      "grad_norm": 1.6216062307357788,
      "learning_rate": 0.00018785546290357068,
      "loss": 2.4518,
      "step": 1141
    },
    {
      "epoch": 0.06102546290111417,
      "grad_norm": 1.3010421991348267,
      "learning_rate": 0.00018784477228992946,
      "loss": 2.3303,
      "step": 1142
    },
    {
      "epoch": 0.06107890025917119,
      "grad_norm": 1.8259239196777344,
      "learning_rate": 0.00018783408167628821,
      "loss": 2.2005,
      "step": 1143
    },
    {
      "epoch": 0.0611323376172282,
      "grad_norm": 1.1457356214523315,
      "learning_rate": 0.00018782339106264702,
      "loss": 2.2389,
      "step": 1144
    },
    {
      "epoch": 0.06118577497528522,
      "grad_norm": 1.1957412958145142,
      "learning_rate": 0.00018781270044900577,
      "loss": 2.1536,
      "step": 1145
    },
    {
      "epoch": 0.06123921233334224,
      "grad_norm": 1.188877820968628,
      "learning_rate": 0.00018780200983536455,
      "loss": 1.9932,
      "step": 1146
    },
    {
      "epoch": 0.061292649691399255,
      "grad_norm": 1.6636176109313965,
      "learning_rate": 0.00018779131922172333,
      "loss": 2.1868,
      "step": 1147
    },
    {
      "epoch": 0.06134608704945627,
      "grad_norm": 1.7992464303970337,
      "learning_rate": 0.0001877806286080821,
      "loss": 2.3179,
      "step": 1148
    },
    {
      "epoch": 0.06139952440751329,
      "grad_norm": 1.2273814678192139,
      "learning_rate": 0.0001877699379944409,
      "loss": 2.1031,
      "step": 1149
    },
    {
      "epoch": 0.06145296176557031,
      "grad_norm": 1.240279197692871,
      "learning_rate": 0.00018775924738079967,
      "loss": 2.0921,
      "step": 1150
    },
    {
      "epoch": 0.06150639912362733,
      "grad_norm": 1.1660515069961548,
      "learning_rate": 0.00018774855676715842,
      "loss": 1.9845,
      "step": 1151
    },
    {
      "epoch": 0.061559836481684345,
      "grad_norm": 2.193105459213257,
      "learning_rate": 0.00018773786615351723,
      "loss": 2.4647,
      "step": 1152
    },
    {
      "epoch": 0.06161327383974136,
      "grad_norm": 1.0673869848251343,
      "learning_rate": 0.000187727175539876,
      "loss": 2.0287,
      "step": 1153
    },
    {
      "epoch": 0.06166671119779838,
      "grad_norm": 1.4779982566833496,
      "learning_rate": 0.00018771648492623476,
      "loss": 2.3075,
      "step": 1154
    },
    {
      "epoch": 0.0617201485558554,
      "grad_norm": 1.4287060499191284,
      "learning_rate": 0.00018770579431259354,
      "loss": 2.2022,
      "step": 1155
    },
    {
      "epoch": 0.06177358591391242,
      "grad_norm": 1.2865797281265259,
      "learning_rate": 0.00018769510369895232,
      "loss": 2.1989,
      "step": 1156
    },
    {
      "epoch": 0.061827023271969435,
      "grad_norm": 1.2410532236099243,
      "learning_rate": 0.0001876844130853111,
      "loss": 2.1833,
      "step": 1157
    },
    {
      "epoch": 0.06188046063002645,
      "grad_norm": 1.2799855470657349,
      "learning_rate": 0.00018767372247166988,
      "loss": 2.19,
      "step": 1158
    },
    {
      "epoch": 0.06193389798808347,
      "grad_norm": 1.490179181098938,
      "learning_rate": 0.00018766303185802866,
      "loss": 2.1332,
      "step": 1159
    },
    {
      "epoch": 0.06198733534614049,
      "grad_norm": 1.1949231624603271,
      "learning_rate": 0.00018765234124438744,
      "loss": 2.1397,
      "step": 1160
    },
    {
      "epoch": 0.06204077270419751,
      "grad_norm": 1.0400441884994507,
      "learning_rate": 0.00018764165063074622,
      "loss": 2.2457,
      "step": 1161
    },
    {
      "epoch": 0.062094210062254525,
      "grad_norm": 1.2406542301177979,
      "learning_rate": 0.00018763096001710497,
      "loss": 2.2893,
      "step": 1162
    },
    {
      "epoch": 0.06214764742031154,
      "grad_norm": 1.904956340789795,
      "learning_rate": 0.00018762026940346378,
      "loss": 2.3423,
      "step": 1163
    },
    {
      "epoch": 0.06220108477836856,
      "grad_norm": 1.5934423208236694,
      "learning_rate": 0.00018760957878982256,
      "loss": 2.1918,
      "step": 1164
    },
    {
      "epoch": 0.06225452213642557,
      "grad_norm": 2.1056764125823975,
      "learning_rate": 0.0001875988881761813,
      "loss": 2.3897,
      "step": 1165
    },
    {
      "epoch": 0.06230795949448259,
      "grad_norm": 1.7116045951843262,
      "learning_rate": 0.0001875881975625401,
      "loss": 2.2483,
      "step": 1166
    },
    {
      "epoch": 0.06236139685253961,
      "grad_norm": 1.8360395431518555,
      "learning_rate": 0.00018757750694889887,
      "loss": 2.4597,
      "step": 1167
    },
    {
      "epoch": 0.062414834210596626,
      "grad_norm": 1.4895747900009155,
      "learning_rate": 0.00018756681633525765,
      "loss": 2.2836,
      "step": 1168
    },
    {
      "epoch": 0.062468271568653644,
      "grad_norm": 1.0561267137527466,
      "learning_rate": 0.00018755612572161643,
      "loss": 2.0434,
      "step": 1169
    },
    {
      "epoch": 0.06252170892671066,
      "grad_norm": 1.683279037475586,
      "learning_rate": 0.0001875454351079752,
      "loss": 2.269,
      "step": 1170
    },
    {
      "epoch": 0.06257514628476768,
      "grad_norm": 1.4615176916122437,
      "learning_rate": 0.000187534744494334,
      "loss": 2.3707,
      "step": 1171
    },
    {
      "epoch": 0.0626285836428247,
      "grad_norm": 1.6511906385421753,
      "learning_rate": 0.00018752405388069277,
      "loss": 2.2897,
      "step": 1172
    },
    {
      "epoch": 0.06268202100088172,
      "grad_norm": 1.3901445865631104,
      "learning_rate": 0.00018751336326705152,
      "loss": 2.2436,
      "step": 1173
    },
    {
      "epoch": 0.06273545835893873,
      "grad_norm": 1.1548112630844116,
      "learning_rate": 0.0001875026726534103,
      "loss": 2.2331,
      "step": 1174
    },
    {
      "epoch": 0.06278889571699575,
      "grad_norm": 1.5714631080627441,
      "learning_rate": 0.0001874919820397691,
      "loss": 2.3336,
      "step": 1175
    },
    {
      "epoch": 0.06284233307505277,
      "grad_norm": 1.3443329334259033,
      "learning_rate": 0.00018748129142612786,
      "loss": 2.1662,
      "step": 1176
    },
    {
      "epoch": 0.06289577043310979,
      "grad_norm": 1.3508234024047852,
      "learning_rate": 0.00018747060081248664,
      "loss": 2.0486,
      "step": 1177
    },
    {
      "epoch": 0.0629492077911668,
      "grad_norm": 1.120066523551941,
      "learning_rate": 0.00018745991019884542,
      "loss": 2.2045,
      "step": 1178
    },
    {
      "epoch": 0.06300264514922382,
      "grad_norm": 1.2957649230957031,
      "learning_rate": 0.0001874492195852042,
      "loss": 2.2942,
      "step": 1179
    },
    {
      "epoch": 0.06305608250728084,
      "grad_norm": 1.2149384021759033,
      "learning_rate": 0.00018743852897156298,
      "loss": 2.0803,
      "step": 1180
    },
    {
      "epoch": 0.06310951986533786,
      "grad_norm": 1.2305740118026733,
      "learning_rate": 0.00018742783835792176,
      "loss": 2.3748,
      "step": 1181
    },
    {
      "epoch": 0.06316295722339488,
      "grad_norm": 1.8519612550735474,
      "learning_rate": 0.00018741714774428054,
      "loss": 2.1344,
      "step": 1182
    },
    {
      "epoch": 0.0632163945814519,
      "grad_norm": 1.0341944694519043,
      "learning_rate": 0.00018740645713063932,
      "loss": 2.172,
      "step": 1183
    },
    {
      "epoch": 0.06326983193950891,
      "grad_norm": 1.3479596376419067,
      "learning_rate": 0.00018739576651699807,
      "loss": 2.2812,
      "step": 1184
    },
    {
      "epoch": 0.06332326929756593,
      "grad_norm": 1.3427224159240723,
      "learning_rate": 0.00018738507590335685,
      "loss": 2.1662,
      "step": 1185
    },
    {
      "epoch": 0.06337670665562295,
      "grad_norm": 1.8898682594299316,
      "learning_rate": 0.00018737438528971566,
      "loss": 2.1702,
      "step": 1186
    },
    {
      "epoch": 0.06343014401367997,
      "grad_norm": 1.1057149171829224,
      "learning_rate": 0.0001873636946760744,
      "loss": 2.3472,
      "step": 1187
    },
    {
      "epoch": 0.06348358137173699,
      "grad_norm": 1.574699878692627,
      "learning_rate": 0.0001873530040624332,
      "loss": 2.1166,
      "step": 1188
    },
    {
      "epoch": 0.063537018729794,
      "grad_norm": 1.492200255393982,
      "learning_rate": 0.00018734231344879197,
      "loss": 2.2903,
      "step": 1189
    },
    {
      "epoch": 0.06359045608785102,
      "grad_norm": 1.4278744459152222,
      "learning_rate": 0.00018733162283515075,
      "loss": 2.2983,
      "step": 1190
    },
    {
      "epoch": 0.06364389344590804,
      "grad_norm": 1.231750249862671,
      "learning_rate": 0.00018732093222150953,
      "loss": 2.1135,
      "step": 1191
    },
    {
      "epoch": 0.06369733080396506,
      "grad_norm": 1.2101372480392456,
      "learning_rate": 0.0001873102416078683,
      "loss": 2.0175,
      "step": 1192
    },
    {
      "epoch": 0.06375076816202208,
      "grad_norm": 1.6361513137817383,
      "learning_rate": 0.00018729955099422706,
      "loss": 2.3376,
      "step": 1193
    },
    {
      "epoch": 0.0638042055200791,
      "grad_norm": 1.368367075920105,
      "learning_rate": 0.00018728886038058587,
      "loss": 2.2612,
      "step": 1194
    },
    {
      "epoch": 0.06385764287813611,
      "grad_norm": 1.3073269128799438,
      "learning_rate": 0.00018727816976694462,
      "loss": 2.153,
      "step": 1195
    },
    {
      "epoch": 0.06391108023619312,
      "grad_norm": 1.2617723941802979,
      "learning_rate": 0.0001872674791533034,
      "loss": 2.2489,
      "step": 1196
    },
    {
      "epoch": 0.06396451759425013,
      "grad_norm": 1.2222336530685425,
      "learning_rate": 0.00018725678853966218,
      "loss": 2.1791,
      "step": 1197
    },
    {
      "epoch": 0.06401795495230715,
      "grad_norm": 1.3469938039779663,
      "learning_rate": 0.00018724609792602096,
      "loss": 2.2071,
      "step": 1198
    },
    {
      "epoch": 0.06407139231036417,
      "grad_norm": 1.844152808189392,
      "learning_rate": 0.00018723540731237974,
      "loss": 2.3629,
      "step": 1199
    },
    {
      "epoch": 0.06412482966842119,
      "grad_norm": 1.4045593738555908,
      "learning_rate": 0.00018722471669873852,
      "loss": 2.2584,
      "step": 1200
    },
    {
      "epoch": 0.0641782670264782,
      "grad_norm": 1.0185531377792358,
      "learning_rate": 0.00018721402608509727,
      "loss": 2.1432,
      "step": 1201
    },
    {
      "epoch": 0.06423170438453522,
      "grad_norm": 1.477858543395996,
      "learning_rate": 0.00018720333547145608,
      "loss": 2.2431,
      "step": 1202
    },
    {
      "epoch": 0.06428514174259224,
      "grad_norm": 1.345423698425293,
      "learning_rate": 0.00018719264485781486,
      "loss": 2.355,
      "step": 1203
    },
    {
      "epoch": 0.06433857910064926,
      "grad_norm": 1.2818424701690674,
      "learning_rate": 0.0001871819542441736,
      "loss": 2.3297,
      "step": 1204
    },
    {
      "epoch": 0.06439201645870628,
      "grad_norm": 1.7878777980804443,
      "learning_rate": 0.00018717126363053242,
      "loss": 2.1328,
      "step": 1205
    },
    {
      "epoch": 0.0644454538167633,
      "grad_norm": 1.5935838222503662,
      "learning_rate": 0.00018716057301689117,
      "loss": 1.9352,
      "step": 1206
    },
    {
      "epoch": 0.06449889117482031,
      "grad_norm": 1.5872386693954468,
      "learning_rate": 0.00018714988240324995,
      "loss": 2.3994,
      "step": 1207
    },
    {
      "epoch": 0.06455232853287733,
      "grad_norm": 1.1806674003601074,
      "learning_rate": 0.00018713919178960873,
      "loss": 1.9563,
      "step": 1208
    },
    {
      "epoch": 0.06460576589093435,
      "grad_norm": 1.0452752113342285,
      "learning_rate": 0.0001871285011759675,
      "loss": 2.0167,
      "step": 1209
    },
    {
      "epoch": 0.06465920324899137,
      "grad_norm": 1.4723173379898071,
      "learning_rate": 0.0001871178105623263,
      "loss": 2.1798,
      "step": 1210
    },
    {
      "epoch": 0.06471264060704839,
      "grad_norm": 1.2491873502731323,
      "learning_rate": 0.00018710711994868507,
      "loss": 1.9329,
      "step": 1211
    },
    {
      "epoch": 0.0647660779651054,
      "grad_norm": 1.440325140953064,
      "learning_rate": 0.00018709642933504382,
      "loss": 2.0991,
      "step": 1212
    },
    {
      "epoch": 0.06481951532316242,
      "grad_norm": 1.572763204574585,
      "learning_rate": 0.00018708573872140263,
      "loss": 2.1119,
      "step": 1213
    },
    {
      "epoch": 0.06487295268121944,
      "grad_norm": 1.1986311674118042,
      "learning_rate": 0.0001870750481077614,
      "loss": 2.0589,
      "step": 1214
    },
    {
      "epoch": 0.06492639003927646,
      "grad_norm": 1.7337582111358643,
      "learning_rate": 0.00018706435749412016,
      "loss": 2.3125,
      "step": 1215
    },
    {
      "epoch": 0.06497982739733348,
      "grad_norm": 1.0905672311782837,
      "learning_rate": 0.00018705366688047894,
      "loss": 2.1522,
      "step": 1216
    },
    {
      "epoch": 0.0650332647553905,
      "grad_norm": 1.5533078908920288,
      "learning_rate": 0.00018704297626683775,
      "loss": 2.349,
      "step": 1217
    },
    {
      "epoch": 0.06508670211344751,
      "grad_norm": 1.5246057510375977,
      "learning_rate": 0.0001870322856531965,
      "loss": 2.3748,
      "step": 1218
    },
    {
      "epoch": 0.06514013947150453,
      "grad_norm": 1.603636622428894,
      "learning_rate": 0.00018702159503955528,
      "loss": 2.2554,
      "step": 1219
    },
    {
      "epoch": 0.06519357682956155,
      "grad_norm": 1.8260084390640259,
      "learning_rate": 0.00018701090442591406,
      "loss": 2.4607,
      "step": 1220
    },
    {
      "epoch": 0.06524701418761857,
      "grad_norm": 1.704854130744934,
      "learning_rate": 0.00018700021381227284,
      "loss": 2.2718,
      "step": 1221
    },
    {
      "epoch": 0.06530045154567558,
      "grad_norm": 1.0907166004180908,
      "learning_rate": 0.00018698952319863162,
      "loss": 2.025,
      "step": 1222
    },
    {
      "epoch": 0.0653538889037326,
      "grad_norm": 1.600178837776184,
      "learning_rate": 0.00018697883258499037,
      "loss": 2.3751,
      "step": 1223
    },
    {
      "epoch": 0.06540732626178962,
      "grad_norm": 1.2833760976791382,
      "learning_rate": 0.00018696814197134915,
      "loss": 2.2658,
      "step": 1224
    },
    {
      "epoch": 0.06546076361984664,
      "grad_norm": 1.1891366243362427,
      "learning_rate": 0.00018695745135770796,
      "loss": 2.2215,
      "step": 1225
    },
    {
      "epoch": 0.06551420097790366,
      "grad_norm": 1.2411980628967285,
      "learning_rate": 0.0001869467607440667,
      "loss": 2.1951,
      "step": 1226
    },
    {
      "epoch": 0.06556763833596067,
      "grad_norm": 1.5022231340408325,
      "learning_rate": 0.0001869360701304255,
      "loss": 2.3438,
      "step": 1227
    },
    {
      "epoch": 0.06562107569401769,
      "grad_norm": 1.6513786315917969,
      "learning_rate": 0.0001869253795167843,
      "loss": 2.2184,
      "step": 1228
    },
    {
      "epoch": 0.06567451305207471,
      "grad_norm": 0.9682894349098206,
      "learning_rate": 0.00018691468890314305,
      "loss": 2.2563,
      "step": 1229
    },
    {
      "epoch": 0.06572795041013173,
      "grad_norm": 1.5352872610092163,
      "learning_rate": 0.00018690399828950183,
      "loss": 2.1252,
      "step": 1230
    },
    {
      "epoch": 0.06578138776818875,
      "grad_norm": 0.9690799117088318,
      "learning_rate": 0.0001868933076758606,
      "loss": 2.0947,
      "step": 1231
    },
    {
      "epoch": 0.06583482512624576,
      "grad_norm": 1.7638417482376099,
      "learning_rate": 0.00018688261706221939,
      "loss": 2.2349,
      "step": 1232
    },
    {
      "epoch": 0.06588826248430278,
      "grad_norm": 1.5254180431365967,
      "learning_rate": 0.00018687192644857817,
      "loss": 2.1491,
      "step": 1233
    },
    {
      "epoch": 0.0659416998423598,
      "grad_norm": 1.187940239906311,
      "learning_rate": 0.00018686123583493692,
      "loss": 2.1991,
      "step": 1234
    },
    {
      "epoch": 0.06599513720041682,
      "grad_norm": 0.9449422359466553,
      "learning_rate": 0.0001868505452212957,
      "loss": 2.167,
      "step": 1235
    },
    {
      "epoch": 0.06604857455847384,
      "grad_norm": 1.6066957712173462,
      "learning_rate": 0.0001868398546076545,
      "loss": 2.2601,
      "step": 1236
    },
    {
      "epoch": 0.06610201191653085,
      "grad_norm": 1.287347435951233,
      "learning_rate": 0.00018682916399401326,
      "loss": 2.3044,
      "step": 1237
    },
    {
      "epoch": 0.06615544927458786,
      "grad_norm": 1.5456123352050781,
      "learning_rate": 0.00018681847338037204,
      "loss": 2.1773,
      "step": 1238
    },
    {
      "epoch": 0.06620888663264488,
      "grad_norm": 1.5839694738388062,
      "learning_rate": 0.00018680778276673082,
      "loss": 2.3302,
      "step": 1239
    },
    {
      "epoch": 0.0662623239907019,
      "grad_norm": 1.9701440334320068,
      "learning_rate": 0.0001867970921530896,
      "loss": 2.3826,
      "step": 1240
    },
    {
      "epoch": 0.06631576134875891,
      "grad_norm": 1.1005277633666992,
      "learning_rate": 0.00018678640153944838,
      "loss": 2.2159,
      "step": 1241
    },
    {
      "epoch": 0.06636919870681593,
      "grad_norm": 1.4130628108978271,
      "learning_rate": 0.00018677571092580716,
      "loss": 2.277,
      "step": 1242
    },
    {
      "epoch": 0.06642263606487295,
      "grad_norm": 1.3661515712738037,
      "learning_rate": 0.0001867650203121659,
      "loss": 2.353,
      "step": 1243
    },
    {
      "epoch": 0.06647607342292997,
      "grad_norm": 0.9903981685638428,
      "learning_rate": 0.00018675432969852472,
      "loss": 1.9955,
      "step": 1244
    },
    {
      "epoch": 0.06652951078098698,
      "grad_norm": 1.2584638595581055,
      "learning_rate": 0.0001867436390848835,
      "loss": 2.367,
      "step": 1245
    },
    {
      "epoch": 0.066582948139044,
      "grad_norm": 1.4046931266784668,
      "learning_rate": 0.00018673294847124225,
      "loss": 2.1281,
      "step": 1246
    },
    {
      "epoch": 0.06663638549710102,
      "grad_norm": 1.3431487083435059,
      "learning_rate": 0.00018672225785760105,
      "loss": 2.1498,
      "step": 1247
    },
    {
      "epoch": 0.06668982285515804,
      "grad_norm": 1.5995800495147705,
      "learning_rate": 0.0001867115672439598,
      "loss": 2.3142,
      "step": 1248
    },
    {
      "epoch": 0.06674326021321506,
      "grad_norm": 1.1342296600341797,
      "learning_rate": 0.00018670087663031859,
      "loss": 2.3318,
      "step": 1249
    },
    {
      "epoch": 0.06679669757127207,
      "grad_norm": 1.2879199981689453,
      "learning_rate": 0.00018669018601667737,
      "loss": 2.2022,
      "step": 1250
    },
    {
      "epoch": 0.06685013492932909,
      "grad_norm": 1.6734880208969116,
      "learning_rate": 0.00018667949540303615,
      "loss": 2.1724,
      "step": 1251
    },
    {
      "epoch": 0.06690357228738611,
      "grad_norm": 1.6643182039260864,
      "learning_rate": 0.00018666880478939493,
      "loss": 2.1239,
      "step": 1252
    },
    {
      "epoch": 0.06695700964544313,
      "grad_norm": 1.2292622327804565,
      "learning_rate": 0.0001866581141757537,
      "loss": 2.1426,
      "step": 1253
    },
    {
      "epoch": 0.06701044700350015,
      "grad_norm": 1.176318883895874,
      "learning_rate": 0.00018664742356211246,
      "loss": 2.0986,
      "step": 1254
    },
    {
      "epoch": 0.06706388436155716,
      "grad_norm": 1.7720534801483154,
      "learning_rate": 0.00018663673294847126,
      "loss": 2.1722,
      "step": 1255
    },
    {
      "epoch": 0.06711732171961418,
      "grad_norm": 1.1613878011703491,
      "learning_rate": 0.00018662604233483004,
      "loss": 2.1643,
      "step": 1256
    },
    {
      "epoch": 0.0671707590776712,
      "grad_norm": 1.255971908569336,
      "learning_rate": 0.0001866153517211888,
      "loss": 2.1674,
      "step": 1257
    },
    {
      "epoch": 0.06722419643572822,
      "grad_norm": 1.5198360681533813,
      "learning_rate": 0.00018660466110754758,
      "loss": 2.2252,
      "step": 1258
    },
    {
      "epoch": 0.06727763379378524,
      "grad_norm": 1.365186333656311,
      "learning_rate": 0.00018659397049390636,
      "loss": 2.1546,
      "step": 1259
    },
    {
      "epoch": 0.06733107115184225,
      "grad_norm": 1.0982798337936401,
      "learning_rate": 0.00018658327988026514,
      "loss": 2.0881,
      "step": 1260
    },
    {
      "epoch": 0.06738450850989927,
      "grad_norm": 1.4143027067184448,
      "learning_rate": 0.00018657258926662391,
      "loss": 2.2013,
      "step": 1261
    },
    {
      "epoch": 0.06743794586795629,
      "grad_norm": 1.9106287956237793,
      "learning_rate": 0.00018656189865298267,
      "loss": 2.1197,
      "step": 1262
    },
    {
      "epoch": 0.06749138322601331,
      "grad_norm": 1.3441907167434692,
      "learning_rate": 0.00018655120803934147,
      "loss": 2.1415,
      "step": 1263
    },
    {
      "epoch": 0.06754482058407033,
      "grad_norm": 1.641274333000183,
      "learning_rate": 0.00018654051742570025,
      "loss": 2.2859,
      "step": 1264
    },
    {
      "epoch": 0.06759825794212734,
      "grad_norm": 1.5652837753295898,
      "learning_rate": 0.000186529826812059,
      "loss": 2.28,
      "step": 1265
    },
    {
      "epoch": 0.06765169530018436,
      "grad_norm": 2.2534592151641846,
      "learning_rate": 0.00018651913619841779,
      "loss": 2.3355,
      "step": 1266
    },
    {
      "epoch": 0.06770513265824138,
      "grad_norm": 1.7344040870666504,
      "learning_rate": 0.0001865084455847766,
      "loss": 2.2652,
      "step": 1267
    },
    {
      "epoch": 0.0677585700162984,
      "grad_norm": 1.2850011587142944,
      "learning_rate": 0.00018649775497113535,
      "loss": 2.0568,
      "step": 1268
    },
    {
      "epoch": 0.06781200737435542,
      "grad_norm": 0.8777760863304138,
      "learning_rate": 0.00018648706435749412,
      "loss": 2.1407,
      "step": 1269
    },
    {
      "epoch": 0.06786544473241243,
      "grad_norm": 1.99284029006958,
      "learning_rate": 0.0001864763737438529,
      "loss": 2.3608,
      "step": 1270
    },
    {
      "epoch": 0.06791888209046945,
      "grad_norm": 1.6353205442428589,
      "learning_rate": 0.00018646568313021168,
      "loss": 2.3132,
      "step": 1271
    },
    {
      "epoch": 0.06797231944852647,
      "grad_norm": 1.3844709396362305,
      "learning_rate": 0.00018645499251657046,
      "loss": 2.118,
      "step": 1272
    },
    {
      "epoch": 0.06802575680658349,
      "grad_norm": 1.8528965711593628,
      "learning_rate": 0.00018644430190292924,
      "loss": 2.1632,
      "step": 1273
    },
    {
      "epoch": 0.0680791941646405,
      "grad_norm": 1.4086723327636719,
      "learning_rate": 0.00018643361128928802,
      "loss": 2.2924,
      "step": 1274
    },
    {
      "epoch": 0.06813263152269752,
      "grad_norm": 1.4485602378845215,
      "learning_rate": 0.0001864229206756468,
      "loss": 2.4413,
      "step": 1275
    },
    {
      "epoch": 0.06818606888075454,
      "grad_norm": 1.4350452423095703,
      "learning_rate": 0.00018641223006200556,
      "loss": 2.1836,
      "step": 1276
    },
    {
      "epoch": 0.06823950623881156,
      "grad_norm": 1.5339176654815674,
      "learning_rate": 0.00018640153944836434,
      "loss": 2.3297,
      "step": 1277
    },
    {
      "epoch": 0.06829294359686858,
      "grad_norm": 1.4407581090927124,
      "learning_rate": 0.00018639084883472314,
      "loss": 2.218,
      "step": 1278
    },
    {
      "epoch": 0.0683463809549256,
      "grad_norm": 1.292507529258728,
      "learning_rate": 0.0001863801582210819,
      "loss": 2.075,
      "step": 1279
    },
    {
      "epoch": 0.0683998183129826,
      "grad_norm": 1.5623703002929688,
      "learning_rate": 0.00018636946760744067,
      "loss": 2.2947,
      "step": 1280
    },
    {
      "epoch": 0.06845325567103962,
      "grad_norm": 1.0760314464569092,
      "learning_rate": 0.00018635877699379945,
      "loss": 2.1205,
      "step": 1281
    },
    {
      "epoch": 0.06850669302909664,
      "grad_norm": 1.0255458354949951,
      "learning_rate": 0.00018634808638015823,
      "loss": 2.0125,
      "step": 1282
    },
    {
      "epoch": 0.06856013038715365,
      "grad_norm": 1.1126635074615479,
      "learning_rate": 0.000186337395766517,
      "loss": 2.1603,
      "step": 1283
    },
    {
      "epoch": 0.06861356774521067,
      "grad_norm": 1.0954760313034058,
      "learning_rate": 0.0001863267051528758,
      "loss": 2.1775,
      "step": 1284
    },
    {
      "epoch": 0.06866700510326769,
      "grad_norm": 1.429276466369629,
      "learning_rate": 0.00018631601453923455,
      "loss": 2.2166,
      "step": 1285
    },
    {
      "epoch": 0.06872044246132471,
      "grad_norm": 1.6309316158294678,
      "learning_rate": 0.00018630532392559335,
      "loss": 2.3416,
      "step": 1286
    },
    {
      "epoch": 0.06877387981938173,
      "grad_norm": 2.2086756229400635,
      "learning_rate": 0.0001862946333119521,
      "loss": 2.2264,
      "step": 1287
    },
    {
      "epoch": 0.06882731717743874,
      "grad_norm": 1.0790635347366333,
      "learning_rate": 0.00018628394269831088,
      "loss": 2.1585,
      "step": 1288
    },
    {
      "epoch": 0.06888075453549576,
      "grad_norm": 1.4356039762496948,
      "learning_rate": 0.00018627325208466966,
      "loss": 1.9695,
      "step": 1289
    },
    {
      "epoch": 0.06893419189355278,
      "grad_norm": 1.5775433778762817,
      "learning_rate": 0.00018626256147102844,
      "loss": 2.3456,
      "step": 1290
    },
    {
      "epoch": 0.0689876292516098,
      "grad_norm": 0.9209877252578735,
      "learning_rate": 0.00018625187085738722,
      "loss": 2.0506,
      "step": 1291
    },
    {
      "epoch": 0.06904106660966682,
      "grad_norm": 1.340585708618164,
      "learning_rate": 0.000186241180243746,
      "loss": 2.2636,
      "step": 1292
    },
    {
      "epoch": 0.06909450396772383,
      "grad_norm": 1.8424830436706543,
      "learning_rate": 0.00018623048963010478,
      "loss": 2.0788,
      "step": 1293
    },
    {
      "epoch": 0.06914794132578085,
      "grad_norm": 1.5933834314346313,
      "learning_rate": 0.00018621979901646356,
      "loss": 2.5326,
      "step": 1294
    },
    {
      "epoch": 0.06920137868383787,
      "grad_norm": 1.4750289916992188,
      "learning_rate": 0.00018620910840282234,
      "loss": 2.2829,
      "step": 1295
    },
    {
      "epoch": 0.06925481604189489,
      "grad_norm": 1.049826741218567,
      "learning_rate": 0.0001861984177891811,
      "loss": 2.2052,
      "step": 1296
    },
    {
      "epoch": 0.0693082533999519,
      "grad_norm": 1.585673451423645,
      "learning_rate": 0.0001861877271755399,
      "loss": 2.144,
      "step": 1297
    },
    {
      "epoch": 0.06936169075800892,
      "grad_norm": 1.1721394062042236,
      "learning_rate": 0.00018617703656189865,
      "loss": 2.0247,
      "step": 1298
    },
    {
      "epoch": 0.06941512811606594,
      "grad_norm": 1.575453758239746,
      "learning_rate": 0.00018616634594825743,
      "loss": 2.0356,
      "step": 1299
    },
    {
      "epoch": 0.06946856547412296,
      "grad_norm": 1.4046339988708496,
      "learning_rate": 0.0001861556553346162,
      "loss": 2.183,
      "step": 1300
    },
    {
      "epoch": 0.06952200283217998,
      "grad_norm": 1.2259160280227661,
      "learning_rate": 0.000186144964720975,
      "loss": 2.1686,
      "step": 1301
    },
    {
      "epoch": 0.069575440190237,
      "grad_norm": 1.1437169313430786,
      "learning_rate": 0.00018613427410733377,
      "loss": 2.1944,
      "step": 1302
    },
    {
      "epoch": 0.06962887754829401,
      "grad_norm": 1.237789511680603,
      "learning_rate": 0.00018612358349369255,
      "loss": 2.2103,
      "step": 1303
    },
    {
      "epoch": 0.06968231490635103,
      "grad_norm": 1.9384722709655762,
      "learning_rate": 0.0001861128928800513,
      "loss": 2.3889,
      "step": 1304
    },
    {
      "epoch": 0.06973575226440805,
      "grad_norm": 1.7615306377410889,
      "learning_rate": 0.0001861022022664101,
      "loss": 2.3511,
      "step": 1305
    },
    {
      "epoch": 0.06978918962246507,
      "grad_norm": 1.1681634187698364,
      "learning_rate": 0.0001860915116527689,
      "loss": 2.1815,
      "step": 1306
    },
    {
      "epoch": 0.06984262698052209,
      "grad_norm": 1.6032905578613281,
      "learning_rate": 0.00018608082103912764,
      "loss": 2.313,
      "step": 1307
    },
    {
      "epoch": 0.0698960643385791,
      "grad_norm": 1.7628589868545532,
      "learning_rate": 0.00018607013042548642,
      "loss": 2.3795,
      "step": 1308
    },
    {
      "epoch": 0.06994950169663612,
      "grad_norm": 1.575709581375122,
      "learning_rate": 0.0001860594398118452,
      "loss": 2.1733,
      "step": 1309
    },
    {
      "epoch": 0.07000293905469314,
      "grad_norm": 1.116281270980835,
      "learning_rate": 0.00018604874919820398,
      "loss": 2.0081,
      "step": 1310
    },
    {
      "epoch": 0.07005637641275016,
      "grad_norm": 1.1445231437683105,
      "learning_rate": 0.00018603805858456276,
      "loss": 2.2425,
      "step": 1311
    },
    {
      "epoch": 0.07010981377080718,
      "grad_norm": 1.4601579904556274,
      "learning_rate": 0.00018602736797092154,
      "loss": 2.1811,
      "step": 1312
    },
    {
      "epoch": 0.07016325112886419,
      "grad_norm": 1.043936014175415,
      "learning_rate": 0.00018601667735728032,
      "loss": 2.2945,
      "step": 1313
    },
    {
      "epoch": 0.07021668848692121,
      "grad_norm": 1.139559268951416,
      "learning_rate": 0.0001860059867436391,
      "loss": 2.2234,
      "step": 1314
    },
    {
      "epoch": 0.07027012584497823,
      "grad_norm": 1.422768473625183,
      "learning_rate": 0.00018599529612999785,
      "loss": 2.0892,
      "step": 1315
    },
    {
      "epoch": 0.07032356320303525,
      "grad_norm": 1.595674753189087,
      "learning_rate": 0.00018598460551635666,
      "loss": 2.252,
      "step": 1316
    },
    {
      "epoch": 0.07037700056109226,
      "grad_norm": 1.4789143800735474,
      "learning_rate": 0.00018597391490271544,
      "loss": 2.0606,
      "step": 1317
    },
    {
      "epoch": 0.07043043791914928,
      "grad_norm": 1.9637277126312256,
      "learning_rate": 0.0001859632242890742,
      "loss": 2.2658,
      "step": 1318
    },
    {
      "epoch": 0.0704838752772063,
      "grad_norm": 1.5863356590270996,
      "learning_rate": 0.00018595253367543297,
      "loss": 2.2536,
      "step": 1319
    },
    {
      "epoch": 0.07053731263526332,
      "grad_norm": 1.0955311059951782,
      "learning_rate": 0.00018594184306179175,
      "loss": 2.0612,
      "step": 1320
    },
    {
      "epoch": 0.07059074999332034,
      "grad_norm": 1.5460015535354614,
      "learning_rate": 0.00018593115244815053,
      "loss": 2.2599,
      "step": 1321
    },
    {
      "epoch": 0.07064418735137734,
      "grad_norm": 0.8105376362800598,
      "learning_rate": 0.0001859204618345093,
      "loss": 2.1816,
      "step": 1322
    },
    {
      "epoch": 0.07069762470943436,
      "grad_norm": 1.2785546779632568,
      "learning_rate": 0.0001859097712208681,
      "loss": 2.1016,
      "step": 1323
    },
    {
      "epoch": 0.07075106206749138,
      "grad_norm": 1.5231291055679321,
      "learning_rate": 0.00018589908060722687,
      "loss": 2.5112,
      "step": 1324
    },
    {
      "epoch": 0.0708044994255484,
      "grad_norm": 1.3462724685668945,
      "learning_rate": 0.00018588838999358565,
      "loss": 2.1574,
      "step": 1325
    },
    {
      "epoch": 0.07085793678360541,
      "grad_norm": 0.950619637966156,
      "learning_rate": 0.0001858776993799444,
      "loss": 1.9715,
      "step": 1326
    },
    {
      "epoch": 0.07091137414166243,
      "grad_norm": 1.2025517225265503,
      "learning_rate": 0.00018586700876630318,
      "loss": 2.0905,
      "step": 1327
    },
    {
      "epoch": 0.07096481149971945,
      "grad_norm": 2.0298330783843994,
      "learning_rate": 0.000185856318152662,
      "loss": 2.323,
      "step": 1328
    },
    {
      "epoch": 0.07101824885777647,
      "grad_norm": 1.4252837896347046,
      "learning_rate": 0.00018584562753902074,
      "loss": 2.2018,
      "step": 1329
    },
    {
      "epoch": 0.07107168621583349,
      "grad_norm": 1.5138872861862183,
      "learning_rate": 0.00018583493692537952,
      "loss": 2.1175,
      "step": 1330
    },
    {
      "epoch": 0.0711251235738905,
      "grad_norm": 1.2307595014572144,
      "learning_rate": 0.0001858242463117383,
      "loss": 2.209,
      "step": 1331
    },
    {
      "epoch": 0.07117856093194752,
      "grad_norm": 1.2220196723937988,
      "learning_rate": 0.00018581355569809708,
      "loss": 2.108,
      "step": 1332
    },
    {
      "epoch": 0.07123199829000454,
      "grad_norm": 0.8599377870559692,
      "learning_rate": 0.00018580286508445586,
      "loss": 1.8907,
      "step": 1333
    },
    {
      "epoch": 0.07128543564806156,
      "grad_norm": 1.4157921075820923,
      "learning_rate": 0.00018579217447081464,
      "loss": 2.2594,
      "step": 1334
    },
    {
      "epoch": 0.07133887300611857,
      "grad_norm": 1.8811167478561401,
      "learning_rate": 0.00018578148385717342,
      "loss": 2.4993,
      "step": 1335
    },
    {
      "epoch": 0.07139231036417559,
      "grad_norm": 1.7578762769699097,
      "learning_rate": 0.0001857707932435322,
      "loss": 2.1875,
      "step": 1336
    },
    {
      "epoch": 0.07144574772223261,
      "grad_norm": 1.117889165878296,
      "learning_rate": 0.00018576010262989095,
      "loss": 2.1167,
      "step": 1337
    },
    {
      "epoch": 0.07149918508028963,
      "grad_norm": 1.4668625593185425,
      "learning_rate": 0.00018574941201624973,
      "loss": 2.1109,
      "step": 1338
    },
    {
      "epoch": 0.07155262243834665,
      "grad_norm": 1.376128077507019,
      "learning_rate": 0.00018573872140260854,
      "loss": 2.2559,
      "step": 1339
    },
    {
      "epoch": 0.07160605979640366,
      "grad_norm": 1.8219201564788818,
      "learning_rate": 0.0001857280307889673,
      "loss": 2.4507,
      "step": 1340
    },
    {
      "epoch": 0.07165949715446068,
      "grad_norm": 1.8500537872314453,
      "learning_rate": 0.00018571734017532607,
      "loss": 2.0589,
      "step": 1341
    },
    {
      "epoch": 0.0717129345125177,
      "grad_norm": 1.547136902809143,
      "learning_rate": 0.00018570664956168485,
      "loss": 2.2866,
      "step": 1342
    },
    {
      "epoch": 0.07176637187057472,
      "grad_norm": 1.4256631135940552,
      "learning_rate": 0.00018569595894804363,
      "loss": 2.257,
      "step": 1343
    },
    {
      "epoch": 0.07181980922863174,
      "grad_norm": 1.257672667503357,
      "learning_rate": 0.0001856852683344024,
      "loss": 2.188,
      "step": 1344
    },
    {
      "epoch": 0.07187324658668875,
      "grad_norm": 1.0880510807037354,
      "learning_rate": 0.0001856745777207612,
      "loss": 1.9993,
      "step": 1345
    },
    {
      "epoch": 0.07192668394474577,
      "grad_norm": 1.8582509756088257,
      "learning_rate": 0.00018566388710711994,
      "loss": 2.2042,
      "step": 1346
    },
    {
      "epoch": 0.07198012130280279,
      "grad_norm": 1.5738598108291626,
      "learning_rate": 0.00018565319649347875,
      "loss": 2.173,
      "step": 1347
    },
    {
      "epoch": 0.07203355866085981,
      "grad_norm": 1.9224529266357422,
      "learning_rate": 0.0001856425058798375,
      "loss": 2.5058,
      "step": 1348
    },
    {
      "epoch": 0.07208699601891683,
      "grad_norm": 1.1597963571548462,
      "learning_rate": 0.00018563181526619628,
      "loss": 2.1478,
      "step": 1349
    },
    {
      "epoch": 0.07214043337697384,
      "grad_norm": 2.0536739826202393,
      "learning_rate": 0.00018562112465255506,
      "loss": 2.4461,
      "step": 1350
    },
    {
      "epoch": 0.07219387073503086,
      "grad_norm": 1.1227514743804932,
      "learning_rate": 0.00018561043403891384,
      "loss": 2.2177,
      "step": 1351
    },
    {
      "epoch": 0.07224730809308788,
      "grad_norm": 1.6366387605667114,
      "learning_rate": 0.00018559974342527262,
      "loss": 2.2726,
      "step": 1352
    },
    {
      "epoch": 0.0723007454511449,
      "grad_norm": 1.4395134449005127,
      "learning_rate": 0.0001855890528116314,
      "loss": 2.2604,
      "step": 1353
    },
    {
      "epoch": 0.07235418280920192,
      "grad_norm": 1.2351953983306885,
      "learning_rate": 0.00018557836219799015,
      "loss": 2.0068,
      "step": 1354
    },
    {
      "epoch": 0.07240762016725893,
      "grad_norm": 1.317794919013977,
      "learning_rate": 0.00018556767158434896,
      "loss": 2.1305,
      "step": 1355
    },
    {
      "epoch": 0.07246105752531595,
      "grad_norm": 1.3166848421096802,
      "learning_rate": 0.00018555698097070774,
      "loss": 2.1909,
      "step": 1356
    },
    {
      "epoch": 0.07251449488337297,
      "grad_norm": 1.2564224004745483,
      "learning_rate": 0.0001855462903570665,
      "loss": 2.1437,
      "step": 1357
    },
    {
      "epoch": 0.07256793224142999,
      "grad_norm": 1.1819238662719727,
      "learning_rate": 0.0001855355997434253,
      "loss": 2.241,
      "step": 1358
    },
    {
      "epoch": 0.072621369599487,
      "grad_norm": 1.2622253894805908,
      "learning_rate": 0.00018552490912978408,
      "loss": 2.0516,
      "step": 1359
    },
    {
      "epoch": 0.07267480695754402,
      "grad_norm": 1.2839627265930176,
      "learning_rate": 0.00018551421851614283,
      "loss": 2.1194,
      "step": 1360
    },
    {
      "epoch": 0.07272824431560104,
      "grad_norm": 1.043582558631897,
      "learning_rate": 0.0001855035279025016,
      "loss": 2.0638,
      "step": 1361
    },
    {
      "epoch": 0.07278168167365806,
      "grad_norm": 1.4825986623764038,
      "learning_rate": 0.0001854928372888604,
      "loss": 2.1095,
      "step": 1362
    },
    {
      "epoch": 0.07283511903171508,
      "grad_norm": 1.2197363376617432,
      "learning_rate": 0.00018548214667521917,
      "loss": 2.3115,
      "step": 1363
    },
    {
      "epoch": 0.07288855638977208,
      "grad_norm": 1.367581844329834,
      "learning_rate": 0.00018547145606157795,
      "loss": 2.342,
      "step": 1364
    },
    {
      "epoch": 0.0729419937478291,
      "grad_norm": 0.9916614294052124,
      "learning_rate": 0.0001854607654479367,
      "loss": 2.0314,
      "step": 1365
    },
    {
      "epoch": 0.07299543110588612,
      "grad_norm": 1.0215859413146973,
      "learning_rate": 0.0001854500748342955,
      "loss": 1.9719,
      "step": 1366
    },
    {
      "epoch": 0.07304886846394314,
      "grad_norm": 1.1219431161880493,
      "learning_rate": 0.0001854393842206543,
      "loss": 2.0929,
      "step": 1367
    },
    {
      "epoch": 0.07310230582200015,
      "grad_norm": 1.3950327634811401,
      "learning_rate": 0.00018542869360701304,
      "loss": 2.1715,
      "step": 1368
    },
    {
      "epoch": 0.07315574318005717,
      "grad_norm": 1.1569100618362427,
      "learning_rate": 0.00018541800299337182,
      "loss": 2.146,
      "step": 1369
    },
    {
      "epoch": 0.07320918053811419,
      "grad_norm": 1.744154453277588,
      "learning_rate": 0.00018540731237973063,
      "loss": 2.2627,
      "step": 1370
    },
    {
      "epoch": 0.07326261789617121,
      "grad_norm": 1.0649769306182861,
      "learning_rate": 0.00018539662176608938,
      "loss": 2.2517,
      "step": 1371
    },
    {
      "epoch": 0.07331605525422823,
      "grad_norm": 1.4627397060394287,
      "learning_rate": 0.00018538593115244816,
      "loss": 2.2431,
      "step": 1372
    },
    {
      "epoch": 0.07336949261228524,
      "grad_norm": 1.4806857109069824,
      "learning_rate": 0.00018537524053880694,
      "loss": 2.1199,
      "step": 1373
    },
    {
      "epoch": 0.07342292997034226,
      "grad_norm": 1.263495683670044,
      "learning_rate": 0.00018536454992516572,
      "loss": 2.1715,
      "step": 1374
    },
    {
      "epoch": 0.07347636732839928,
      "grad_norm": 1.3792665004730225,
      "learning_rate": 0.0001853538593115245,
      "loss": 2.0836,
      "step": 1375
    },
    {
      "epoch": 0.0735298046864563,
      "grad_norm": 1.5555400848388672,
      "learning_rate": 0.00018534316869788325,
      "loss": 2.4722,
      "step": 1376
    },
    {
      "epoch": 0.07358324204451332,
      "grad_norm": 1.0970503091812134,
      "learning_rate": 0.00018533247808424203,
      "loss": 2.1061,
      "step": 1377
    },
    {
      "epoch": 0.07363667940257033,
      "grad_norm": 1.8206746578216553,
      "learning_rate": 0.00018532178747060084,
      "loss": 2.2635,
      "step": 1378
    },
    {
      "epoch": 0.07369011676062735,
      "grad_norm": 1.584054946899414,
      "learning_rate": 0.0001853110968569596,
      "loss": 2.3181,
      "step": 1379
    },
    {
      "epoch": 0.07374355411868437,
      "grad_norm": 1.161438226699829,
      "learning_rate": 0.00018530040624331837,
      "loss": 2.2695,
      "step": 1380
    },
    {
      "epoch": 0.07379699147674139,
      "grad_norm": 1.191291093826294,
      "learning_rate": 0.00018528971562967717,
      "loss": 2.1859,
      "step": 1381
    },
    {
      "epoch": 0.0738504288347984,
      "grad_norm": 1.2499217987060547,
      "learning_rate": 0.00018527902501603593,
      "loss": 2.0459,
      "step": 1382
    },
    {
      "epoch": 0.07390386619285542,
      "grad_norm": 1.4041775465011597,
      "learning_rate": 0.0001852683344023947,
      "loss": 2.15,
      "step": 1383
    },
    {
      "epoch": 0.07395730355091244,
      "grad_norm": 1.027524471282959,
      "learning_rate": 0.00018525764378875349,
      "loss": 2.1661,
      "step": 1384
    },
    {
      "epoch": 0.07401074090896946,
      "grad_norm": 1.1130601167678833,
      "learning_rate": 0.00018524695317511227,
      "loss": 2.1729,
      "step": 1385
    },
    {
      "epoch": 0.07406417826702648,
      "grad_norm": 0.894344687461853,
      "learning_rate": 0.00018523626256147105,
      "loss": 2.0146,
      "step": 1386
    },
    {
      "epoch": 0.0741176156250835,
      "grad_norm": 1.496914029121399,
      "learning_rate": 0.00018522557194782983,
      "loss": 2.3952,
      "step": 1387
    },
    {
      "epoch": 0.07417105298314051,
      "grad_norm": 1.7647151947021484,
      "learning_rate": 0.00018521488133418858,
      "loss": 2.2039,
      "step": 1388
    },
    {
      "epoch": 0.07422449034119753,
      "grad_norm": 1.111034870147705,
      "learning_rate": 0.00018520419072054738,
      "loss": 2.1294,
      "step": 1389
    },
    {
      "epoch": 0.07427792769925455,
      "grad_norm": 0.9369603991508484,
      "learning_rate": 0.00018519350010690614,
      "loss": 2.1361,
      "step": 1390
    },
    {
      "epoch": 0.07433136505731157,
      "grad_norm": 1.1687074899673462,
      "learning_rate": 0.00018518280949326492,
      "loss": 2.0223,
      "step": 1391
    },
    {
      "epoch": 0.07438480241536859,
      "grad_norm": 1.365147352218628,
      "learning_rate": 0.0001851721188796237,
      "loss": 2.0778,
      "step": 1392
    },
    {
      "epoch": 0.0744382397734256,
      "grad_norm": 1.1111750602722168,
      "learning_rate": 0.00018516142826598248,
      "loss": 2.0461,
      "step": 1393
    },
    {
      "epoch": 0.07449167713148262,
      "grad_norm": 1.251800298690796,
      "learning_rate": 0.00018515073765234126,
      "loss": 2.236,
      "step": 1394
    },
    {
      "epoch": 0.07454511448953964,
      "grad_norm": 0.9338720440864563,
      "learning_rate": 0.00018514004703870004,
      "loss": 2.0647,
      "step": 1395
    },
    {
      "epoch": 0.07459855184759666,
      "grad_norm": 0.9217129945755005,
      "learning_rate": 0.0001851293564250588,
      "loss": 2.0366,
      "step": 1396
    },
    {
      "epoch": 0.07465198920565368,
      "grad_norm": 1.5872493982315063,
      "learning_rate": 0.0001851186658114176,
      "loss": 2.1953,
      "step": 1397
    },
    {
      "epoch": 0.0747054265637107,
      "grad_norm": 0.9142429828643799,
      "learning_rate": 0.00018510797519777637,
      "loss": 2.1945,
      "step": 1398
    },
    {
      "epoch": 0.07475886392176771,
      "grad_norm": 1.6480177640914917,
      "learning_rate": 0.00018509728458413513,
      "loss": 2.0502,
      "step": 1399
    },
    {
      "epoch": 0.07481230127982473,
      "grad_norm": 1.1093789339065552,
      "learning_rate": 0.0001850865939704939,
      "loss": 2.1885,
      "step": 1400
    },
    {
      "epoch": 0.07486573863788175,
      "grad_norm": 0.8869802355766296,
      "learning_rate": 0.00018507590335685269,
      "loss": 2.2572,
      "step": 1401
    },
    {
      "epoch": 0.07491917599593877,
      "grad_norm": 1.1750528812408447,
      "learning_rate": 0.00018506521274321147,
      "loss": 2.2407,
      "step": 1402
    },
    {
      "epoch": 0.07497261335399578,
      "grad_norm": 1.1639519929885864,
      "learning_rate": 0.00018505452212957025,
      "loss": 2.3702,
      "step": 1403
    },
    {
      "epoch": 0.0750260507120528,
      "grad_norm": 1.8328074216842651,
      "learning_rate": 0.00018504383151592903,
      "loss": 2.4252,
      "step": 1404
    },
    {
      "epoch": 0.07507948807010982,
      "grad_norm": 1.2834749221801758,
      "learning_rate": 0.0001850331409022878,
      "loss": 2.2806,
      "step": 1405
    },
    {
      "epoch": 0.07513292542816682,
      "grad_norm": 1.1987396478652954,
      "learning_rate": 0.00018502245028864658,
      "loss": 2.1526,
      "step": 1406
    },
    {
      "epoch": 0.07518636278622384,
      "grad_norm": 1.4605019092559814,
      "learning_rate": 0.00018501175967500534,
      "loss": 2.1523,
      "step": 1407
    },
    {
      "epoch": 0.07523980014428086,
      "grad_norm": 1.0520721673965454,
      "learning_rate": 0.00018500106906136414,
      "loss": 2.2597,
      "step": 1408
    },
    {
      "epoch": 0.07529323750233788,
      "grad_norm": 1.3336281776428223,
      "learning_rate": 0.00018499037844772292,
      "loss": 2.3262,
      "step": 1409
    },
    {
      "epoch": 0.0753466748603949,
      "grad_norm": 1.2520201206207275,
      "learning_rate": 0.00018497968783408168,
      "loss": 1.9806,
      "step": 1410
    },
    {
      "epoch": 0.07540011221845191,
      "grad_norm": 1.2229039669036865,
      "learning_rate": 0.00018496899722044046,
      "loss": 2.07,
      "step": 1411
    },
    {
      "epoch": 0.07545354957650893,
      "grad_norm": 1.6235650777816772,
      "learning_rate": 0.00018495830660679924,
      "loss": 2.3858,
      "step": 1412
    },
    {
      "epoch": 0.07550698693456595,
      "grad_norm": 1.3199859857559204,
      "learning_rate": 0.00018494761599315801,
      "loss": 2.3442,
      "step": 1413
    },
    {
      "epoch": 0.07556042429262297,
      "grad_norm": 1.4772944450378418,
      "learning_rate": 0.0001849369253795168,
      "loss": 2.1186,
      "step": 1414
    },
    {
      "epoch": 0.07561386165067999,
      "grad_norm": 1.1633269786834717,
      "learning_rate": 0.00018492623476587557,
      "loss": 2.2269,
      "step": 1415
    },
    {
      "epoch": 0.075667299008737,
      "grad_norm": 1.3076108694076538,
      "learning_rate": 0.00018491554415223435,
      "loss": 2.2839,
      "step": 1416
    },
    {
      "epoch": 0.07572073636679402,
      "grad_norm": 0.9179841876029968,
      "learning_rate": 0.00018490485353859313,
      "loss": 2.0933,
      "step": 1417
    },
    {
      "epoch": 0.07577417372485104,
      "grad_norm": 1.0348197221755981,
      "learning_rate": 0.00018489416292495189,
      "loss": 2.2783,
      "step": 1418
    },
    {
      "epoch": 0.07582761108290806,
      "grad_norm": 1.2609754800796509,
      "learning_rate": 0.00018488347231131067,
      "loss": 2.3651,
      "step": 1419
    },
    {
      "epoch": 0.07588104844096508,
      "grad_norm": 1.277836561203003,
      "learning_rate": 0.00018487278169766947,
      "loss": 2.2564,
      "step": 1420
    },
    {
      "epoch": 0.0759344857990221,
      "grad_norm": 0.8754411935806274,
      "learning_rate": 0.00018486209108402823,
      "loss": 2.1607,
      "step": 1421
    },
    {
      "epoch": 0.07598792315707911,
      "grad_norm": 0.7011754512786865,
      "learning_rate": 0.000184851400470387,
      "loss": 2.0608,
      "step": 1422
    },
    {
      "epoch": 0.07604136051513613,
      "grad_norm": 1.1323899030685425,
      "learning_rate": 0.00018484070985674578,
      "loss": 2.0695,
      "step": 1423
    },
    {
      "epoch": 0.07609479787319315,
      "grad_norm": 1.3745743036270142,
      "learning_rate": 0.00018483001924310456,
      "loss": 2.3002,
      "step": 1424
    },
    {
      "epoch": 0.07614823523125017,
      "grad_norm": 1.585746169090271,
      "learning_rate": 0.00018481932862946334,
      "loss": 2.0703,
      "step": 1425
    },
    {
      "epoch": 0.07620167258930718,
      "grad_norm": 1.622780442237854,
      "learning_rate": 0.00018480863801582212,
      "loss": 2.4258,
      "step": 1426
    },
    {
      "epoch": 0.0762551099473642,
      "grad_norm": 1.6825134754180908,
      "learning_rate": 0.0001847979474021809,
      "loss": 2.4037,
      "step": 1427
    },
    {
      "epoch": 0.07630854730542122,
      "grad_norm": 1.3495222330093384,
      "learning_rate": 0.00018478725678853968,
      "loss": 2.1234,
      "step": 1428
    },
    {
      "epoch": 0.07636198466347824,
      "grad_norm": 1.3591076135635376,
      "learning_rate": 0.00018477656617489844,
      "loss": 2.1142,
      "step": 1429
    },
    {
      "epoch": 0.07641542202153526,
      "grad_norm": 1.240540862083435,
      "learning_rate": 0.00018476587556125721,
      "loss": 2.4029,
      "step": 1430
    },
    {
      "epoch": 0.07646885937959227,
      "grad_norm": 1.1039568185806274,
      "learning_rate": 0.00018475518494761602,
      "loss": 2.2397,
      "step": 1431
    },
    {
      "epoch": 0.07652229673764929,
      "grad_norm": 1.373592495918274,
      "learning_rate": 0.00018474449433397477,
      "loss": 2.338,
      "step": 1432
    },
    {
      "epoch": 0.07657573409570631,
      "grad_norm": 1.2579948902130127,
      "learning_rate": 0.00018473380372033355,
      "loss": 2.4083,
      "step": 1433
    },
    {
      "epoch": 0.07662917145376333,
      "grad_norm": 1.465743899345398,
      "learning_rate": 0.00018472311310669233,
      "loss": 2.3162,
      "step": 1434
    },
    {
      "epoch": 0.07668260881182035,
      "grad_norm": 2.0150601863861084,
      "learning_rate": 0.0001847124224930511,
      "loss": 2.4354,
      "step": 1435
    },
    {
      "epoch": 0.07673604616987736,
      "grad_norm": 1.7430120706558228,
      "learning_rate": 0.0001847017318794099,
      "loss": 2.4819,
      "step": 1436
    },
    {
      "epoch": 0.07678948352793438,
      "grad_norm": 1.1265990734100342,
      "learning_rate": 0.00018469104126576867,
      "loss": 2.1821,
      "step": 1437
    },
    {
      "epoch": 0.0768429208859914,
      "grad_norm": 1.7148066759109497,
      "learning_rate": 0.00018468035065212742,
      "loss": 2.3346,
      "step": 1438
    },
    {
      "epoch": 0.07689635824404842,
      "grad_norm": 1.4707157611846924,
      "learning_rate": 0.00018466966003848623,
      "loss": 2.1683,
      "step": 1439
    },
    {
      "epoch": 0.07694979560210544,
      "grad_norm": 1.8918126821517944,
      "learning_rate": 0.00018465896942484498,
      "loss": 2.2599,
      "step": 1440
    },
    {
      "epoch": 0.07700323296016245,
      "grad_norm": 1.3402528762817383,
      "learning_rate": 0.00018464827881120376,
      "loss": 2.2554,
      "step": 1441
    },
    {
      "epoch": 0.07705667031821947,
      "grad_norm": 1.3678282499313354,
      "learning_rate": 0.00018463758819756254,
      "loss": 2.2806,
      "step": 1442
    },
    {
      "epoch": 0.07711010767627649,
      "grad_norm": 1.0010387897491455,
      "learning_rate": 0.00018462689758392132,
      "loss": 2.0967,
      "step": 1443
    },
    {
      "epoch": 0.07716354503433351,
      "grad_norm": 1.0295865535736084,
      "learning_rate": 0.0001846162069702801,
      "loss": 2.0847,
      "step": 1444
    },
    {
      "epoch": 0.07721698239239053,
      "grad_norm": 1.16831636428833,
      "learning_rate": 0.00018460551635663888,
      "loss": 2.1549,
      "step": 1445
    },
    {
      "epoch": 0.07727041975044754,
      "grad_norm": 1.3778198957443237,
      "learning_rate": 0.00018459482574299766,
      "loss": 2.2465,
      "step": 1446
    },
    {
      "epoch": 0.07732385710850456,
      "grad_norm": 1.254870057106018,
      "learning_rate": 0.00018458413512935644,
      "loss": 2.1492,
      "step": 1447
    },
    {
      "epoch": 0.07737729446656157,
      "grad_norm": 1.3300044536590576,
      "learning_rate": 0.00018457344451571522,
      "loss": 2.5017,
      "step": 1448
    },
    {
      "epoch": 0.07743073182461858,
      "grad_norm": 0.9782474040985107,
      "learning_rate": 0.00018456275390207397,
      "loss": 2.2438,
      "step": 1449
    },
    {
      "epoch": 0.0774841691826756,
      "grad_norm": 1.063170075416565,
      "learning_rate": 0.00018455206328843278,
      "loss": 2.118,
      "step": 1450
    },
    {
      "epoch": 0.07753760654073262,
      "grad_norm": 1.466935396194458,
      "learning_rate": 0.00018454137267479153,
      "loss": 2.1839,
      "step": 1451
    },
    {
      "epoch": 0.07759104389878964,
      "grad_norm": 1.3866820335388184,
      "learning_rate": 0.0001845306820611503,
      "loss": 1.9989,
      "step": 1452
    },
    {
      "epoch": 0.07764448125684666,
      "grad_norm": 0.9486314058303833,
      "learning_rate": 0.0001845199914475091,
      "loss": 2.1639,
      "step": 1453
    },
    {
      "epoch": 0.07769791861490367,
      "grad_norm": 1.0430450439453125,
      "learning_rate": 0.00018450930083386787,
      "loss": 2.1047,
      "step": 1454
    },
    {
      "epoch": 0.07775135597296069,
      "grad_norm": 1.4159191846847534,
      "learning_rate": 0.00018449861022022665,
      "loss": 2.2319,
      "step": 1455
    },
    {
      "epoch": 0.07780479333101771,
      "grad_norm": 1.5884158611297607,
      "learning_rate": 0.00018448791960658543,
      "loss": 2.315,
      "step": 1456
    },
    {
      "epoch": 0.07785823068907473,
      "grad_norm": 1.2802202701568604,
      "learning_rate": 0.00018447722899294418,
      "loss": 2.2823,
      "step": 1457
    },
    {
      "epoch": 0.07791166804713175,
      "grad_norm": 1.3481462001800537,
      "learning_rate": 0.000184466538379303,
      "loss": 2.2961,
      "step": 1458
    },
    {
      "epoch": 0.07796510540518876,
      "grad_norm": 1.454977035522461,
      "learning_rate": 0.00018445584776566177,
      "loss": 2.1577,
      "step": 1459
    },
    {
      "epoch": 0.07801854276324578,
      "grad_norm": 1.4093549251556396,
      "learning_rate": 0.00018444515715202052,
      "loss": 2.0784,
      "step": 1460
    },
    {
      "epoch": 0.0780719801213028,
      "grad_norm": 1.465936303138733,
      "learning_rate": 0.0001844344665383793,
      "loss": 2.4659,
      "step": 1461
    },
    {
      "epoch": 0.07812541747935982,
      "grad_norm": 1.2005161046981812,
      "learning_rate": 0.0001844237759247381,
      "loss": 2.0547,
      "step": 1462
    },
    {
      "epoch": 0.07817885483741684,
      "grad_norm": 1.3010168075561523,
      "learning_rate": 0.00018441308531109686,
      "loss": 2.3018,
      "step": 1463
    },
    {
      "epoch": 0.07823229219547385,
      "grad_norm": 1.1562459468841553,
      "learning_rate": 0.00018440239469745564,
      "loss": 2.1371,
      "step": 1464
    },
    {
      "epoch": 0.07828572955353087,
      "grad_norm": 1.3355119228363037,
      "learning_rate": 0.00018439170408381442,
      "loss": 2.1213,
      "step": 1465
    },
    {
      "epoch": 0.07833916691158789,
      "grad_norm": 0.939846396446228,
      "learning_rate": 0.0001843810134701732,
      "loss": 2.1221,
      "step": 1466
    },
    {
      "epoch": 0.07839260426964491,
      "grad_norm": 1.2768737077713013,
      "learning_rate": 0.00018437032285653198,
      "loss": 2.1253,
      "step": 1467
    },
    {
      "epoch": 0.07844604162770193,
      "grad_norm": 1.5253170728683472,
      "learning_rate": 0.00018435963224289073,
      "loss": 2.4486,
      "step": 1468
    },
    {
      "epoch": 0.07849947898575894,
      "grad_norm": 1.1231833696365356,
      "learning_rate": 0.00018434894162924954,
      "loss": 2.2114,
      "step": 1469
    },
    {
      "epoch": 0.07855291634381596,
      "grad_norm": 1.310603141784668,
      "learning_rate": 0.00018433825101560832,
      "loss": 1.9915,
      "step": 1470
    },
    {
      "epoch": 0.07860635370187298,
      "grad_norm": 1.4520127773284912,
      "learning_rate": 0.00018432756040196707,
      "loss": 2.1875,
      "step": 1471
    },
    {
      "epoch": 0.07865979105993,
      "grad_norm": 1.498261570930481,
      "learning_rate": 0.00018431686978832585,
      "loss": 2.2978,
      "step": 1472
    },
    {
      "epoch": 0.07871322841798702,
      "grad_norm": 1.5616196393966675,
      "learning_rate": 0.00018430617917468466,
      "loss": 2.0213,
      "step": 1473
    },
    {
      "epoch": 0.07876666577604403,
      "grad_norm": 1.054109811782837,
      "learning_rate": 0.0001842954885610434,
      "loss": 2.0964,
      "step": 1474
    },
    {
      "epoch": 0.07882010313410105,
      "grad_norm": 1.3977645635604858,
      "learning_rate": 0.0001842847979474022,
      "loss": 2.2203,
      "step": 1475
    },
    {
      "epoch": 0.07887354049215807,
      "grad_norm": 1.4963586330413818,
      "learning_rate": 0.00018427410733376097,
      "loss": 2.1514,
      "step": 1476
    },
    {
      "epoch": 0.07892697785021509,
      "grad_norm": 1.4857877492904663,
      "learning_rate": 0.00018426341672011975,
      "loss": 1.9692,
      "step": 1477
    },
    {
      "epoch": 0.0789804152082721,
      "grad_norm": 1.5081250667572021,
      "learning_rate": 0.00018425272610647853,
      "loss": 2.2692,
      "step": 1478
    },
    {
      "epoch": 0.07903385256632912,
      "grad_norm": 0.8412270545959473,
      "learning_rate": 0.00018424203549283728,
      "loss": 1.929,
      "step": 1479
    },
    {
      "epoch": 0.07908728992438614,
      "grad_norm": 1.6573902368545532,
      "learning_rate": 0.00018423134487919606,
      "loss": 2.2576,
      "step": 1480
    },
    {
      "epoch": 0.07914072728244316,
      "grad_norm": 1.3366477489471436,
      "learning_rate": 0.00018422065426555487,
      "loss": 2.2195,
      "step": 1481
    },
    {
      "epoch": 0.07919416464050018,
      "grad_norm": 1.2992316484451294,
      "learning_rate": 0.00018420996365191362,
      "loss": 2.3169,
      "step": 1482
    },
    {
      "epoch": 0.0792476019985572,
      "grad_norm": 1.4961371421813965,
      "learning_rate": 0.0001841992730382724,
      "loss": 2.3425,
      "step": 1483
    },
    {
      "epoch": 0.07930103935661421,
      "grad_norm": 1.1146414279937744,
      "learning_rate": 0.00018418858242463118,
      "loss": 2.3138,
      "step": 1484
    },
    {
      "epoch": 0.07935447671467123,
      "grad_norm": 1.2617188692092896,
      "learning_rate": 0.00018417789181098996,
      "loss": 2.0872,
      "step": 1485
    },
    {
      "epoch": 0.07940791407272825,
      "grad_norm": 1.2330149412155151,
      "learning_rate": 0.00018416720119734874,
      "loss": 1.8688,
      "step": 1486
    },
    {
      "epoch": 0.07946135143078527,
      "grad_norm": 1.3062851428985596,
      "learning_rate": 0.00018415651058370752,
      "loss": 2.3392,
      "step": 1487
    },
    {
      "epoch": 0.07951478878884229,
      "grad_norm": 1.2138588428497314,
      "learning_rate": 0.00018414581997006627,
      "loss": 2.0969,
      "step": 1488
    },
    {
      "epoch": 0.0795682261468993,
      "grad_norm": 1.0242164134979248,
      "learning_rate": 0.00018413512935642508,
      "loss": 2.1721,
      "step": 1489
    },
    {
      "epoch": 0.07962166350495631,
      "grad_norm": 0.8858180642127991,
      "learning_rate": 0.00018412443874278386,
      "loss": 2.0284,
      "step": 1490
    },
    {
      "epoch": 0.07967510086301333,
      "grad_norm": 1.6179516315460205,
      "learning_rate": 0.0001841137481291426,
      "loss": 2.2084,
      "step": 1491
    },
    {
      "epoch": 0.07972853822107034,
      "grad_norm": 0.9881219863891602,
      "learning_rate": 0.00018410305751550142,
      "loss": 2.097,
      "step": 1492
    },
    {
      "epoch": 0.07978197557912736,
      "grad_norm": 1.0262027978897095,
      "learning_rate": 0.00018409236690186017,
      "loss": 2.1512,
      "step": 1493
    },
    {
      "epoch": 0.07983541293718438,
      "grad_norm": 1.3271629810333252,
      "learning_rate": 0.00018408167628821895,
      "loss": 2.0817,
      "step": 1494
    },
    {
      "epoch": 0.0798888502952414,
      "grad_norm": 1.5143940448760986,
      "learning_rate": 0.00018407098567457773,
      "loss": 2.1663,
      "step": 1495
    },
    {
      "epoch": 0.07994228765329842,
      "grad_norm": 1.4426125288009644,
      "learning_rate": 0.0001840602950609365,
      "loss": 2.3454,
      "step": 1496
    },
    {
      "epoch": 0.07999572501135543,
      "grad_norm": 1.7929176092147827,
      "learning_rate": 0.0001840496044472953,
      "loss": 2.2702,
      "step": 1497
    },
    {
      "epoch": 0.08004916236941245,
      "grad_norm": 0.8975403904914856,
      "learning_rate": 0.00018403891383365407,
      "loss": 2.0152,
      "step": 1498
    },
    {
      "epoch": 0.08010259972746947,
      "grad_norm": 1.4510252475738525,
      "learning_rate": 0.00018402822322001282,
      "loss": 2.2469,
      "step": 1499
    },
    {
      "epoch": 0.08015603708552649,
      "grad_norm": 1.3406847715377808,
      "learning_rate": 0.00018401753260637163,
      "loss": 2.0988,
      "step": 1500
    },
    {
      "epoch": 0.0802094744435835,
      "grad_norm": 1.3701486587524414,
      "learning_rate": 0.0001840068419927304,
      "loss": 2.0518,
      "step": 1501
    },
    {
      "epoch": 0.08026291180164052,
      "grad_norm": 1.4251466989517212,
      "learning_rate": 0.00018399615137908916,
      "loss": 2.3303,
      "step": 1502
    },
    {
      "epoch": 0.08031634915969754,
      "grad_norm": 1.1117993593215942,
      "learning_rate": 0.00018398546076544794,
      "loss": 2.1596,
      "step": 1503
    },
    {
      "epoch": 0.08036978651775456,
      "grad_norm": 1.2926753759384155,
      "learning_rate": 0.00018397477015180672,
      "loss": 2.0318,
      "step": 1504
    },
    {
      "epoch": 0.08042322387581158,
      "grad_norm": 1.2225011587142944,
      "learning_rate": 0.0001839640795381655,
      "loss": 2.1074,
      "step": 1505
    },
    {
      "epoch": 0.0804766612338686,
      "grad_norm": 1.0560258626937866,
      "learning_rate": 0.00018395338892452428,
      "loss": 2.1056,
      "step": 1506
    },
    {
      "epoch": 0.08053009859192561,
      "grad_norm": 1.3010554313659668,
      "learning_rate": 0.00018394269831088303,
      "loss": 2.3501,
      "step": 1507
    },
    {
      "epoch": 0.08058353594998263,
      "grad_norm": 1.3635542392730713,
      "learning_rate": 0.00018393200769724184,
      "loss": 2.1228,
      "step": 1508
    },
    {
      "epoch": 0.08063697330803965,
      "grad_norm": 0.778488039970398,
      "learning_rate": 0.00018392131708360062,
      "loss": 1.9458,
      "step": 1509
    },
    {
      "epoch": 0.08069041066609667,
      "grad_norm": 1.216748595237732,
      "learning_rate": 0.00018391062646995937,
      "loss": 2.3015,
      "step": 1510
    },
    {
      "epoch": 0.08074384802415369,
      "grad_norm": 1.1408543586730957,
      "learning_rate": 0.00018389993585631815,
      "loss": 2.0388,
      "step": 1511
    },
    {
      "epoch": 0.0807972853822107,
      "grad_norm": 1.2275961637496948,
      "learning_rate": 0.00018388924524267696,
      "loss": 2.1196,
      "step": 1512
    },
    {
      "epoch": 0.08085072274026772,
      "grad_norm": 1.3727152347564697,
      "learning_rate": 0.0001838785546290357,
      "loss": 2.1942,
      "step": 1513
    },
    {
      "epoch": 0.08090416009832474,
      "grad_norm": 1.2490538358688354,
      "learning_rate": 0.0001838678640153945,
      "loss": 2.3603,
      "step": 1514
    },
    {
      "epoch": 0.08095759745638176,
      "grad_norm": 1.1601736545562744,
      "learning_rate": 0.00018385717340175327,
      "loss": 2.0917,
      "step": 1515
    },
    {
      "epoch": 0.08101103481443878,
      "grad_norm": 1.2334096431732178,
      "learning_rate": 0.00018384648278811205,
      "loss": 2.1974,
      "step": 1516
    },
    {
      "epoch": 0.0810644721724958,
      "grad_norm": 1.372328519821167,
      "learning_rate": 0.00018383579217447083,
      "loss": 2.1403,
      "step": 1517
    },
    {
      "epoch": 0.08111790953055281,
      "grad_norm": 1.0253887176513672,
      "learning_rate": 0.00018382510156082958,
      "loss": 2.1392,
      "step": 1518
    },
    {
      "epoch": 0.08117134688860983,
      "grad_norm": 1.3904759883880615,
      "learning_rate": 0.0001838144109471884,
      "loss": 2.1736,
      "step": 1519
    },
    {
      "epoch": 0.08122478424666685,
      "grad_norm": 1.217583179473877,
      "learning_rate": 0.00018380372033354717,
      "loss": 2.0523,
      "step": 1520
    },
    {
      "epoch": 0.08127822160472387,
      "grad_norm": 1.4430617094039917,
      "learning_rate": 0.00018379302971990592,
      "loss": 2.2516,
      "step": 1521
    },
    {
      "epoch": 0.08133165896278088,
      "grad_norm": 1.2746784687042236,
      "learning_rate": 0.0001837823391062647,
      "loss": 2.1513,
      "step": 1522
    },
    {
      "epoch": 0.0813850963208379,
      "grad_norm": 0.9644296765327454,
      "learning_rate": 0.0001837716484926235,
      "loss": 2.003,
      "step": 1523
    },
    {
      "epoch": 0.08143853367889492,
      "grad_norm": 1.2799776792526245,
      "learning_rate": 0.00018376095787898226,
      "loss": 2.0654,
      "step": 1524
    },
    {
      "epoch": 0.08149197103695194,
      "grad_norm": 1.4576033353805542,
      "learning_rate": 0.00018375026726534104,
      "loss": 2.262,
      "step": 1525
    },
    {
      "epoch": 0.08154540839500896,
      "grad_norm": 1.579733967781067,
      "learning_rate": 0.00018373957665169982,
      "loss": 2.3116,
      "step": 1526
    },
    {
      "epoch": 0.08159884575306597,
      "grad_norm": 1.2683826684951782,
      "learning_rate": 0.0001837288860380586,
      "loss": 2.2245,
      "step": 1527
    },
    {
      "epoch": 0.08165228311112299,
      "grad_norm": 1.2856266498565674,
      "learning_rate": 0.00018371819542441738,
      "loss": 2.0684,
      "step": 1528
    },
    {
      "epoch": 0.08170572046918001,
      "grad_norm": 1.4126310348510742,
      "learning_rate": 0.00018370750481077616,
      "loss": 2.279,
      "step": 1529
    },
    {
      "epoch": 0.08175915782723703,
      "grad_norm": 2.2512080669403076,
      "learning_rate": 0.0001836968141971349,
      "loss": 2.4603,
      "step": 1530
    },
    {
      "epoch": 0.08181259518529405,
      "grad_norm": 1.329189658164978,
      "learning_rate": 0.00018368612358349372,
      "loss": 2.0972,
      "step": 1531
    },
    {
      "epoch": 0.08186603254335105,
      "grad_norm": 1.59017014503479,
      "learning_rate": 0.00018367543296985247,
      "loss": 2.1077,
      "step": 1532
    },
    {
      "epoch": 0.08191946990140807,
      "grad_norm": 1.4865577220916748,
      "learning_rate": 0.00018366474235621125,
      "loss": 2.2885,
      "step": 1533
    },
    {
      "epoch": 0.08197290725946509,
      "grad_norm": 2.0070323944091797,
      "learning_rate": 0.00018365405174257003,
      "loss": 2.2349,
      "step": 1534
    },
    {
      "epoch": 0.0820263446175221,
      "grad_norm": 1.0361169576644897,
      "learning_rate": 0.0001836433611289288,
      "loss": 2.0386,
      "step": 1535
    },
    {
      "epoch": 0.08207978197557912,
      "grad_norm": 1.4304592609405518,
      "learning_rate": 0.0001836326705152876,
      "loss": 2.2403,
      "step": 1536
    },
    {
      "epoch": 0.08213321933363614,
      "grad_norm": 1.1641919612884521,
      "learning_rate": 0.00018362197990164637,
      "loss": 2.2451,
      "step": 1537
    },
    {
      "epoch": 0.08218665669169316,
      "grad_norm": 1.3209491968154907,
      "learning_rate": 0.00018361128928800515,
      "loss": 2.2431,
      "step": 1538
    },
    {
      "epoch": 0.08224009404975018,
      "grad_norm": 0.91121906042099,
      "learning_rate": 0.00018360059867436393,
      "loss": 1.9598,
      "step": 1539
    },
    {
      "epoch": 0.0822935314078072,
      "grad_norm": 1.8113468885421753,
      "learning_rate": 0.0001835899080607227,
      "loss": 2.3394,
      "step": 1540
    },
    {
      "epoch": 0.08234696876586421,
      "grad_norm": 1.075783371925354,
      "learning_rate": 0.00018357921744708146,
      "loss": 2.1745,
      "step": 1541
    },
    {
      "epoch": 0.08240040612392123,
      "grad_norm": 1.3553552627563477,
      "learning_rate": 0.00018356852683344026,
      "loss": 2.2567,
      "step": 1542
    },
    {
      "epoch": 0.08245384348197825,
      "grad_norm": 1.0561113357543945,
      "learning_rate": 0.00018355783621979902,
      "loss": 2.0057,
      "step": 1543
    },
    {
      "epoch": 0.08250728084003527,
      "grad_norm": 1.3202319145202637,
      "learning_rate": 0.0001835471456061578,
      "loss": 2.1125,
      "step": 1544
    },
    {
      "epoch": 0.08256071819809228,
      "grad_norm": 1.1317745447158813,
      "learning_rate": 0.00018353645499251658,
      "loss": 2.1296,
      "step": 1545
    },
    {
      "epoch": 0.0826141555561493,
      "grad_norm": 1.291662335395813,
      "learning_rate": 0.00018352576437887536,
      "loss": 2.4137,
      "step": 1546
    },
    {
      "epoch": 0.08266759291420632,
      "grad_norm": 0.9140318036079407,
      "learning_rate": 0.00018351507376523414,
      "loss": 2.1162,
      "step": 1547
    },
    {
      "epoch": 0.08272103027226334,
      "grad_norm": 0.7501223087310791,
      "learning_rate": 0.00018350438315159292,
      "loss": 2.0398,
      "step": 1548
    },
    {
      "epoch": 0.08277446763032036,
      "grad_norm": 1.2890652418136597,
      "learning_rate": 0.00018349369253795167,
      "loss": 2.3026,
      "step": 1549
    },
    {
      "epoch": 0.08282790498837737,
      "grad_norm": 1.7471140623092651,
      "learning_rate": 0.00018348300192431047,
      "loss": 2.2569,
      "step": 1550
    },
    {
      "epoch": 0.08288134234643439,
      "grad_norm": 2.0723679065704346,
      "learning_rate": 0.00018347231131066925,
      "loss": 2.609,
      "step": 1551
    },
    {
      "epoch": 0.08293477970449141,
      "grad_norm": 1.5498666763305664,
      "learning_rate": 0.000183461620697028,
      "loss": 2.3871,
      "step": 1552
    },
    {
      "epoch": 0.08298821706254843,
      "grad_norm": 1.6101758480072021,
      "learning_rate": 0.00018345093008338679,
      "loss": 2.0012,
      "step": 1553
    },
    {
      "epoch": 0.08304165442060545,
      "grad_norm": 1.5624338388442993,
      "learning_rate": 0.00018344023946974557,
      "loss": 2.206,
      "step": 1554
    },
    {
      "epoch": 0.08309509177866246,
      "grad_norm": 1.2193464040756226,
      "learning_rate": 0.00018342954885610435,
      "loss": 2.1426,
      "step": 1555
    },
    {
      "epoch": 0.08314852913671948,
      "grad_norm": 0.9931802153587341,
      "learning_rate": 0.00018341885824246313,
      "loss": 2.0151,
      "step": 1556
    },
    {
      "epoch": 0.0832019664947765,
      "grad_norm": 1.532541275024414,
      "learning_rate": 0.0001834081676288219,
      "loss": 2.2531,
      "step": 1557
    },
    {
      "epoch": 0.08325540385283352,
      "grad_norm": 0.9058617949485779,
      "learning_rate": 0.00018339747701518068,
      "loss": 2.0175,
      "step": 1558
    },
    {
      "epoch": 0.08330884121089054,
      "grad_norm": 1.3223707675933838,
      "learning_rate": 0.00018338678640153946,
      "loss": 2.1246,
      "step": 1559
    },
    {
      "epoch": 0.08336227856894755,
      "grad_norm": 1.0643826723098755,
      "learning_rate": 0.00018337609578789822,
      "loss": 2.0538,
      "step": 1560
    },
    {
      "epoch": 0.08341571592700457,
      "grad_norm": 1.3155478239059448,
      "learning_rate": 0.00018336540517425702,
      "loss": 2.1629,
      "step": 1561
    },
    {
      "epoch": 0.08346915328506159,
      "grad_norm": 1.3360587358474731,
      "learning_rate": 0.0001833547145606158,
      "loss": 2.1596,
      "step": 1562
    },
    {
      "epoch": 0.08352259064311861,
      "grad_norm": 1.1527718305587769,
      "learning_rate": 0.00018334402394697456,
      "loss": 2.3661,
      "step": 1563
    },
    {
      "epoch": 0.08357602800117563,
      "grad_norm": 1.5184227228164673,
      "learning_rate": 0.00018333333333333334,
      "loss": 2.3183,
      "step": 1564
    },
    {
      "epoch": 0.08362946535923264,
      "grad_norm": 1.1599175930023193,
      "learning_rate": 0.00018332264271969211,
      "loss": 2.1196,
      "step": 1565
    },
    {
      "epoch": 0.08368290271728966,
      "grad_norm": 0.9087843298912048,
      "learning_rate": 0.0001833119521060509,
      "loss": 2.0659,
      "step": 1566
    },
    {
      "epoch": 0.08373634007534668,
      "grad_norm": 1.7396881580352783,
      "learning_rate": 0.00018330126149240967,
      "loss": 2.1286,
      "step": 1567
    },
    {
      "epoch": 0.0837897774334037,
      "grad_norm": 1.1297377347946167,
      "learning_rate": 0.00018329057087876845,
      "loss": 2.2049,
      "step": 1568
    },
    {
      "epoch": 0.08384321479146072,
      "grad_norm": 1.7742371559143066,
      "learning_rate": 0.00018327988026512723,
      "loss": 2.3543,
      "step": 1569
    },
    {
      "epoch": 0.08389665214951773,
      "grad_norm": 1.327775478363037,
      "learning_rate": 0.000183269189651486,
      "loss": 2.1982,
      "step": 1570
    },
    {
      "epoch": 0.08395008950757475,
      "grad_norm": 1.0120902061462402,
      "learning_rate": 0.00018325849903784477,
      "loss": 2.2532,
      "step": 1571
    },
    {
      "epoch": 0.08400352686563177,
      "grad_norm": 1.5194319486618042,
      "learning_rate": 0.00018324780842420355,
      "loss": 2.0996,
      "step": 1572
    },
    {
      "epoch": 0.08405696422368879,
      "grad_norm": 0.9061247706413269,
      "learning_rate": 0.00018323711781056235,
      "loss": 2.1617,
      "step": 1573
    },
    {
      "epoch": 0.08411040158174579,
      "grad_norm": 1.3291462659835815,
      "learning_rate": 0.0001832264271969211,
      "loss": 2.2152,
      "step": 1574
    },
    {
      "epoch": 0.08416383893980281,
      "grad_norm": 1.3961360454559326,
      "learning_rate": 0.00018321573658327988,
      "loss": 2.3886,
      "step": 1575
    },
    {
      "epoch": 0.08421727629785983,
      "grad_norm": 1.3748916387557983,
      "learning_rate": 0.00018320504596963866,
      "loss": 2.1394,
      "step": 1576
    },
    {
      "epoch": 0.08427071365591685,
      "grad_norm": 1.4088431596755981,
      "learning_rate": 0.00018319435535599744,
      "loss": 2.2608,
      "step": 1577
    },
    {
      "epoch": 0.08432415101397386,
      "grad_norm": 1.4256943464279175,
      "learning_rate": 0.00018318366474235622,
      "loss": 2.2547,
      "step": 1578
    },
    {
      "epoch": 0.08437758837203088,
      "grad_norm": 1.0048905611038208,
      "learning_rate": 0.000183172974128715,
      "loss": 2.1395,
      "step": 1579
    },
    {
      "epoch": 0.0844310257300879,
      "grad_norm": 1.3617275953292847,
      "learning_rate": 0.00018316228351507378,
      "loss": 2.2416,
      "step": 1580
    },
    {
      "epoch": 0.08448446308814492,
      "grad_norm": 1.2250062227249146,
      "learning_rate": 0.00018315159290143256,
      "loss": 2.1819,
      "step": 1581
    },
    {
      "epoch": 0.08453790044620194,
      "grad_norm": 1.5230172872543335,
      "learning_rate": 0.00018314090228779131,
      "loss": 2.2893,
      "step": 1582
    },
    {
      "epoch": 0.08459133780425895,
      "grad_norm": 2.0512502193450928,
      "learning_rate": 0.0001831302116741501,
      "loss": 2.2373,
      "step": 1583
    },
    {
      "epoch": 0.08464477516231597,
      "grad_norm": 1.1596968173980713,
      "learning_rate": 0.0001831195210605089,
      "loss": 2.1392,
      "step": 1584
    },
    {
      "epoch": 0.08469821252037299,
      "grad_norm": 0.9278336763381958,
      "learning_rate": 0.00018310883044686765,
      "loss": 2.0375,
      "step": 1585
    },
    {
      "epoch": 0.08475164987843001,
      "grad_norm": 1.3756269216537476,
      "learning_rate": 0.00018309813983322643,
      "loss": 2.3362,
      "step": 1586
    },
    {
      "epoch": 0.08480508723648703,
      "grad_norm": 1.5040428638458252,
      "learning_rate": 0.0001830874492195852,
      "loss": 2.3627,
      "step": 1587
    },
    {
      "epoch": 0.08485852459454404,
      "grad_norm": 1.2925916910171509,
      "learning_rate": 0.000183076758605944,
      "loss": 2.1773,
      "step": 1588
    },
    {
      "epoch": 0.08491196195260106,
      "grad_norm": 1.137256145477295,
      "learning_rate": 0.00018306606799230277,
      "loss": 2.0268,
      "step": 1589
    },
    {
      "epoch": 0.08496539931065808,
      "grad_norm": 1.1938356161117554,
      "learning_rate": 0.00018305537737866155,
      "loss": 2.1775,
      "step": 1590
    },
    {
      "epoch": 0.0850188366687151,
      "grad_norm": 1.5905399322509766,
      "learning_rate": 0.0001830446867650203,
      "loss": 2.2357,
      "step": 1591
    },
    {
      "epoch": 0.08507227402677212,
      "grad_norm": 1.2304155826568604,
      "learning_rate": 0.0001830339961513791,
      "loss": 2.2299,
      "step": 1592
    },
    {
      "epoch": 0.08512571138482913,
      "grad_norm": 1.4583057165145874,
      "learning_rate": 0.00018302330553773786,
      "loss": 2.3,
      "step": 1593
    },
    {
      "epoch": 0.08517914874288615,
      "grad_norm": 1.4677232503890991,
      "learning_rate": 0.00018301261492409664,
      "loss": 2.3841,
      "step": 1594
    },
    {
      "epoch": 0.08523258610094317,
      "grad_norm": 1.5507456064224243,
      "learning_rate": 0.00018300192431045542,
      "loss": 2.1648,
      "step": 1595
    },
    {
      "epoch": 0.08528602345900019,
      "grad_norm": 1.5555388927459717,
      "learning_rate": 0.0001829912336968142,
      "loss": 2.2027,
      "step": 1596
    },
    {
      "epoch": 0.0853394608170572,
      "grad_norm": 1.5031704902648926,
      "learning_rate": 0.00018298054308317298,
      "loss": 2.2417,
      "step": 1597
    },
    {
      "epoch": 0.08539289817511422,
      "grad_norm": 0.8961129188537598,
      "learning_rate": 0.00018296985246953176,
      "loss": 2.243,
      "step": 1598
    },
    {
      "epoch": 0.08544633553317124,
      "grad_norm": 1.8183612823486328,
      "learning_rate": 0.00018295916185589051,
      "loss": 2.159,
      "step": 1599
    },
    {
      "epoch": 0.08549977289122826,
      "grad_norm": 1.5926131010055542,
      "learning_rate": 0.00018294847124224932,
      "loss": 2.2065,
      "step": 1600
    },
    {
      "epoch": 0.08555321024928528,
      "grad_norm": 0.9943414926528931,
      "learning_rate": 0.0001829377806286081,
      "loss": 2.0128,
      "step": 1601
    },
    {
      "epoch": 0.0856066476073423,
      "grad_norm": 1.0669629573822021,
      "learning_rate": 0.00018292709001496685,
      "loss": 2.1805,
      "step": 1602
    },
    {
      "epoch": 0.08566008496539931,
      "grad_norm": 1.6995919942855835,
      "learning_rate": 0.00018291639940132566,
      "loss": 2.1024,
      "step": 1603
    },
    {
      "epoch": 0.08571352232345633,
      "grad_norm": 1.7502658367156982,
      "learning_rate": 0.00018290570878768444,
      "loss": 2.2389,
      "step": 1604
    },
    {
      "epoch": 0.08576695968151335,
      "grad_norm": 1.4611715078353882,
      "learning_rate": 0.0001828950181740432,
      "loss": 2.1936,
      "step": 1605
    },
    {
      "epoch": 0.08582039703957037,
      "grad_norm": 1.2236714363098145,
      "learning_rate": 0.00018288432756040197,
      "loss": 2.2234,
      "step": 1606
    },
    {
      "epoch": 0.08587383439762739,
      "grad_norm": 1.5039136409759521,
      "learning_rate": 0.00018287363694676075,
      "loss": 2.3999,
      "step": 1607
    },
    {
      "epoch": 0.0859272717556844,
      "grad_norm": 1.7213101387023926,
      "learning_rate": 0.00018286294633311953,
      "loss": 2.2608,
      "step": 1608
    },
    {
      "epoch": 0.08598070911374142,
      "grad_norm": 1.1940624713897705,
      "learning_rate": 0.0001828522557194783,
      "loss": 2.1734,
      "step": 1609
    },
    {
      "epoch": 0.08603414647179844,
      "grad_norm": 0.9170020222663879,
      "learning_rate": 0.00018284156510583706,
      "loss": 2.1013,
      "step": 1610
    },
    {
      "epoch": 0.08608758382985546,
      "grad_norm": 1.1999350786209106,
      "learning_rate": 0.00018283087449219587,
      "loss": 2.1502,
      "step": 1611
    },
    {
      "epoch": 0.08614102118791248,
      "grad_norm": 1.476794719696045,
      "learning_rate": 0.00018282018387855465,
      "loss": 2.3819,
      "step": 1612
    },
    {
      "epoch": 0.0861944585459695,
      "grad_norm": 1.063352346420288,
      "learning_rate": 0.0001828094932649134,
      "loss": 2.1771,
      "step": 1613
    },
    {
      "epoch": 0.08624789590402651,
      "grad_norm": 1.2844253778457642,
      "learning_rate": 0.00018279880265127218,
      "loss": 2.2149,
      "step": 1614
    },
    {
      "epoch": 0.08630133326208353,
      "grad_norm": 1.5335997343063354,
      "learning_rate": 0.000182788112037631,
      "loss": 2.4136,
      "step": 1615
    },
    {
      "epoch": 0.08635477062014053,
      "grad_norm": 1.4726852178573608,
      "learning_rate": 0.00018277742142398974,
      "loss": 2.3403,
      "step": 1616
    },
    {
      "epoch": 0.08640820797819755,
      "grad_norm": 1.2125253677368164,
      "learning_rate": 0.00018276673081034852,
      "loss": 2.0982,
      "step": 1617
    },
    {
      "epoch": 0.08646164533625457,
      "grad_norm": 1.3223581314086914,
      "learning_rate": 0.0001827560401967073,
      "loss": 2.0525,
      "step": 1618
    },
    {
      "epoch": 0.08651508269431159,
      "grad_norm": 1.4053363800048828,
      "learning_rate": 0.00018274534958306608,
      "loss": 2.2962,
      "step": 1619
    },
    {
      "epoch": 0.0865685200523686,
      "grad_norm": 1.1393781900405884,
      "learning_rate": 0.00018273465896942486,
      "loss": 1.8845,
      "step": 1620
    },
    {
      "epoch": 0.08662195741042562,
      "grad_norm": 0.9465835094451904,
      "learning_rate": 0.0001827239683557836,
      "loss": 1.9784,
      "step": 1621
    },
    {
      "epoch": 0.08667539476848264,
      "grad_norm": 1.197410225868225,
      "learning_rate": 0.0001827132777421424,
      "loss": 2.1804,
      "step": 1622
    },
    {
      "epoch": 0.08672883212653966,
      "grad_norm": 1.3355690240859985,
      "learning_rate": 0.0001827025871285012,
      "loss": 2.2404,
      "step": 1623
    },
    {
      "epoch": 0.08678226948459668,
      "grad_norm": 1.1410108804702759,
      "learning_rate": 0.00018269189651485995,
      "loss": 2.3034,
      "step": 1624
    },
    {
      "epoch": 0.0868357068426537,
      "grad_norm": 0.8731045126914978,
      "learning_rate": 0.00018268120590121873,
      "loss": 2.076,
      "step": 1625
    },
    {
      "epoch": 0.08688914420071071,
      "grad_norm": 1.415365219116211,
      "learning_rate": 0.00018267051528757754,
      "loss": 2.4189,
      "step": 1626
    },
    {
      "epoch": 0.08694258155876773,
      "grad_norm": 1.6676381826400757,
      "learning_rate": 0.0001826598246739363,
      "loss": 2.1066,
      "step": 1627
    },
    {
      "epoch": 0.08699601891682475,
      "grad_norm": 1.3261241912841797,
      "learning_rate": 0.00018264913406029507,
      "loss": 2.2474,
      "step": 1628
    },
    {
      "epoch": 0.08704945627488177,
      "grad_norm": 1.0507991313934326,
      "learning_rate": 0.00018263844344665385,
      "loss": 2.1632,
      "step": 1629
    },
    {
      "epoch": 0.08710289363293879,
      "grad_norm": 1.3128337860107422,
      "learning_rate": 0.00018262775283301263,
      "loss": 1.9323,
      "step": 1630
    },
    {
      "epoch": 0.0871563309909958,
      "grad_norm": 1.3934564590454102,
      "learning_rate": 0.0001826170622193714,
      "loss": 2.162,
      "step": 1631
    },
    {
      "epoch": 0.08720976834905282,
      "grad_norm": 1.1278486251831055,
      "learning_rate": 0.0001826063716057302,
      "loss": 2.1058,
      "step": 1632
    },
    {
      "epoch": 0.08726320570710984,
      "grad_norm": 1.681832194328308,
      "learning_rate": 0.00018259568099208894,
      "loss": 2.2676,
      "step": 1633
    },
    {
      "epoch": 0.08731664306516686,
      "grad_norm": 1.677276372909546,
      "learning_rate": 0.00018258499037844775,
      "loss": 2.3708,
      "step": 1634
    },
    {
      "epoch": 0.08737008042322388,
      "grad_norm": 1.5622279644012451,
      "learning_rate": 0.0001825742997648065,
      "loss": 2.382,
      "step": 1635
    },
    {
      "epoch": 0.0874235177812809,
      "grad_norm": 1.5450499057769775,
      "learning_rate": 0.00018256360915116528,
      "loss": 2.1227,
      "step": 1636
    },
    {
      "epoch": 0.08747695513933791,
      "grad_norm": 1.168044090270996,
      "learning_rate": 0.00018255291853752406,
      "loss": 2.264,
      "step": 1637
    },
    {
      "epoch": 0.08753039249739493,
      "grad_norm": 1.520545244216919,
      "learning_rate": 0.00018254222792388284,
      "loss": 2.25,
      "step": 1638
    },
    {
      "epoch": 0.08758382985545195,
      "grad_norm": 1.1913880109786987,
      "learning_rate": 0.00018253153731024162,
      "loss": 2.0057,
      "step": 1639
    },
    {
      "epoch": 0.08763726721350897,
      "grad_norm": 1.2376291751861572,
      "learning_rate": 0.0001825208466966004,
      "loss": 2.2029,
      "step": 1640
    },
    {
      "epoch": 0.08769070457156598,
      "grad_norm": 1.216123342514038,
      "learning_rate": 0.00018251015608295915,
      "loss": 2.3965,
      "step": 1641
    },
    {
      "epoch": 0.087744141929623,
      "grad_norm": 1.442359447479248,
      "learning_rate": 0.00018249946546931796,
      "loss": 2.1315,
      "step": 1642
    },
    {
      "epoch": 0.08779757928768002,
      "grad_norm": 1.3589937686920166,
      "learning_rate": 0.00018248877485567674,
      "loss": 2.2008,
      "step": 1643
    },
    {
      "epoch": 0.08785101664573704,
      "grad_norm": 1.0908119678497314,
      "learning_rate": 0.0001824780842420355,
      "loss": 2.0959,
      "step": 1644
    },
    {
      "epoch": 0.08790445400379406,
      "grad_norm": 1.2791900634765625,
      "learning_rate": 0.00018246739362839427,
      "loss": 2.224,
      "step": 1645
    },
    {
      "epoch": 0.08795789136185107,
      "grad_norm": 1.5373039245605469,
      "learning_rate": 0.00018245670301475305,
      "loss": 2.3486,
      "step": 1646
    },
    {
      "epoch": 0.08801132871990809,
      "grad_norm": 0.9406495094299316,
      "learning_rate": 0.00018244601240111183,
      "loss": 2.096,
      "step": 1647
    },
    {
      "epoch": 0.08806476607796511,
      "grad_norm": 1.191472053527832,
      "learning_rate": 0.0001824353217874706,
      "loss": 2.2447,
      "step": 1648
    },
    {
      "epoch": 0.08811820343602213,
      "grad_norm": 1.4035077095031738,
      "learning_rate": 0.0001824246311738294,
      "loss": 2.1158,
      "step": 1649
    },
    {
      "epoch": 0.08817164079407915,
      "grad_norm": 1.1370431184768677,
      "learning_rate": 0.00018241394056018817,
      "loss": 2.2741,
      "step": 1650
    },
    {
      "epoch": 0.08822507815213616,
      "grad_norm": 1.2512295246124268,
      "learning_rate": 0.00018240324994654695,
      "loss": 2.4266,
      "step": 1651
    },
    {
      "epoch": 0.08827851551019318,
      "grad_norm": 1.1475104093551636,
      "learning_rate": 0.0001823925593329057,
      "loss": 2.0416,
      "step": 1652
    },
    {
      "epoch": 0.0883319528682502,
      "grad_norm": 1.0980286598205566,
      "learning_rate": 0.0001823818687192645,
      "loss": 2.1777,
      "step": 1653
    },
    {
      "epoch": 0.08838539022630722,
      "grad_norm": 1.6567283868789673,
      "learning_rate": 0.0001823711781056233,
      "loss": 2.0928,
      "step": 1654
    },
    {
      "epoch": 0.08843882758436424,
      "grad_norm": 1.3025277853012085,
      "learning_rate": 0.00018236048749198204,
      "loss": 2.238,
      "step": 1655
    },
    {
      "epoch": 0.08849226494242125,
      "grad_norm": 1.1650618314743042,
      "learning_rate": 0.00018234979687834082,
      "loss": 2.0384,
      "step": 1656
    },
    {
      "epoch": 0.08854570230047827,
      "grad_norm": 1.500718593597412,
      "learning_rate": 0.0001823391062646996,
      "loss": 2.1065,
      "step": 1657
    },
    {
      "epoch": 0.08859913965853528,
      "grad_norm": 1.1285946369171143,
      "learning_rate": 0.00018232841565105838,
      "loss": 2.3064,
      "step": 1658
    },
    {
      "epoch": 0.0886525770165923,
      "grad_norm": 1.5087428092956543,
      "learning_rate": 0.00018231772503741716,
      "loss": 2.4535,
      "step": 1659
    },
    {
      "epoch": 0.08870601437464931,
      "grad_norm": 1.174681305885315,
      "learning_rate": 0.00018230703442377594,
      "loss": 2.2873,
      "step": 1660
    },
    {
      "epoch": 0.08875945173270633,
      "grad_norm": 1.4160847663879395,
      "learning_rate": 0.00018229634381013472,
      "loss": 2.1179,
      "step": 1661
    },
    {
      "epoch": 0.08881288909076335,
      "grad_norm": 1.1874767541885376,
      "learning_rate": 0.0001822856531964935,
      "loss": 2.0957,
      "step": 1662
    },
    {
      "epoch": 0.08886632644882037,
      "grad_norm": 1.2689629793167114,
      "learning_rate": 0.00018227496258285225,
      "loss": 2.1564,
      "step": 1663
    },
    {
      "epoch": 0.08891976380687738,
      "grad_norm": 0.9900625944137573,
      "learning_rate": 0.00018226427196921103,
      "loss": 1.9135,
      "step": 1664
    },
    {
      "epoch": 0.0889732011649344,
      "grad_norm": 1.4720427989959717,
      "learning_rate": 0.00018225358135556984,
      "loss": 2.4602,
      "step": 1665
    },
    {
      "epoch": 0.08902663852299142,
      "grad_norm": 1.9896831512451172,
      "learning_rate": 0.0001822428907419286,
      "loss": 2.5151,
      "step": 1666
    },
    {
      "epoch": 0.08908007588104844,
      "grad_norm": 1.1865360736846924,
      "learning_rate": 0.00018223220012828737,
      "loss": 2.0311,
      "step": 1667
    },
    {
      "epoch": 0.08913351323910546,
      "grad_norm": 1.3025373220443726,
      "learning_rate": 0.00018222150951464615,
      "loss": 2.1367,
      "step": 1668
    },
    {
      "epoch": 0.08918695059716247,
      "grad_norm": 0.9693952202796936,
      "learning_rate": 0.00018221081890100493,
      "loss": 2.1217,
      "step": 1669
    },
    {
      "epoch": 0.08924038795521949,
      "grad_norm": 1.2337591648101807,
      "learning_rate": 0.0001822001282873637,
      "loss": 2.0733,
      "step": 1670
    },
    {
      "epoch": 0.08929382531327651,
      "grad_norm": 1.0720549821853638,
      "learning_rate": 0.0001821894376737225,
      "loss": 2.0873,
      "step": 1671
    },
    {
      "epoch": 0.08934726267133353,
      "grad_norm": 1.2782342433929443,
      "learning_rate": 0.00018217874706008127,
      "loss": 2.1472,
      "step": 1672
    },
    {
      "epoch": 0.08940070002939055,
      "grad_norm": 1.1108696460723877,
      "learning_rate": 0.00018216805644644005,
      "loss": 2.0794,
      "step": 1673
    },
    {
      "epoch": 0.08945413738744756,
      "grad_norm": 1.5810143947601318,
      "learning_rate": 0.0001821573658327988,
      "loss": 2.2207,
      "step": 1674
    },
    {
      "epoch": 0.08950757474550458,
      "grad_norm": 1.0167828798294067,
      "learning_rate": 0.00018214667521915758,
      "loss": 2.1467,
      "step": 1675
    },
    {
      "epoch": 0.0895610121035616,
      "grad_norm": 0.9457551836967468,
      "learning_rate": 0.00018213598460551638,
      "loss": 2.0048,
      "step": 1676
    },
    {
      "epoch": 0.08961444946161862,
      "grad_norm": 1.113338589668274,
      "learning_rate": 0.00018212529399187514,
      "loss": 2.2095,
      "step": 1677
    },
    {
      "epoch": 0.08966788681967564,
      "grad_norm": 1.0850635766983032,
      "learning_rate": 0.00018211460337823392,
      "loss": 2.1223,
      "step": 1678
    },
    {
      "epoch": 0.08972132417773265,
      "grad_norm": 1.2379200458526611,
      "learning_rate": 0.0001821039127645927,
      "loss": 2.5158,
      "step": 1679
    },
    {
      "epoch": 0.08977476153578967,
      "grad_norm": 1.458617091178894,
      "learning_rate": 0.00018209322215095148,
      "loss": 2.2677,
      "step": 1680
    },
    {
      "epoch": 0.08982819889384669,
      "grad_norm": 0.9230892062187195,
      "learning_rate": 0.00018208253153731026,
      "loss": 2.0127,
      "step": 1681
    },
    {
      "epoch": 0.08988163625190371,
      "grad_norm": 1.0479352474212646,
      "learning_rate": 0.00018207184092366904,
      "loss": 2.0586,
      "step": 1682
    },
    {
      "epoch": 0.08993507360996073,
      "grad_norm": 1.1048927307128906,
      "learning_rate": 0.0001820611503100278,
      "loss": 2.136,
      "step": 1683
    },
    {
      "epoch": 0.08998851096801774,
      "grad_norm": 1.8000091314315796,
      "learning_rate": 0.0001820504596963866,
      "loss": 2.2773,
      "step": 1684
    },
    {
      "epoch": 0.09004194832607476,
      "grad_norm": 1.208949327468872,
      "learning_rate": 0.00018203976908274535,
      "loss": 2.1149,
      "step": 1685
    },
    {
      "epoch": 0.09009538568413178,
      "grad_norm": 1.861444115638733,
      "learning_rate": 0.00018202907846910413,
      "loss": 2.6129,
      "step": 1686
    },
    {
      "epoch": 0.0901488230421888,
      "grad_norm": 1.0523762702941895,
      "learning_rate": 0.0001820183878554629,
      "loss": 2.0593,
      "step": 1687
    },
    {
      "epoch": 0.09020226040024582,
      "grad_norm": 1.4435577392578125,
      "learning_rate": 0.0001820076972418217,
      "loss": 2.3589,
      "step": 1688
    },
    {
      "epoch": 0.09025569775830283,
      "grad_norm": 1.370453953742981,
      "learning_rate": 0.00018199700662818047,
      "loss": 2.1882,
      "step": 1689
    },
    {
      "epoch": 0.09030913511635985,
      "grad_norm": 1.3438644409179688,
      "learning_rate": 0.00018198631601453925,
      "loss": 2.2542,
      "step": 1690
    },
    {
      "epoch": 0.09036257247441687,
      "grad_norm": 1.4520528316497803,
      "learning_rate": 0.00018197562540089803,
      "loss": 2.0966,
      "step": 1691
    },
    {
      "epoch": 0.09041600983247389,
      "grad_norm": 1.0669997930526733,
      "learning_rate": 0.0001819649347872568,
      "loss": 2.0707,
      "step": 1692
    },
    {
      "epoch": 0.0904694471905309,
      "grad_norm": 1.258993148803711,
      "learning_rate": 0.00018195424417361558,
      "loss": 2.2238,
      "step": 1693
    },
    {
      "epoch": 0.09052288454858792,
      "grad_norm": 1.3801339864730835,
      "learning_rate": 0.00018194355355997434,
      "loss": 2.1936,
      "step": 1694
    },
    {
      "epoch": 0.09057632190664494,
      "grad_norm": 1.6571396589279175,
      "learning_rate": 0.00018193286294633314,
      "loss": 2.095,
      "step": 1695
    },
    {
      "epoch": 0.09062975926470196,
      "grad_norm": 1.120945930480957,
      "learning_rate": 0.0001819221723326919,
      "loss": 2.2316,
      "step": 1696
    },
    {
      "epoch": 0.09068319662275898,
      "grad_norm": 0.9734783172607422,
      "learning_rate": 0.00018191148171905068,
      "loss": 2.0566,
      "step": 1697
    },
    {
      "epoch": 0.090736633980816,
      "grad_norm": 1.279892086982727,
      "learning_rate": 0.00018190079110540946,
      "loss": 2.1361,
      "step": 1698
    },
    {
      "epoch": 0.09079007133887301,
      "grad_norm": 0.9189510345458984,
      "learning_rate": 0.00018189010049176824,
      "loss": 1.9876,
      "step": 1699
    },
    {
      "epoch": 0.09084350869693002,
      "grad_norm": 1.2605592012405396,
      "learning_rate": 0.00018187940987812702,
      "loss": 2.0586,
      "step": 1700
    },
    {
      "epoch": 0.09089694605498704,
      "grad_norm": 0.9911908507347107,
      "learning_rate": 0.0001818687192644858,
      "loss": 1.9777,
      "step": 1701
    },
    {
      "epoch": 0.09095038341304405,
      "grad_norm": 1.7956347465515137,
      "learning_rate": 0.00018185802865084455,
      "loss": 2.2182,
      "step": 1702
    },
    {
      "epoch": 0.09100382077110107,
      "grad_norm": 1.0654046535491943,
      "learning_rate": 0.00018184733803720335,
      "loss": 1.987,
      "step": 1703
    },
    {
      "epoch": 0.09105725812915809,
      "grad_norm": 1.1089242696762085,
      "learning_rate": 0.00018183664742356213,
      "loss": 1.9839,
      "step": 1704
    },
    {
      "epoch": 0.09111069548721511,
      "grad_norm": 1.4975217580795288,
      "learning_rate": 0.00018182595680992089,
      "loss": 2.1397,
      "step": 1705
    },
    {
      "epoch": 0.09116413284527213,
      "grad_norm": 1.0744961500167847,
      "learning_rate": 0.00018181526619627967,
      "loss": 2.2232,
      "step": 1706
    },
    {
      "epoch": 0.09121757020332914,
      "grad_norm": 1.492066502571106,
      "learning_rate": 0.00018180457558263845,
      "loss": 2.2487,
      "step": 1707
    },
    {
      "epoch": 0.09127100756138616,
      "grad_norm": 1.1418474912643433,
      "learning_rate": 0.00018179388496899723,
      "loss": 2.3925,
      "step": 1708
    },
    {
      "epoch": 0.09132444491944318,
      "grad_norm": 0.8353655934333801,
      "learning_rate": 0.000181783194355356,
      "loss": 1.9104,
      "step": 1709
    },
    {
      "epoch": 0.0913778822775002,
      "grad_norm": 1.302309274673462,
      "learning_rate": 0.00018177250374171478,
      "loss": 2.2946,
      "step": 1710
    },
    {
      "epoch": 0.09143131963555721,
      "grad_norm": 1.4934184551239014,
      "learning_rate": 0.00018176181312807356,
      "loss": 2.2568,
      "step": 1711
    },
    {
      "epoch": 0.09148475699361423,
      "grad_norm": 1.2107597589492798,
      "learning_rate": 0.00018175112251443234,
      "loss": 2.2565,
      "step": 1712
    },
    {
      "epoch": 0.09153819435167125,
      "grad_norm": 1.3920509815216064,
      "learning_rate": 0.0001817404319007911,
      "loss": 2.1804,
      "step": 1713
    },
    {
      "epoch": 0.09159163170972827,
      "grad_norm": 1.5216742753982544,
      "learning_rate": 0.0001817297412871499,
      "loss": 2.3552,
      "step": 1714
    },
    {
      "epoch": 0.09164506906778529,
      "grad_norm": 1.5767731666564941,
      "learning_rate": 0.00018171905067350868,
      "loss": 2.1837,
      "step": 1715
    },
    {
      "epoch": 0.0916985064258423,
      "grad_norm": 1.0591894388198853,
      "learning_rate": 0.00018170836005986744,
      "loss": 2.2442,
      "step": 1716
    },
    {
      "epoch": 0.09175194378389932,
      "grad_norm": 1.2527705430984497,
      "learning_rate": 0.00018169766944622622,
      "loss": 2.2811,
      "step": 1717
    },
    {
      "epoch": 0.09180538114195634,
      "grad_norm": 1.416054368019104,
      "learning_rate": 0.00018168697883258502,
      "loss": 2.5667,
      "step": 1718
    },
    {
      "epoch": 0.09185881850001336,
      "grad_norm": 1.272900938987732,
      "learning_rate": 0.00018167628821894377,
      "loss": 2.1609,
      "step": 1719
    },
    {
      "epoch": 0.09191225585807038,
      "grad_norm": 1.2121107578277588,
      "learning_rate": 0.00018166559760530255,
      "loss": 2.2174,
      "step": 1720
    },
    {
      "epoch": 0.0919656932161274,
      "grad_norm": 1.5432981252670288,
      "learning_rate": 0.00018165490699166133,
      "loss": 2.3255,
      "step": 1721
    },
    {
      "epoch": 0.09201913057418441,
      "grad_norm": 1.3185175657272339,
      "learning_rate": 0.0001816442163780201,
      "loss": 2.3021,
      "step": 1722
    },
    {
      "epoch": 0.09207256793224143,
      "grad_norm": 1.5704904794692993,
      "learning_rate": 0.0001816335257643789,
      "loss": 2.3197,
      "step": 1723
    },
    {
      "epoch": 0.09212600529029845,
      "grad_norm": 1.3227955102920532,
      "learning_rate": 0.00018162283515073765,
      "loss": 2.3057,
      "step": 1724
    },
    {
      "epoch": 0.09217944264835547,
      "grad_norm": 1.0292048454284668,
      "learning_rate": 0.00018161214453709643,
      "loss": 2.126,
      "step": 1725
    },
    {
      "epoch": 0.09223288000641248,
      "grad_norm": 1.029311180114746,
      "learning_rate": 0.00018160145392345523,
      "loss": 2.1552,
      "step": 1726
    },
    {
      "epoch": 0.0922863173644695,
      "grad_norm": 1.2701385021209717,
      "learning_rate": 0.00018159076330981398,
      "loss": 2.1516,
      "step": 1727
    },
    {
      "epoch": 0.09233975472252652,
      "grad_norm": 1.2235064506530762,
      "learning_rate": 0.00018158007269617276,
      "loss": 2.0075,
      "step": 1728
    },
    {
      "epoch": 0.09239319208058354,
      "grad_norm": 1.4657113552093506,
      "learning_rate": 0.00018156938208253154,
      "loss": 2.3168,
      "step": 1729
    },
    {
      "epoch": 0.09244662943864056,
      "grad_norm": 1.0536222457885742,
      "learning_rate": 0.00018155869146889032,
      "loss": 2.1131,
      "step": 1730
    },
    {
      "epoch": 0.09250006679669757,
      "grad_norm": 0.8867160081863403,
      "learning_rate": 0.0001815480008552491,
      "loss": 2.0656,
      "step": 1731
    },
    {
      "epoch": 0.09255350415475459,
      "grad_norm": 1.4496417045593262,
      "learning_rate": 0.00018153731024160788,
      "loss": 2.1513,
      "step": 1732
    },
    {
      "epoch": 0.09260694151281161,
      "grad_norm": 1.564332365989685,
      "learning_rate": 0.00018152661962796664,
      "loss": 2.1355,
      "step": 1733
    },
    {
      "epoch": 0.09266037887086863,
      "grad_norm": 1.7184929847717285,
      "learning_rate": 0.00018151592901432544,
      "loss": 2.1907,
      "step": 1734
    },
    {
      "epoch": 0.09271381622892565,
      "grad_norm": 0.8382840156555176,
      "learning_rate": 0.0001815052384006842,
      "loss": 2.0916,
      "step": 1735
    },
    {
      "epoch": 0.09276725358698266,
      "grad_norm": 1.0911198854446411,
      "learning_rate": 0.00018149454778704297,
      "loss": 1.9539,
      "step": 1736
    },
    {
      "epoch": 0.09282069094503968,
      "grad_norm": 1.0077722072601318,
      "learning_rate": 0.00018148385717340178,
      "loss": 2.184,
      "step": 1737
    },
    {
      "epoch": 0.0928741283030967,
      "grad_norm": 1.450655460357666,
      "learning_rate": 0.00018147316655976053,
      "loss": 2.3048,
      "step": 1738
    },
    {
      "epoch": 0.09292756566115372,
      "grad_norm": 1.2607769966125488,
      "learning_rate": 0.0001814624759461193,
      "loss": 2.2566,
      "step": 1739
    },
    {
      "epoch": 0.09298100301921074,
      "grad_norm": 1.3489904403686523,
      "learning_rate": 0.0001814517853324781,
      "loss": 2.383,
      "step": 1740
    },
    {
      "epoch": 0.09303444037726775,
      "grad_norm": 1.056447148323059,
      "learning_rate": 0.00018144109471883687,
      "loss": 2.3229,
      "step": 1741
    },
    {
      "epoch": 0.09308787773532476,
      "grad_norm": 1.185947299003601,
      "learning_rate": 0.00018143040410519565,
      "loss": 2.2373,
      "step": 1742
    },
    {
      "epoch": 0.09314131509338178,
      "grad_norm": 1.2862415313720703,
      "learning_rate": 0.00018141971349155443,
      "loss": 2.1649,
      "step": 1743
    },
    {
      "epoch": 0.0931947524514388,
      "grad_norm": 1.556928038597107,
      "learning_rate": 0.00018140902287791318,
      "loss": 2.3454,
      "step": 1744
    },
    {
      "epoch": 0.09324818980949581,
      "grad_norm": 1.2264282703399658,
      "learning_rate": 0.000181398332264272,
      "loss": 2.0965,
      "step": 1745
    },
    {
      "epoch": 0.09330162716755283,
      "grad_norm": 1.36226487159729,
      "learning_rate": 0.00018138764165063077,
      "loss": 2.2662,
      "step": 1746
    },
    {
      "epoch": 0.09335506452560985,
      "grad_norm": 1.1561315059661865,
      "learning_rate": 0.00018137695103698952,
      "loss": 2.0479,
      "step": 1747
    },
    {
      "epoch": 0.09340850188366687,
      "grad_norm": 1.108999490737915,
      "learning_rate": 0.0001813662604233483,
      "loss": 2.1633,
      "step": 1748
    },
    {
      "epoch": 0.09346193924172388,
      "grad_norm": 1.297783613204956,
      "learning_rate": 0.00018135556980970708,
      "loss": 2.2066,
      "step": 1749
    },
    {
      "epoch": 0.0935153765997809,
      "grad_norm": 1.6139962673187256,
      "learning_rate": 0.00018134487919606586,
      "loss": 1.9069,
      "step": 1750
    },
    {
      "epoch": 0.09356881395783792,
      "grad_norm": 1.2588530778884888,
      "learning_rate": 0.00018133418858242464,
      "loss": 2.0591,
      "step": 1751
    },
    {
      "epoch": 0.09362225131589494,
      "grad_norm": 1.0950963497161865,
      "learning_rate": 0.0001813234979687834,
      "loss": 2.1407,
      "step": 1752
    },
    {
      "epoch": 0.09367568867395196,
      "grad_norm": 1.5274473428726196,
      "learning_rate": 0.0001813128073551422,
      "loss": 2.2331,
      "step": 1753
    },
    {
      "epoch": 0.09372912603200897,
      "grad_norm": 1.9298018217086792,
      "learning_rate": 0.00018130211674150098,
      "loss": 2.4228,
      "step": 1754
    },
    {
      "epoch": 0.09378256339006599,
      "grad_norm": 1.1702924966812134,
      "learning_rate": 0.00018129142612785973,
      "loss": 2.0294,
      "step": 1755
    },
    {
      "epoch": 0.09383600074812301,
      "grad_norm": 1.2087254524230957,
      "learning_rate": 0.0001812807355142185,
      "loss": 2.1816,
      "step": 1756
    },
    {
      "epoch": 0.09388943810618003,
      "grad_norm": 1.0968399047851562,
      "learning_rate": 0.00018127004490057732,
      "loss": 2.0932,
      "step": 1757
    },
    {
      "epoch": 0.09394287546423705,
      "grad_norm": 1.5849730968475342,
      "learning_rate": 0.00018125935428693607,
      "loss": 2.1779,
      "step": 1758
    },
    {
      "epoch": 0.09399631282229406,
      "grad_norm": 1.0600638389587402,
      "learning_rate": 0.00018124866367329485,
      "loss": 2.1539,
      "step": 1759
    },
    {
      "epoch": 0.09404975018035108,
      "grad_norm": 0.8312429189682007,
      "learning_rate": 0.00018123797305965363,
      "loss": 2.0055,
      "step": 1760
    },
    {
      "epoch": 0.0941031875384081,
      "grad_norm": 1.1112010478973389,
      "learning_rate": 0.0001812272824460124,
      "loss": 2.1416,
      "step": 1761
    },
    {
      "epoch": 0.09415662489646512,
      "grad_norm": 1.489575743675232,
      "learning_rate": 0.0001812165918323712,
      "loss": 2.0683,
      "step": 1762
    },
    {
      "epoch": 0.09421006225452214,
      "grad_norm": 1.1958872079849243,
      "learning_rate": 0.00018120590121872994,
      "loss": 2.0792,
      "step": 1763
    },
    {
      "epoch": 0.09426349961257915,
      "grad_norm": 1.2152137756347656,
      "learning_rate": 0.00018119521060508875,
      "loss": 2.2623,
      "step": 1764
    },
    {
      "epoch": 0.09431693697063617,
      "grad_norm": 1.8706555366516113,
      "learning_rate": 0.00018118451999144753,
      "loss": 2.3275,
      "step": 1765
    },
    {
      "epoch": 0.09437037432869319,
      "grad_norm": 1.6730313301086426,
      "learning_rate": 0.00018117382937780628,
      "loss": 2.448,
      "step": 1766
    },
    {
      "epoch": 0.09442381168675021,
      "grad_norm": 1.0963194370269775,
      "learning_rate": 0.00018116313876416506,
      "loss": 1.9923,
      "step": 1767
    },
    {
      "epoch": 0.09447724904480723,
      "grad_norm": 1.4578330516815186,
      "learning_rate": 0.00018115244815052387,
      "loss": 2.3073,
      "step": 1768
    },
    {
      "epoch": 0.09453068640286424,
      "grad_norm": 1.1572866439819336,
      "learning_rate": 0.00018114175753688262,
      "loss": 2.1971,
      "step": 1769
    },
    {
      "epoch": 0.09458412376092126,
      "grad_norm": 1.1666390895843506,
      "learning_rate": 0.0001811310669232414,
      "loss": 2.144,
      "step": 1770
    },
    {
      "epoch": 0.09463756111897828,
      "grad_norm": 1.4061864614486694,
      "learning_rate": 0.00018112037630960018,
      "loss": 2.349,
      "step": 1771
    },
    {
      "epoch": 0.0946909984770353,
      "grad_norm": 1.3389792442321777,
      "learning_rate": 0.00018110968569595896,
      "loss": 2.1293,
      "step": 1772
    },
    {
      "epoch": 0.09474443583509232,
      "grad_norm": 0.7671211361885071,
      "learning_rate": 0.00018109899508231774,
      "loss": 1.9666,
      "step": 1773
    },
    {
      "epoch": 0.09479787319314933,
      "grad_norm": 1.4152919054031372,
      "learning_rate": 0.00018108830446867652,
      "loss": 2.2543,
      "step": 1774
    },
    {
      "epoch": 0.09485131055120635,
      "grad_norm": 1.6669895648956299,
      "learning_rate": 0.00018107761385503527,
      "loss": 2.3026,
      "step": 1775
    },
    {
      "epoch": 0.09490474790926337,
      "grad_norm": 1.467732310295105,
      "learning_rate": 0.00018106692324139408,
      "loss": 2.3537,
      "step": 1776
    },
    {
      "epoch": 0.09495818526732039,
      "grad_norm": 1.531425952911377,
      "learning_rate": 0.00018105623262775283,
      "loss": 2.0314,
      "step": 1777
    },
    {
      "epoch": 0.0950116226253774,
      "grad_norm": 1.3038387298583984,
      "learning_rate": 0.0001810455420141116,
      "loss": 2.1371,
      "step": 1778
    },
    {
      "epoch": 0.09506505998343442,
      "grad_norm": 1.1628007888793945,
      "learning_rate": 0.00018103485140047042,
      "loss": 2.1037,
      "step": 1779
    },
    {
      "epoch": 0.09511849734149144,
      "grad_norm": 1.440220832824707,
      "learning_rate": 0.00018102416078682917,
      "loss": 2.2312,
      "step": 1780
    },
    {
      "epoch": 0.09517193469954846,
      "grad_norm": 1.1769660711288452,
      "learning_rate": 0.00018101347017318795,
      "loss": 2.3121,
      "step": 1781
    },
    {
      "epoch": 0.09522537205760548,
      "grad_norm": 1.5416512489318848,
      "learning_rate": 0.00018100277955954673,
      "loss": 2.154,
      "step": 1782
    },
    {
      "epoch": 0.0952788094156625,
      "grad_norm": 1.0426585674285889,
      "learning_rate": 0.0001809920889459055,
      "loss": 2.2178,
      "step": 1783
    },
    {
      "epoch": 0.0953322467737195,
      "grad_norm": 1.5805245637893677,
      "learning_rate": 0.0001809813983322643,
      "loss": 2.3163,
      "step": 1784
    },
    {
      "epoch": 0.09538568413177652,
      "grad_norm": 1.7888736724853516,
      "learning_rate": 0.00018097070771862307,
      "loss": 2.2233,
      "step": 1785
    },
    {
      "epoch": 0.09543912148983354,
      "grad_norm": 1.6133525371551514,
      "learning_rate": 0.00018096001710498182,
      "loss": 2.2676,
      "step": 1786
    },
    {
      "epoch": 0.09549255884789055,
      "grad_norm": 0.932722270488739,
      "learning_rate": 0.00018094932649134063,
      "loss": 2.3927,
      "step": 1787
    },
    {
      "epoch": 0.09554599620594757,
      "grad_norm": 1.633812427520752,
      "learning_rate": 0.00018093863587769938,
      "loss": 2.0469,
      "step": 1788
    },
    {
      "epoch": 0.09559943356400459,
      "grad_norm": 0.8330687880516052,
      "learning_rate": 0.00018092794526405816,
      "loss": 2.211,
      "step": 1789
    },
    {
      "epoch": 0.09565287092206161,
      "grad_norm": 1.0871025323867798,
      "learning_rate": 0.00018091725465041694,
      "loss": 2.0657,
      "step": 1790
    },
    {
      "epoch": 0.09570630828011863,
      "grad_norm": 1.255500078201294,
      "learning_rate": 0.00018090656403677572,
      "loss": 2.0803,
      "step": 1791
    },
    {
      "epoch": 0.09575974563817564,
      "grad_norm": 1.0783809423446655,
      "learning_rate": 0.0001808958734231345,
      "loss": 2.1678,
      "step": 1792
    },
    {
      "epoch": 0.09581318299623266,
      "grad_norm": 1.272560715675354,
      "learning_rate": 0.00018088518280949328,
      "loss": 2.1103,
      "step": 1793
    },
    {
      "epoch": 0.09586662035428968,
      "grad_norm": 1.3809330463409424,
      "learning_rate": 0.00018087449219585203,
      "loss": 2.0465,
      "step": 1794
    },
    {
      "epoch": 0.0959200577123467,
      "grad_norm": 0.9741944670677185,
      "learning_rate": 0.00018086380158221084,
      "loss": 2.0534,
      "step": 1795
    },
    {
      "epoch": 0.09597349507040372,
      "grad_norm": 1.4386190176010132,
      "learning_rate": 0.00018085311096856962,
      "loss": 2.1401,
      "step": 1796
    },
    {
      "epoch": 0.09602693242846073,
      "grad_norm": 1.1740531921386719,
      "learning_rate": 0.00018084242035492837,
      "loss": 2.2569,
      "step": 1797
    },
    {
      "epoch": 0.09608036978651775,
      "grad_norm": 1.3490874767303467,
      "learning_rate": 0.00018083172974128715,
      "loss": 2.2878,
      "step": 1798
    },
    {
      "epoch": 0.09613380714457477,
      "grad_norm": 1.9786912202835083,
      "learning_rate": 0.00018082103912764593,
      "loss": 2.2999,
      "step": 1799
    },
    {
      "epoch": 0.09618724450263179,
      "grad_norm": 2.200923204421997,
      "learning_rate": 0.0001808103485140047,
      "loss": 2.289,
      "step": 1800
    },
    {
      "epoch": 0.0962406818606888,
      "grad_norm": 1.178407907485962,
      "learning_rate": 0.0001807996579003635,
      "loss": 1.9946,
      "step": 1801
    },
    {
      "epoch": 0.09629411921874582,
      "grad_norm": 1.3120521306991577,
      "learning_rate": 0.00018078896728672227,
      "loss": 2.0554,
      "step": 1802
    },
    {
      "epoch": 0.09634755657680284,
      "grad_norm": 0.8096914291381836,
      "learning_rate": 0.00018077827667308105,
      "loss": 1.9588,
      "step": 1803
    },
    {
      "epoch": 0.09640099393485986,
      "grad_norm": 1.0738158226013184,
      "learning_rate": 0.00018076758605943983,
      "loss": 2.213,
      "step": 1804
    },
    {
      "epoch": 0.09645443129291688,
      "grad_norm": 1.7332353591918945,
      "learning_rate": 0.00018075689544579858,
      "loss": 2.5601,
      "step": 1805
    },
    {
      "epoch": 0.0965078686509739,
      "grad_norm": 0.9192349314689636,
      "learning_rate": 0.0001807462048321574,
      "loss": 2.1308,
      "step": 1806
    },
    {
      "epoch": 0.09656130600903091,
      "grad_norm": 1.2212162017822266,
      "learning_rate": 0.00018073551421851617,
      "loss": 2.216,
      "step": 1807
    },
    {
      "epoch": 0.09661474336708793,
      "grad_norm": 1.1156747341156006,
      "learning_rate": 0.00018072482360487492,
      "loss": 2.2679,
      "step": 1808
    },
    {
      "epoch": 0.09666818072514495,
      "grad_norm": 1.3503189086914062,
      "learning_rate": 0.0001807141329912337,
      "loss": 2.1652,
      "step": 1809
    },
    {
      "epoch": 0.09672161808320197,
      "grad_norm": 1.0837043523788452,
      "learning_rate": 0.00018070344237759248,
      "loss": 2.0616,
      "step": 1810
    },
    {
      "epoch": 0.09677505544125899,
      "grad_norm": 1.3301382064819336,
      "learning_rate": 0.00018069275176395126,
      "loss": 2.194,
      "step": 1811
    },
    {
      "epoch": 0.096828492799316,
      "grad_norm": 1.5716549158096313,
      "learning_rate": 0.00018068206115031004,
      "loss": 2.2005,
      "step": 1812
    },
    {
      "epoch": 0.09688193015737302,
      "grad_norm": 0.9784141182899475,
      "learning_rate": 0.00018067137053666882,
      "loss": 2.1934,
      "step": 1813
    },
    {
      "epoch": 0.09693536751543004,
      "grad_norm": 1.3185721635818481,
      "learning_rate": 0.0001806606799230276,
      "loss": 2.3714,
      "step": 1814
    },
    {
      "epoch": 0.09698880487348706,
      "grad_norm": 1.1877723932266235,
      "learning_rate": 0.00018064998930938638,
      "loss": 2.0913,
      "step": 1815
    },
    {
      "epoch": 0.09704224223154408,
      "grad_norm": 1.4813649654388428,
      "learning_rate": 0.00018063929869574513,
      "loss": 2.2113,
      "step": 1816
    },
    {
      "epoch": 0.0970956795896011,
      "grad_norm": 1.4625523090362549,
      "learning_rate": 0.0001806286080821039,
      "loss": 2.1473,
      "step": 1817
    },
    {
      "epoch": 0.09714911694765811,
      "grad_norm": 1.1931407451629639,
      "learning_rate": 0.00018061791746846272,
      "loss": 1.9927,
      "step": 1818
    },
    {
      "epoch": 0.09720255430571513,
      "grad_norm": 1.8923733234405518,
      "learning_rate": 0.00018060722685482147,
      "loss": 2.4097,
      "step": 1819
    },
    {
      "epoch": 0.09725599166377215,
      "grad_norm": 1.038619041442871,
      "learning_rate": 0.00018059653624118025,
      "loss": 2.0406,
      "step": 1820
    },
    {
      "epoch": 0.09730942902182917,
      "grad_norm": 1.1912587881088257,
      "learning_rate": 0.00018058584562753903,
      "loss": 2.184,
      "step": 1821
    },
    {
      "epoch": 0.09736286637988618,
      "grad_norm": 1.1036150455474854,
      "learning_rate": 0.0001805751550138978,
      "loss": 2.1055,
      "step": 1822
    },
    {
      "epoch": 0.0974163037379432,
      "grad_norm": 1.0768064260482788,
      "learning_rate": 0.0001805644644002566,
      "loss": 2.2154,
      "step": 1823
    },
    {
      "epoch": 0.09746974109600022,
      "grad_norm": 1.233999252319336,
      "learning_rate": 0.00018055377378661537,
      "loss": 2.2925,
      "step": 1824
    },
    {
      "epoch": 0.09752317845405724,
      "grad_norm": 0.96048504114151,
      "learning_rate": 0.00018054308317297415,
      "loss": 2.1241,
      "step": 1825
    },
    {
      "epoch": 0.09757661581211424,
      "grad_norm": 0.9953716397285461,
      "learning_rate": 0.00018053239255933293,
      "loss": 2.0045,
      "step": 1826
    },
    {
      "epoch": 0.09763005317017126,
      "grad_norm": 1.404998540878296,
      "learning_rate": 0.00018052170194569168,
      "loss": 2.3607,
      "step": 1827
    },
    {
      "epoch": 0.09768349052822828,
      "grad_norm": 1.6771116256713867,
      "learning_rate": 0.00018051101133205046,
      "loss": 2.2698,
      "step": 1828
    },
    {
      "epoch": 0.0977369278862853,
      "grad_norm": 1.1466621160507202,
      "learning_rate": 0.00018050032071840926,
      "loss": 2.0082,
      "step": 1829
    },
    {
      "epoch": 0.09779036524434231,
      "grad_norm": 1.272828459739685,
      "learning_rate": 0.00018048963010476802,
      "loss": 2.3272,
      "step": 1830
    },
    {
      "epoch": 0.09784380260239933,
      "grad_norm": 1.2650890350341797,
      "learning_rate": 0.0001804789394911268,
      "loss": 2.0131,
      "step": 1831
    },
    {
      "epoch": 0.09789723996045635,
      "grad_norm": 1.2753151655197144,
      "learning_rate": 0.00018046824887748558,
      "loss": 2.2998,
      "step": 1832
    },
    {
      "epoch": 0.09795067731851337,
      "grad_norm": 0.890386164188385,
      "learning_rate": 0.00018045755826384436,
      "loss": 2.0413,
      "step": 1833
    },
    {
      "epoch": 0.09800411467657039,
      "grad_norm": 1.2399905920028687,
      "learning_rate": 0.00018044686765020314,
      "loss": 2.2951,
      "step": 1834
    },
    {
      "epoch": 0.0980575520346274,
      "grad_norm": 0.9705721139907837,
      "learning_rate": 0.00018043617703656192,
      "loss": 2.272,
      "step": 1835
    },
    {
      "epoch": 0.09811098939268442,
      "grad_norm": 0.9580217599868774,
      "learning_rate": 0.00018042548642292067,
      "loss": 2.1673,
      "step": 1836
    },
    {
      "epoch": 0.09816442675074144,
      "grad_norm": 1.1386293172836304,
      "learning_rate": 0.00018041479580927947,
      "loss": 2.1979,
      "step": 1837
    },
    {
      "epoch": 0.09821786410879846,
      "grad_norm": 1.16373610496521,
      "learning_rate": 0.00018040410519563823,
      "loss": 2.1638,
      "step": 1838
    },
    {
      "epoch": 0.09827130146685548,
      "grad_norm": 1.3419420719146729,
      "learning_rate": 0.000180393414581997,
      "loss": 2.2497,
      "step": 1839
    },
    {
      "epoch": 0.0983247388249125,
      "grad_norm": 1.0608454942703247,
      "learning_rate": 0.0001803827239683558,
      "loss": 2.3172,
      "step": 1840
    },
    {
      "epoch": 0.09837817618296951,
      "grad_norm": 1.6041268110275269,
      "learning_rate": 0.00018037203335471457,
      "loss": 2.0865,
      "step": 1841
    },
    {
      "epoch": 0.09843161354102653,
      "grad_norm": 1.3268624544143677,
      "learning_rate": 0.00018036134274107335,
      "loss": 2.1696,
      "step": 1842
    },
    {
      "epoch": 0.09848505089908355,
      "grad_norm": 1.518253207206726,
      "learning_rate": 0.00018035065212743213,
      "loss": 2.2146,
      "step": 1843
    },
    {
      "epoch": 0.09853848825714057,
      "grad_norm": 1.1109877824783325,
      "learning_rate": 0.00018033996151379088,
      "loss": 2.2837,
      "step": 1844
    },
    {
      "epoch": 0.09859192561519758,
      "grad_norm": 1.3629151582717896,
      "learning_rate": 0.00018032927090014968,
      "loss": 2.1822,
      "step": 1845
    },
    {
      "epoch": 0.0986453629732546,
      "grad_norm": 1.4092791080474854,
      "learning_rate": 0.00018031858028650846,
      "loss": 2.2358,
      "step": 1846
    },
    {
      "epoch": 0.09869880033131162,
      "grad_norm": 1.1932902336120605,
      "learning_rate": 0.00018030788967286722,
      "loss": 2.09,
      "step": 1847
    },
    {
      "epoch": 0.09875223768936864,
      "grad_norm": 1.2336487770080566,
      "learning_rate": 0.00018029719905922602,
      "loss": 2.0873,
      "step": 1848
    },
    {
      "epoch": 0.09880567504742566,
      "grad_norm": 1.1992955207824707,
      "learning_rate": 0.00018028650844558478,
      "loss": 2.2804,
      "step": 1849
    },
    {
      "epoch": 0.09885911240548267,
      "grad_norm": 1.1376066207885742,
      "learning_rate": 0.00018027581783194356,
      "loss": 1.9026,
      "step": 1850
    },
    {
      "epoch": 0.09891254976353969,
      "grad_norm": 1.599445104598999,
      "learning_rate": 0.00018026512721830234,
      "loss": 2.347,
      "step": 1851
    },
    {
      "epoch": 0.09896598712159671,
      "grad_norm": 1.6280083656311035,
      "learning_rate": 0.00018025443660466112,
      "loss": 2.2563,
      "step": 1852
    },
    {
      "epoch": 0.09901942447965373,
      "grad_norm": 1.4407860040664673,
      "learning_rate": 0.0001802437459910199,
      "loss": 2.3863,
      "step": 1853
    },
    {
      "epoch": 0.09907286183771075,
      "grad_norm": 1.6274296045303345,
      "learning_rate": 0.00018023305537737867,
      "loss": 2.2862,
      "step": 1854
    },
    {
      "epoch": 0.09912629919576776,
      "grad_norm": 0.9199209809303284,
      "learning_rate": 0.00018022236476373743,
      "loss": 1.9549,
      "step": 1855
    },
    {
      "epoch": 0.09917973655382478,
      "grad_norm": 0.9423142075538635,
      "learning_rate": 0.00018021167415009623,
      "loss": 2.1424,
      "step": 1856
    },
    {
      "epoch": 0.0992331739118818,
      "grad_norm": 1.0432459115982056,
      "learning_rate": 0.00018020098353645501,
      "loss": 1.8522,
      "step": 1857
    },
    {
      "epoch": 0.09928661126993882,
      "grad_norm": 1.6294795274734497,
      "learning_rate": 0.00018019029292281377,
      "loss": 2.2894,
      "step": 1858
    },
    {
      "epoch": 0.09934004862799584,
      "grad_norm": 1.3551446199417114,
      "learning_rate": 0.00018017960230917255,
      "loss": 2.2079,
      "step": 1859
    },
    {
      "epoch": 0.09939348598605285,
      "grad_norm": 1.4488048553466797,
      "learning_rate": 0.00018016891169553135,
      "loss": 2.1691,
      "step": 1860
    },
    {
      "epoch": 0.09944692334410987,
      "grad_norm": 1.394226312637329,
      "learning_rate": 0.0001801582210818901,
      "loss": 2.223,
      "step": 1861
    },
    {
      "epoch": 0.09950036070216689,
      "grad_norm": 1.1823819875717163,
      "learning_rate": 0.00018014753046824888,
      "loss": 2.0795,
      "step": 1862
    },
    {
      "epoch": 0.09955379806022391,
      "grad_norm": 1.0804996490478516,
      "learning_rate": 0.00018013683985460766,
      "loss": 2.18,
      "step": 1863
    },
    {
      "epoch": 0.09960723541828093,
      "grad_norm": 0.8963199257850647,
      "learning_rate": 0.00018012614924096644,
      "loss": 2.0679,
      "step": 1864
    },
    {
      "epoch": 0.09966067277633794,
      "grad_norm": 1.226938009262085,
      "learning_rate": 0.00018011545862732522,
      "loss": 2.1159,
      "step": 1865
    },
    {
      "epoch": 0.09971411013439496,
      "grad_norm": 1.1686612367630005,
      "learning_rate": 0.00018010476801368398,
      "loss": 2.2391,
      "step": 1866
    },
    {
      "epoch": 0.09976754749245198,
      "grad_norm": 1.2829197645187378,
      "learning_rate": 0.00018009407740004278,
      "loss": 2.1043,
      "step": 1867
    },
    {
      "epoch": 0.09982098485050898,
      "grad_norm": 1.5708916187286377,
      "learning_rate": 0.00018008338678640156,
      "loss": 2.2015,
      "step": 1868
    },
    {
      "epoch": 0.099874422208566,
      "grad_norm": 1.3815571069717407,
      "learning_rate": 0.00018007269617276032,
      "loss": 2.2627,
      "step": 1869
    },
    {
      "epoch": 0.09992785956662302,
      "grad_norm": 1.1004174947738647,
      "learning_rate": 0.0001800620055591191,
      "loss": 1.8242,
      "step": 1870
    },
    {
      "epoch": 0.09998129692468004,
      "grad_norm": 1.2526723146438599,
      "learning_rate": 0.0001800513149454779,
      "loss": 1.9584,
      "step": 1871
    },
    {
      "epoch": 0.10003473428273706,
      "grad_norm": 1.3914035558700562,
      "learning_rate": 0.00018004062433183665,
      "loss": 2.2302,
      "step": 1872
    },
    {
      "epoch": 0.10008817164079407,
      "grad_norm": 1.1624119281768799,
      "learning_rate": 0.00018002993371819543,
      "loss": 2.1571,
      "step": 1873
    },
    {
      "epoch": 0.10014160899885109,
      "grad_norm": 1.5081982612609863,
      "learning_rate": 0.0001800192431045542,
      "loss": 2.3259,
      "step": 1874
    },
    {
      "epoch": 0.10019504635690811,
      "grad_norm": 1.1724337339401245,
      "learning_rate": 0.000180008552490913,
      "loss": 1.9236,
      "step": 1875
    },
    {
      "epoch": 0.10024848371496513,
      "grad_norm": 1.5280777215957642,
      "learning_rate": 0.00017999786187727177,
      "loss": 2.3837,
      "step": 1876
    },
    {
      "epoch": 0.10030192107302215,
      "grad_norm": 1.3624268770217896,
      "learning_rate": 0.00017998717126363053,
      "loss": 2.2876,
      "step": 1877
    },
    {
      "epoch": 0.10035535843107916,
      "grad_norm": 1.5802150964736938,
      "learning_rate": 0.0001799764806499893,
      "loss": 2.2716,
      "step": 1878
    },
    {
      "epoch": 0.10040879578913618,
      "grad_norm": 1.2629598379135132,
      "learning_rate": 0.0001799657900363481,
      "loss": 2.304,
      "step": 1879
    },
    {
      "epoch": 0.1004622331471932,
      "grad_norm": 1.6409964561462402,
      "learning_rate": 0.00017995509942270686,
      "loss": 2.3491,
      "step": 1880
    },
    {
      "epoch": 0.10051567050525022,
      "grad_norm": 1.50693941116333,
      "learning_rate": 0.00017994440880906564,
      "loss": 2.3257,
      "step": 1881
    },
    {
      "epoch": 0.10056910786330724,
      "grad_norm": 1.2719720602035522,
      "learning_rate": 0.00017993371819542442,
      "loss": 2.3402,
      "step": 1882
    },
    {
      "epoch": 0.10062254522136425,
      "grad_norm": 1.3064569234848022,
      "learning_rate": 0.0001799230275817832,
      "loss": 2.1035,
      "step": 1883
    },
    {
      "epoch": 0.10067598257942127,
      "grad_norm": 0.815207302570343,
      "learning_rate": 0.00017991233696814198,
      "loss": 2.0448,
      "step": 1884
    },
    {
      "epoch": 0.10072941993747829,
      "grad_norm": 1.3067448139190674,
      "learning_rate": 0.00017990164635450076,
      "loss": 2.1445,
      "step": 1885
    },
    {
      "epoch": 0.10078285729553531,
      "grad_norm": 1.4244252443313599,
      "learning_rate": 0.00017989095574085951,
      "loss": 2.196,
      "step": 1886
    },
    {
      "epoch": 0.10083629465359233,
      "grad_norm": 1.059455394744873,
      "learning_rate": 0.00017988026512721832,
      "loss": 2.1607,
      "step": 1887
    },
    {
      "epoch": 0.10088973201164934,
      "grad_norm": 1.1604814529418945,
      "learning_rate": 0.0001798695745135771,
      "loss": 2.034,
      "step": 1888
    },
    {
      "epoch": 0.10094316936970636,
      "grad_norm": 1.1268491744995117,
      "learning_rate": 0.00017985888389993585,
      "loss": 2.121,
      "step": 1889
    },
    {
      "epoch": 0.10099660672776338,
      "grad_norm": 1.153203010559082,
      "learning_rate": 0.00017984819328629466,
      "loss": 2.2316,
      "step": 1890
    },
    {
      "epoch": 0.1010500440858204,
      "grad_norm": 1.1058212518692017,
      "learning_rate": 0.0001798375026726534,
      "loss": 2.0635,
      "step": 1891
    },
    {
      "epoch": 0.10110348144387742,
      "grad_norm": 1.2023380994796753,
      "learning_rate": 0.0001798268120590122,
      "loss": 2.1646,
      "step": 1892
    },
    {
      "epoch": 0.10115691880193443,
      "grad_norm": 1.2379552125930786,
      "learning_rate": 0.00017981612144537097,
      "loss": 2.2059,
      "step": 1893
    },
    {
      "epoch": 0.10121035615999145,
      "grad_norm": 1.386638879776001,
      "learning_rate": 0.00017980543083172975,
      "loss": 2.1911,
      "step": 1894
    },
    {
      "epoch": 0.10126379351804847,
      "grad_norm": 1.0608186721801758,
      "learning_rate": 0.00017979474021808853,
      "loss": 2.0797,
      "step": 1895
    },
    {
      "epoch": 0.10131723087610549,
      "grad_norm": 1.414190411567688,
      "learning_rate": 0.0001797840496044473,
      "loss": 2.0516,
      "step": 1896
    },
    {
      "epoch": 0.1013706682341625,
      "grad_norm": 1.1751397848129272,
      "learning_rate": 0.00017977335899080606,
      "loss": 2.1874,
      "step": 1897
    },
    {
      "epoch": 0.10142410559221952,
      "grad_norm": 1.3532601594924927,
      "learning_rate": 0.00017976266837716487,
      "loss": 2.151,
      "step": 1898
    },
    {
      "epoch": 0.10147754295027654,
      "grad_norm": 1.2490966320037842,
      "learning_rate": 0.00017975197776352365,
      "loss": 2.2763,
      "step": 1899
    },
    {
      "epoch": 0.10153098030833356,
      "grad_norm": 1.5967061519622803,
      "learning_rate": 0.0001797412871498824,
      "loss": 2.2936,
      "step": 1900
    },
    {
      "epoch": 0.10158441766639058,
      "grad_norm": 1.3239848613739014,
      "learning_rate": 0.00017973059653624118,
      "loss": 2.3684,
      "step": 1901
    },
    {
      "epoch": 0.1016378550244476,
      "grad_norm": 1.1373406648635864,
      "learning_rate": 0.00017971990592259996,
      "loss": 2.1224,
      "step": 1902
    },
    {
      "epoch": 0.10169129238250461,
      "grad_norm": 1.5554940700531006,
      "learning_rate": 0.00017970921530895874,
      "loss": 2.1479,
      "step": 1903
    },
    {
      "epoch": 0.10174472974056163,
      "grad_norm": 1.2285852432250977,
      "learning_rate": 0.00017969852469531752,
      "loss": 2.2588,
      "step": 1904
    },
    {
      "epoch": 0.10179816709861865,
      "grad_norm": 0.954900860786438,
      "learning_rate": 0.00017968783408167627,
      "loss": 2.2131,
      "step": 1905
    },
    {
      "epoch": 0.10185160445667567,
      "grad_norm": 1.259300708770752,
      "learning_rate": 0.00017967714346803508,
      "loss": 2.3709,
      "step": 1906
    },
    {
      "epoch": 0.10190504181473269,
      "grad_norm": 1.634781837463379,
      "learning_rate": 0.00017966645285439386,
      "loss": 2.4779,
      "step": 1907
    },
    {
      "epoch": 0.1019584791727897,
      "grad_norm": 1.43214750289917,
      "learning_rate": 0.0001796557622407526,
      "loss": 1.9541,
      "step": 1908
    },
    {
      "epoch": 0.10201191653084672,
      "grad_norm": 1.2592161893844604,
      "learning_rate": 0.0001796450716271114,
      "loss": 1.9639,
      "step": 1909
    },
    {
      "epoch": 0.10206535388890373,
      "grad_norm": 1.2434192895889282,
      "learning_rate": 0.0001796343810134702,
      "loss": 2.0718,
      "step": 1910
    },
    {
      "epoch": 0.10211879124696074,
      "grad_norm": 1.0434597730636597,
      "learning_rate": 0.00017962369039982895,
      "loss": 2.2564,
      "step": 1911
    },
    {
      "epoch": 0.10217222860501776,
      "grad_norm": 1.6224656105041504,
      "learning_rate": 0.00017961299978618773,
      "loss": 2.2778,
      "step": 1912
    },
    {
      "epoch": 0.10222566596307478,
      "grad_norm": 1.2234731912612915,
      "learning_rate": 0.0001796023091725465,
      "loss": 2.2281,
      "step": 1913
    },
    {
      "epoch": 0.1022791033211318,
      "grad_norm": 1.036693811416626,
      "learning_rate": 0.0001795916185589053,
      "loss": 2.1327,
      "step": 1914
    },
    {
      "epoch": 0.10233254067918882,
      "grad_norm": 1.0913920402526855,
      "learning_rate": 0.00017958092794526407,
      "loss": 2.2918,
      "step": 1915
    },
    {
      "epoch": 0.10238597803724583,
      "grad_norm": 1.7125253677368164,
      "learning_rate": 0.00017957023733162285,
      "loss": 2.1063,
      "step": 1916
    },
    {
      "epoch": 0.10243941539530285,
      "grad_norm": 1.1202421188354492,
      "learning_rate": 0.00017955954671798163,
      "loss": 1.9758,
      "step": 1917
    },
    {
      "epoch": 0.10249285275335987,
      "grad_norm": 1.7284988164901733,
      "learning_rate": 0.0001795488561043404,
      "loss": 2.3744,
      "step": 1918
    },
    {
      "epoch": 0.10254629011141689,
      "grad_norm": 1.0561195611953735,
      "learning_rate": 0.00017953816549069916,
      "loss": 2.1489,
      "step": 1919
    },
    {
      "epoch": 0.1025997274694739,
      "grad_norm": 1.2330259084701538,
      "learning_rate": 0.00017952747487705794,
      "loss": 1.9698,
      "step": 1920
    },
    {
      "epoch": 0.10265316482753092,
      "grad_norm": 1.4632128477096558,
      "learning_rate": 0.00017951678426341675,
      "loss": 2.1327,
      "step": 1921
    },
    {
      "epoch": 0.10270660218558794,
      "grad_norm": 1.1985055208206177,
      "learning_rate": 0.0001795060936497755,
      "loss": 2.1599,
      "step": 1922
    },
    {
      "epoch": 0.10276003954364496,
      "grad_norm": 1.3247497081756592,
      "learning_rate": 0.00017949540303613428,
      "loss": 2.0864,
      "step": 1923
    },
    {
      "epoch": 0.10281347690170198,
      "grad_norm": 1.7488229274749756,
      "learning_rate": 0.00017948471242249306,
      "loss": 2.1549,
      "step": 1924
    },
    {
      "epoch": 0.102866914259759,
      "grad_norm": 1.1472219228744507,
      "learning_rate": 0.00017947402180885184,
      "loss": 1.8745,
      "step": 1925
    },
    {
      "epoch": 0.10292035161781601,
      "grad_norm": 1.4549018144607544,
      "learning_rate": 0.00017946333119521062,
      "loss": 2.4364,
      "step": 1926
    },
    {
      "epoch": 0.10297378897587303,
      "grad_norm": 1.0157874822616577,
      "learning_rate": 0.0001794526405815694,
      "loss": 2.0941,
      "step": 1927
    },
    {
      "epoch": 0.10302722633393005,
      "grad_norm": 1.2752944231033325,
      "learning_rate": 0.00017944194996792815,
      "loss": 2.1774,
      "step": 1928
    },
    {
      "epoch": 0.10308066369198707,
      "grad_norm": 1.269263744354248,
      "learning_rate": 0.00017943125935428696,
      "loss": 1.8959,
      "step": 1929
    },
    {
      "epoch": 0.10313410105004409,
      "grad_norm": 1.4254354238510132,
      "learning_rate": 0.0001794205687406457,
      "loss": 2.3919,
      "step": 1930
    },
    {
      "epoch": 0.1031875384081011,
      "grad_norm": 0.9763908386230469,
      "learning_rate": 0.0001794098781270045,
      "loss": 2.2924,
      "step": 1931
    },
    {
      "epoch": 0.10324097576615812,
      "grad_norm": 1.4375405311584473,
      "learning_rate": 0.00017939918751336327,
      "loss": 2.1692,
      "step": 1932
    },
    {
      "epoch": 0.10329441312421514,
      "grad_norm": 1.504630446434021,
      "learning_rate": 0.00017938849689972205,
      "loss": 2.105,
      "step": 1933
    },
    {
      "epoch": 0.10334785048227216,
      "grad_norm": 1.181902289390564,
      "learning_rate": 0.00017937780628608083,
      "loss": 1.9871,
      "step": 1934
    },
    {
      "epoch": 0.10340128784032918,
      "grad_norm": 1.547904372215271,
      "learning_rate": 0.0001793671156724396,
      "loss": 2.0619,
      "step": 1935
    },
    {
      "epoch": 0.1034547251983862,
      "grad_norm": 1.188767910003662,
      "learning_rate": 0.0001793564250587984,
      "loss": 2.1499,
      "step": 1936
    },
    {
      "epoch": 0.10350816255644321,
      "grad_norm": 1.2499279975891113,
      "learning_rate": 0.00017934573444515717,
      "loss": 2.3027,
      "step": 1937
    },
    {
      "epoch": 0.10356159991450023,
      "grad_norm": 1.6280772686004639,
      "learning_rate": 0.00017933504383151595,
      "loss": 2.2279,
      "step": 1938
    },
    {
      "epoch": 0.10361503727255725,
      "grad_norm": 1.2232657670974731,
      "learning_rate": 0.0001793243532178747,
      "loss": 2.2193,
      "step": 1939
    },
    {
      "epoch": 0.10366847463061427,
      "grad_norm": 1.2498912811279297,
      "learning_rate": 0.0001793136626042335,
      "loss": 2.2265,
      "step": 1940
    },
    {
      "epoch": 0.10372191198867128,
      "grad_norm": 1.5392651557922363,
      "learning_rate": 0.00017930297199059226,
      "loss": 2.2373,
      "step": 1941
    },
    {
      "epoch": 0.1037753493467283,
      "grad_norm": 1.1403491497039795,
      "learning_rate": 0.00017929228137695104,
      "loss": 2.373,
      "step": 1942
    },
    {
      "epoch": 0.10382878670478532,
      "grad_norm": 1.552067756652832,
      "learning_rate": 0.00017928159076330982,
      "loss": 2.0359,
      "step": 1943
    },
    {
      "epoch": 0.10388222406284234,
      "grad_norm": 1.214004635810852,
      "learning_rate": 0.0001792709001496686,
      "loss": 2.3253,
      "step": 1944
    },
    {
      "epoch": 0.10393566142089936,
      "grad_norm": 1.4142701625823975,
      "learning_rate": 0.00017926020953602738,
      "loss": 2.2533,
      "step": 1945
    },
    {
      "epoch": 0.10398909877895637,
      "grad_norm": 1.6369661092758179,
      "learning_rate": 0.00017924951892238616,
      "loss": 2.2141,
      "step": 1946
    },
    {
      "epoch": 0.10404253613701339,
      "grad_norm": 1.3067395687103271,
      "learning_rate": 0.0001792388283087449,
      "loss": 2.336,
      "step": 1947
    },
    {
      "epoch": 0.10409597349507041,
      "grad_norm": 1.0537761449813843,
      "learning_rate": 0.00017922813769510372,
      "loss": 1.9688,
      "step": 1948
    },
    {
      "epoch": 0.10414941085312743,
      "grad_norm": 1.2038319110870361,
      "learning_rate": 0.0001792174470814625,
      "loss": 2.1892,
      "step": 1949
    },
    {
      "epoch": 0.10420284821118445,
      "grad_norm": 1.466919183731079,
      "learning_rate": 0.00017920675646782125,
      "loss": 2.262,
      "step": 1950
    },
    {
      "epoch": 0.10425628556924146,
      "grad_norm": 1.1292530298233032,
      "learning_rate": 0.00017919606585418003,
      "loss": 2.0681,
      "step": 1951
    },
    {
      "epoch": 0.10430972292729847,
      "grad_norm": 1.1846461296081543,
      "learning_rate": 0.0001791853752405388,
      "loss": 2.1146,
      "step": 1952
    },
    {
      "epoch": 0.10436316028535549,
      "grad_norm": 1.5097068548202515,
      "learning_rate": 0.0001791746846268976,
      "loss": 2.1301,
      "step": 1953
    },
    {
      "epoch": 0.1044165976434125,
      "grad_norm": 1.4437165260314941,
      "learning_rate": 0.00017916399401325637,
      "loss": 2.2396,
      "step": 1954
    },
    {
      "epoch": 0.10447003500146952,
      "grad_norm": 2.330099582672119,
      "learning_rate": 0.00017915330339961515,
      "loss": 2.2256,
      "step": 1955
    },
    {
      "epoch": 0.10452347235952654,
      "grad_norm": 0.9882769584655762,
      "learning_rate": 0.00017914261278597393,
      "loss": 2.056,
      "step": 1956
    },
    {
      "epoch": 0.10457690971758356,
      "grad_norm": 1.254730224609375,
      "learning_rate": 0.0001791319221723327,
      "loss": 2.3642,
      "step": 1957
    },
    {
      "epoch": 0.10463034707564058,
      "grad_norm": 1.5519224405288696,
      "learning_rate": 0.00017912123155869146,
      "loss": 2.1962,
      "step": 1958
    },
    {
      "epoch": 0.1046837844336976,
      "grad_norm": 1.8897209167480469,
      "learning_rate": 0.00017911054094505027,
      "loss": 2.3355,
      "step": 1959
    },
    {
      "epoch": 0.10473722179175461,
      "grad_norm": 1.6575820446014404,
      "learning_rate": 0.00017909985033140905,
      "loss": 2.2209,
      "step": 1960
    },
    {
      "epoch": 0.10479065914981163,
      "grad_norm": 1.1669297218322754,
      "learning_rate": 0.0001790891597177678,
      "loss": 2.2369,
      "step": 1961
    },
    {
      "epoch": 0.10484409650786865,
      "grad_norm": 1.7715110778808594,
      "learning_rate": 0.00017907846910412658,
      "loss": 2.3675,
      "step": 1962
    },
    {
      "epoch": 0.10489753386592567,
      "grad_norm": 1.2590380907058716,
      "learning_rate": 0.00017906777849048536,
      "loss": 2.0725,
      "step": 1963
    },
    {
      "epoch": 0.10495097122398268,
      "grad_norm": 1.5356802940368652,
      "learning_rate": 0.00017905708787684414,
      "loss": 2.0828,
      "step": 1964
    },
    {
      "epoch": 0.1050044085820397,
      "grad_norm": 1.0631439685821533,
      "learning_rate": 0.00017904639726320292,
      "loss": 2.1784,
      "step": 1965
    },
    {
      "epoch": 0.10505784594009672,
      "grad_norm": 1.0055522918701172,
      "learning_rate": 0.0001790357066495617,
      "loss": 2.2724,
      "step": 1966
    },
    {
      "epoch": 0.10511128329815374,
      "grad_norm": 1.5139145851135254,
      "learning_rate": 0.00017902501603592048,
      "loss": 2.3516,
      "step": 1967
    },
    {
      "epoch": 0.10516472065621076,
      "grad_norm": 1.1604312658309937,
      "learning_rate": 0.00017901432542227926,
      "loss": 2.2477,
      "step": 1968
    },
    {
      "epoch": 0.10521815801426777,
      "grad_norm": 1.1923736333847046,
      "learning_rate": 0.000179003634808638,
      "loss": 2.2315,
      "step": 1969
    },
    {
      "epoch": 0.10527159537232479,
      "grad_norm": 1.3335555791854858,
      "learning_rate": 0.0001789929441949968,
      "loss": 2.2007,
      "step": 1970
    },
    {
      "epoch": 0.10532503273038181,
      "grad_norm": 1.1585769653320312,
      "learning_rate": 0.0001789822535813556,
      "loss": 2.2327,
      "step": 1971
    },
    {
      "epoch": 0.10537847008843883,
      "grad_norm": 1.62908935546875,
      "learning_rate": 0.00017897156296771435,
      "loss": 2.2718,
      "step": 1972
    },
    {
      "epoch": 0.10543190744649585,
      "grad_norm": 1.2186211347579956,
      "learning_rate": 0.00017896087235407313,
      "loss": 2.2554,
      "step": 1973
    },
    {
      "epoch": 0.10548534480455286,
      "grad_norm": 1.1396794319152832,
      "learning_rate": 0.0001789501817404319,
      "loss": 2.3477,
      "step": 1974
    },
    {
      "epoch": 0.10553878216260988,
      "grad_norm": 1.5299516916275024,
      "learning_rate": 0.0001789394911267907,
      "loss": 2.4154,
      "step": 1975
    },
    {
      "epoch": 0.1055922195206669,
      "grad_norm": 1.4308596849441528,
      "learning_rate": 0.00017892880051314947,
      "loss": 2.3556,
      "step": 1976
    },
    {
      "epoch": 0.10564565687872392,
      "grad_norm": 1.1185121536254883,
      "learning_rate": 0.00017891810989950825,
      "loss": 2.2002,
      "step": 1977
    },
    {
      "epoch": 0.10569909423678094,
      "grad_norm": 1.304773211479187,
      "learning_rate": 0.00017890741928586703,
      "loss": 2.1551,
      "step": 1978
    },
    {
      "epoch": 0.10575253159483795,
      "grad_norm": 1.7763350009918213,
      "learning_rate": 0.0001788967286722258,
      "loss": 2.3099,
      "step": 1979
    },
    {
      "epoch": 0.10580596895289497,
      "grad_norm": 0.9112146496772766,
      "learning_rate": 0.00017888603805858456,
      "loss": 2.186,
      "step": 1980
    },
    {
      "epoch": 0.10585940631095199,
      "grad_norm": 1.5509247779846191,
      "learning_rate": 0.00017887534744494334,
      "loss": 2.2146,
      "step": 1981
    },
    {
      "epoch": 0.10591284366900901,
      "grad_norm": 1.4883887767791748,
      "learning_rate": 0.00017886465683130214,
      "loss": 2.2117,
      "step": 1982
    },
    {
      "epoch": 0.10596628102706603,
      "grad_norm": 1.6149460077285767,
      "learning_rate": 0.0001788539662176609,
      "loss": 2.4465,
      "step": 1983
    },
    {
      "epoch": 0.10601971838512304,
      "grad_norm": 1.7810109853744507,
      "learning_rate": 0.00017884327560401968,
      "loss": 2.0282,
      "step": 1984
    },
    {
      "epoch": 0.10607315574318006,
      "grad_norm": 1.1415228843688965,
      "learning_rate": 0.00017883258499037846,
      "loss": 2.2496,
      "step": 1985
    },
    {
      "epoch": 0.10612659310123708,
      "grad_norm": 1.1777652502059937,
      "learning_rate": 0.00017882189437673724,
      "loss": 2.0422,
      "step": 1986
    },
    {
      "epoch": 0.1061800304592941,
      "grad_norm": 1.1951900720596313,
      "learning_rate": 0.00017881120376309602,
      "loss": 2.1074,
      "step": 1987
    },
    {
      "epoch": 0.10623346781735112,
      "grad_norm": 1.7889233827590942,
      "learning_rate": 0.0001788005131494548,
      "loss": 2.3342,
      "step": 1988
    },
    {
      "epoch": 0.10628690517540813,
      "grad_norm": 1.5761164426803589,
      "learning_rate": 0.00017878982253581355,
      "loss": 2.128,
      "step": 1989
    },
    {
      "epoch": 0.10634034253346515,
      "grad_norm": 1.2711893320083618,
      "learning_rate": 0.00017877913192217235,
      "loss": 2.2884,
      "step": 1990
    },
    {
      "epoch": 0.10639377989152217,
      "grad_norm": 1.149791955947876,
      "learning_rate": 0.0001787684413085311,
      "loss": 2.1088,
      "step": 1991
    },
    {
      "epoch": 0.10644721724957919,
      "grad_norm": 1.6747952699661255,
      "learning_rate": 0.0001787577506948899,
      "loss": 2.102,
      "step": 1992
    },
    {
      "epoch": 0.1065006546076362,
      "grad_norm": 1.1299949884414673,
      "learning_rate": 0.00017874706008124867,
      "loss": 2.0214,
      "step": 1993
    },
    {
      "epoch": 0.10655409196569321,
      "grad_norm": 1.165697455406189,
      "learning_rate": 0.00017873636946760745,
      "loss": 2.3074,
      "step": 1994
    },
    {
      "epoch": 0.10660752932375023,
      "grad_norm": 1.3375128507614136,
      "learning_rate": 0.00017872567885396623,
      "loss": 2.232,
      "step": 1995
    },
    {
      "epoch": 0.10666096668180725,
      "grad_norm": 1.2422369718551636,
      "learning_rate": 0.000178714988240325,
      "loss": 2.1118,
      "step": 1996
    },
    {
      "epoch": 0.10671440403986426,
      "grad_norm": 1.2126742601394653,
      "learning_rate": 0.00017870429762668376,
      "loss": 2.2894,
      "step": 1997
    },
    {
      "epoch": 0.10676784139792128,
      "grad_norm": 0.8327566385269165,
      "learning_rate": 0.00017869360701304256,
      "loss": 2.0845,
      "step": 1998
    },
    {
      "epoch": 0.1068212787559783,
      "grad_norm": 1.2393385171890259,
      "learning_rate": 0.00017868291639940134,
      "loss": 2.1985,
      "step": 1999
    },
    {
      "epoch": 0.10687471611403532,
      "grad_norm": 1.1021534204483032,
      "learning_rate": 0.0001786722257857601,
      "loss": 2.1643,
      "step": 2000
    }
  ],
  "logging_steps": 1,
  "max_steps": 18713,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2637937787023872.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
