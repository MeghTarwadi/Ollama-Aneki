{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.02671867902850883,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 5.343735805701766e-05,
      "grad_norm": 11.970237731933594,
      "learning_rate": 4e-05,
      "loss": 7.7476,
      "step": 1
    },
    {
      "epoch": 0.00010687471611403532,
      "grad_norm": 10.701306343078613,
      "learning_rate": 8e-05,
      "loss": 7.9686,
      "step": 2
    },
    {
      "epoch": 0.00016031207417105297,
      "grad_norm": 11.011762619018555,
      "learning_rate": 0.00012,
      "loss": 7.7773,
      "step": 3
    },
    {
      "epoch": 0.00021374943222807064,
      "grad_norm": 9.025511741638184,
      "learning_rate": 0.00016,
      "loss": 7.3149,
      "step": 4
    },
    {
      "epoch": 0.0002671867902850883,
      "grad_norm": 13.776436805725098,
      "learning_rate": 0.0002,
      "loss": 6.8264,
      "step": 5
    },
    {
      "epoch": 0.00032062414834210595,
      "grad_norm": 12.221830368041992,
      "learning_rate": 0.0001999893093863588,
      "loss": 5.7921,
      "step": 6
    },
    {
      "epoch": 0.0003740615063991236,
      "grad_norm": 9.13806438446045,
      "learning_rate": 0.00019997861877271757,
      "loss": 5.5478,
      "step": 7
    },
    {
      "epoch": 0.0004274988644561413,
      "grad_norm": 12.186979293823242,
      "learning_rate": 0.00019996792815907635,
      "loss": 4.7754,
      "step": 8
    },
    {
      "epoch": 0.00048093622251315894,
      "grad_norm": 12.555595397949219,
      "learning_rate": 0.00019995723754543513,
      "loss": 4.6827,
      "step": 9
    },
    {
      "epoch": 0.0005343735805701766,
      "grad_norm": 18.793758392333984,
      "learning_rate": 0.00019994654693179388,
      "loss": 4.0134,
      "step": 10
    },
    {
      "epoch": 0.0005878109386271942,
      "grad_norm": 14.1957368850708,
      "learning_rate": 0.0001999358563181527,
      "loss": 3.7564,
      "step": 11
    },
    {
      "epoch": 0.0006412482966842119,
      "grad_norm": 13.329666137695312,
      "learning_rate": 0.00019992516570451144,
      "loss": 3.9015,
      "step": 12
    },
    {
      "epoch": 0.0006946856547412296,
      "grad_norm": 22.324115753173828,
      "learning_rate": 0.00019991447509087022,
      "loss": 3.82,
      "step": 13
    },
    {
      "epoch": 0.0007481230127982472,
      "grad_norm": 11.132011413574219,
      "learning_rate": 0.000199903784477229,
      "loss": 3.1148,
      "step": 14
    },
    {
      "epoch": 0.0008015603708552649,
      "grad_norm": 13.737192153930664,
      "learning_rate": 0.00019989309386358778,
      "loss": 3.2672,
      "step": 15
    },
    {
      "epoch": 0.0008549977289122826,
      "grad_norm": 8.938203811645508,
      "learning_rate": 0.00019988240324994656,
      "loss": 3.1601,
      "step": 16
    },
    {
      "epoch": 0.0009084350869693002,
      "grad_norm": 7.181746959686279,
      "learning_rate": 0.00019987171263630534,
      "loss": 2.8089,
      "step": 17
    },
    {
      "epoch": 0.0009618724450263179,
      "grad_norm": 6.332230567932129,
      "learning_rate": 0.0001998610220226641,
      "loss": 2.9769,
      "step": 18
    },
    {
      "epoch": 0.0010153098030833356,
      "grad_norm": 9.867545127868652,
      "learning_rate": 0.0001998503314090229,
      "loss": 2.9206,
      "step": 19
    },
    {
      "epoch": 0.0010687471611403531,
      "grad_norm": 4.944268703460693,
      "learning_rate": 0.00019983964079538168,
      "loss": 2.878,
      "step": 20
    },
    {
      "epoch": 0.0011221845191973709,
      "grad_norm": 3.3289639949798584,
      "learning_rate": 0.00019982895018174043,
      "loss": 2.6061,
      "step": 21
    },
    {
      "epoch": 0.0011756218772543884,
      "grad_norm": 3.9966819286346436,
      "learning_rate": 0.0001998182595680992,
      "loss": 2.4531,
      "step": 22
    },
    {
      "epoch": 0.0012290592353114062,
      "grad_norm": 3.5843348503112793,
      "learning_rate": 0.000199807568954458,
      "loss": 2.5386,
      "step": 23
    },
    {
      "epoch": 0.0012824965933684238,
      "grad_norm": 3.203960418701172,
      "learning_rate": 0.00019979687834081677,
      "loss": 2.5833,
      "step": 24
    },
    {
      "epoch": 0.0013359339514254416,
      "grad_norm": 4.620530128479004,
      "learning_rate": 0.00019978618772717555,
      "loss": 2.7913,
      "step": 25
    },
    {
      "epoch": 0.0013893713094824591,
      "grad_norm": 3.376312732696533,
      "learning_rate": 0.00019977549711353433,
      "loss": 2.7973,
      "step": 26
    },
    {
      "epoch": 0.0014428086675394769,
      "grad_norm": 3.6215131282806396,
      "learning_rate": 0.0001997648064998931,
      "loss": 2.6454,
      "step": 27
    },
    {
      "epoch": 0.0014962460255964944,
      "grad_norm": 3.8309831619262695,
      "learning_rate": 0.0001997541158862519,
      "loss": 2.6092,
      "step": 28
    },
    {
      "epoch": 0.0015496833836535122,
      "grad_norm": 4.693153381347656,
      "learning_rate": 0.00019974342527261064,
      "loss": 2.5944,
      "step": 29
    },
    {
      "epoch": 0.0016031207417105298,
      "grad_norm": 3.486189365386963,
      "learning_rate": 0.00019973273465896945,
      "loss": 2.4438,
      "step": 30
    },
    {
      "epoch": 0.0016565580997675476,
      "grad_norm": 3.6962625980377197,
      "learning_rate": 0.00019972204404532823,
      "loss": 2.7958,
      "step": 31
    },
    {
      "epoch": 0.0017099954578245651,
      "grad_norm": 2.6351699829101562,
      "learning_rate": 0.00019971135343168698,
      "loss": 2.5024,
      "step": 32
    },
    {
      "epoch": 0.0017634328158815829,
      "grad_norm": 2.681365966796875,
      "learning_rate": 0.00019970066281804576,
      "loss": 2.5983,
      "step": 33
    },
    {
      "epoch": 0.0018168701739386004,
      "grad_norm": 2.860957384109497,
      "learning_rate": 0.00019968997220440457,
      "loss": 2.5358,
      "step": 34
    },
    {
      "epoch": 0.0018703075319956182,
      "grad_norm": 2.709834337234497,
      "learning_rate": 0.00019967928159076332,
      "loss": 2.4489,
      "step": 35
    },
    {
      "epoch": 0.0019237448900526358,
      "grad_norm": 2.413336753845215,
      "learning_rate": 0.0001996685909771221,
      "loss": 2.7474,
      "step": 36
    },
    {
      "epoch": 0.0019771822481096536,
      "grad_norm": 2.816228151321411,
      "learning_rate": 0.00019965790036348088,
      "loss": 2.6311,
      "step": 37
    },
    {
      "epoch": 0.002030619606166671,
      "grad_norm": 1.9335052967071533,
      "learning_rate": 0.00019964720974983966,
      "loss": 2.4506,
      "step": 38
    },
    {
      "epoch": 0.0020840569642236887,
      "grad_norm": 2.1110637187957764,
      "learning_rate": 0.00019963651913619844,
      "loss": 2.4318,
      "step": 39
    },
    {
      "epoch": 0.0021374943222807062,
      "grad_norm": 3.6771485805511475,
      "learning_rate": 0.0001996258285225572,
      "loss": 2.5202,
      "step": 40
    },
    {
      "epoch": 0.0021909316803377242,
      "grad_norm": 2.4947004318237305,
      "learning_rate": 0.00019961513790891597,
      "loss": 2.3713,
      "step": 41
    },
    {
      "epoch": 0.0022443690383947418,
      "grad_norm": 3.3227171897888184,
      "learning_rate": 0.00019960444729527478,
      "loss": 2.4246,
      "step": 42
    },
    {
      "epoch": 0.0022978063964517593,
      "grad_norm": 3.305040121078491,
      "learning_rate": 0.00019959375668163353,
      "loss": 2.3929,
      "step": 43
    },
    {
      "epoch": 0.002351243754508777,
      "grad_norm": 3.740335464477539,
      "learning_rate": 0.0001995830660679923,
      "loss": 2.4188,
      "step": 44
    },
    {
      "epoch": 0.002404681112565795,
      "grad_norm": 2.857349157333374,
      "learning_rate": 0.0001995723754543511,
      "loss": 2.4117,
      "step": 45
    },
    {
      "epoch": 0.0024581184706228124,
      "grad_norm": 3.1033103466033936,
      "learning_rate": 0.00019956168484070987,
      "loss": 2.6588,
      "step": 46
    },
    {
      "epoch": 0.00251155582867983,
      "grad_norm": 3.260343551635742,
      "learning_rate": 0.00019955099422706865,
      "loss": 2.5025,
      "step": 47
    },
    {
      "epoch": 0.0025649931867368476,
      "grad_norm": 3.641688346862793,
      "learning_rate": 0.00019954030361342743,
      "loss": 2.4434,
      "step": 48
    },
    {
      "epoch": 0.0026184305447938656,
      "grad_norm": 2.715123176574707,
      "learning_rate": 0.00019952961299978618,
      "loss": 2.3227,
      "step": 49
    },
    {
      "epoch": 0.002671867902850883,
      "grad_norm": 2.881333112716675,
      "learning_rate": 0.00019951892238614499,
      "loss": 2.3485,
      "step": 50
    },
    {
      "epoch": 0.0027253052609079007,
      "grad_norm": 3.0994696617126465,
      "learning_rate": 0.00019950823177250374,
      "loss": 2.4613,
      "step": 51
    },
    {
      "epoch": 0.0027787426189649182,
      "grad_norm": 2.9128899574279785,
      "learning_rate": 0.00019949754115886252,
      "loss": 2.2512,
      "step": 52
    },
    {
      "epoch": 0.0028321799770219362,
      "grad_norm": 3.4108967781066895,
      "learning_rate": 0.00019948685054522132,
      "loss": 2.7673,
      "step": 53
    },
    {
      "epoch": 0.0028856173350789538,
      "grad_norm": 3.5357625484466553,
      "learning_rate": 0.00019947615993158008,
      "loss": 2.2592,
      "step": 54
    },
    {
      "epoch": 0.0029390546931359713,
      "grad_norm": 2.1669394969940186,
      "learning_rate": 0.00019946546931793886,
      "loss": 2.2161,
      "step": 55
    },
    {
      "epoch": 0.002992492051192989,
      "grad_norm": 2.566709041595459,
      "learning_rate": 0.00019945477870429764,
      "loss": 2.3943,
      "step": 56
    },
    {
      "epoch": 0.003045929409250007,
      "grad_norm": 2.169023036956787,
      "learning_rate": 0.00019944408809065642,
      "loss": 2.4839,
      "step": 57
    },
    {
      "epoch": 0.0030993667673070244,
      "grad_norm": 2.2498955726623535,
      "learning_rate": 0.0001994333974770152,
      "loss": 2.1503,
      "step": 58
    },
    {
      "epoch": 0.003152804125364042,
      "grad_norm": 1.9470359086990356,
      "learning_rate": 0.00019942270686337398,
      "loss": 2.0655,
      "step": 59
    },
    {
      "epoch": 0.0032062414834210596,
      "grad_norm": 1.4817441701889038,
      "learning_rate": 0.00019941201624973273,
      "loss": 2.1665,
      "step": 60
    },
    {
      "epoch": 0.003259678841478077,
      "grad_norm": 2.762702703475952,
      "learning_rate": 0.00019940132563609153,
      "loss": 2.6132,
      "step": 61
    },
    {
      "epoch": 0.003313116199535095,
      "grad_norm": 1.7849444150924683,
      "learning_rate": 0.00019939063502245031,
      "loss": 2.1542,
      "step": 62
    },
    {
      "epoch": 0.0033665535575921127,
      "grad_norm": 2.055856466293335,
      "learning_rate": 0.00019937994440880907,
      "loss": 2.2057,
      "step": 63
    },
    {
      "epoch": 0.0034199909156491302,
      "grad_norm": 2.6720077991485596,
      "learning_rate": 0.00019936925379516785,
      "loss": 2.3751,
      "step": 64
    },
    {
      "epoch": 0.0034734282737061478,
      "grad_norm": 2.4066174030303955,
      "learning_rate": 0.00019935856318152663,
      "loss": 2.3271,
      "step": 65
    },
    {
      "epoch": 0.0035268656317631658,
      "grad_norm": 2.374605178833008,
      "learning_rate": 0.0001993478725678854,
      "loss": 2.3435,
      "step": 66
    },
    {
      "epoch": 0.0035803029898201833,
      "grad_norm": 2.8954312801361084,
      "learning_rate": 0.00019933718195424419,
      "loss": 2.404,
      "step": 67
    },
    {
      "epoch": 0.003633740347877201,
      "grad_norm": 4.533381938934326,
      "learning_rate": 0.00019932649134060294,
      "loss": 2.6031,
      "step": 68
    },
    {
      "epoch": 0.0036871777059342184,
      "grad_norm": 1.7986892461776733,
      "learning_rate": 0.00019931580072696174,
      "loss": 2.1513,
      "step": 69
    },
    {
      "epoch": 0.0037406150639912364,
      "grad_norm": 3.086799383163452,
      "learning_rate": 0.00019930511011332052,
      "loss": 2.695,
      "step": 70
    },
    {
      "epoch": 0.003794052422048254,
      "grad_norm": 2.4031572341918945,
      "learning_rate": 0.00019929441949967928,
      "loss": 2.4092,
      "step": 71
    },
    {
      "epoch": 0.0038474897801052716,
      "grad_norm": 2.7464113235473633,
      "learning_rate": 0.00019928372888603808,
      "loss": 2.2453,
      "step": 72
    },
    {
      "epoch": 0.003900927138162289,
      "grad_norm": 4.422228813171387,
      "learning_rate": 0.00019927303827239686,
      "loss": 2.3859,
      "step": 73
    },
    {
      "epoch": 0.003954364496219307,
      "grad_norm": 2.501262903213501,
      "learning_rate": 0.00019926234765875562,
      "loss": 2.2071,
      "step": 74
    },
    {
      "epoch": 0.004007801854276324,
      "grad_norm": 2.0280261039733887,
      "learning_rate": 0.0001992516570451144,
      "loss": 2.1383,
      "step": 75
    },
    {
      "epoch": 0.004061239212333342,
      "grad_norm": 1.853374719619751,
      "learning_rate": 0.00019924096643147317,
      "loss": 2.4608,
      "step": 76
    },
    {
      "epoch": 0.00411467657039036,
      "grad_norm": 1.887791395187378,
      "learning_rate": 0.00019923027581783195,
      "loss": 2.2432,
      "step": 77
    },
    {
      "epoch": 0.004168113928447377,
      "grad_norm": 2.1457149982452393,
      "learning_rate": 0.00019921958520419073,
      "loss": 2.44,
      "step": 78
    },
    {
      "epoch": 0.004221551286504395,
      "grad_norm": 2.7834243774414062,
      "learning_rate": 0.0001992088945905495,
      "loss": 2.3315,
      "step": 79
    },
    {
      "epoch": 0.0042749886445614125,
      "grad_norm": 2.035062074661255,
      "learning_rate": 0.0001991982039769083,
      "loss": 2.1417,
      "step": 80
    },
    {
      "epoch": 0.0043284260026184304,
      "grad_norm": 2.0203514099121094,
      "learning_rate": 0.00019918751336326707,
      "loss": 2.4737,
      "step": 81
    },
    {
      "epoch": 0.0043818633606754484,
      "grad_norm": 2.110635757446289,
      "learning_rate": 0.00019917682274962583,
      "loss": 2.1977,
      "step": 82
    },
    {
      "epoch": 0.0044353007187324656,
      "grad_norm": 2.089613199234009,
      "learning_rate": 0.0001991661321359846,
      "loss": 2.3423,
      "step": 83
    },
    {
      "epoch": 0.0044887380767894836,
      "grad_norm": 2.3507418632507324,
      "learning_rate": 0.0001991554415223434,
      "loss": 2.4084,
      "step": 84
    },
    {
      "epoch": 0.0045421754348465015,
      "grad_norm": 3.0323617458343506,
      "learning_rate": 0.00019914475090870216,
      "loss": 2.1922,
      "step": 85
    },
    {
      "epoch": 0.004595612792903519,
      "grad_norm": 2.4876773357391357,
      "learning_rate": 0.00019913406029506094,
      "loss": 2.3589,
      "step": 86
    },
    {
      "epoch": 0.004649050150960537,
      "grad_norm": 2.5492403507232666,
      "learning_rate": 0.00019912336968141972,
      "loss": 2.4409,
      "step": 87
    },
    {
      "epoch": 0.004702487509017554,
      "grad_norm": 2.1316730976104736,
      "learning_rate": 0.0001991126790677785,
      "loss": 2.2339,
      "step": 88
    },
    {
      "epoch": 0.004755924867074572,
      "grad_norm": 2.2884395122528076,
      "learning_rate": 0.00019910198845413728,
      "loss": 2.2245,
      "step": 89
    },
    {
      "epoch": 0.00480936222513159,
      "grad_norm": 2.0484306812286377,
      "learning_rate": 0.00019909129784049606,
      "loss": 2.5584,
      "step": 90
    },
    {
      "epoch": 0.004862799583188607,
      "grad_norm": 4.033507823944092,
      "learning_rate": 0.00019908060722685482,
      "loss": 2.5435,
      "step": 91
    },
    {
      "epoch": 0.004916236941245625,
      "grad_norm": 3.3569581508636475,
      "learning_rate": 0.00019906991661321362,
      "loss": 2.7566,
      "step": 92
    },
    {
      "epoch": 0.004969674299302643,
      "grad_norm": 1.7402949333190918,
      "learning_rate": 0.00019905922599957237,
      "loss": 1.9427,
      "step": 93
    },
    {
      "epoch": 0.00502311165735966,
      "grad_norm": 2.9608747959136963,
      "learning_rate": 0.00019904853538593115,
      "loss": 2.3295,
      "step": 94
    },
    {
      "epoch": 0.005076549015416678,
      "grad_norm": 2.370903491973877,
      "learning_rate": 0.00019903784477228996,
      "loss": 2.2999,
      "step": 95
    },
    {
      "epoch": 0.005129986373473695,
      "grad_norm": 2.3415687084198,
      "learning_rate": 0.00019902715415864871,
      "loss": 2.3111,
      "step": 96
    },
    {
      "epoch": 0.005183423731530713,
      "grad_norm": 1.7979177236557007,
      "learning_rate": 0.0001990164635450075,
      "loss": 2.3156,
      "step": 97
    },
    {
      "epoch": 0.005236861089587731,
      "grad_norm": 3.4317069053649902,
      "learning_rate": 0.00019900577293136627,
      "loss": 2.6324,
      "step": 98
    },
    {
      "epoch": 0.005290298447644748,
      "grad_norm": 2.148063898086548,
      "learning_rate": 0.00019899508231772505,
      "loss": 2.3485,
      "step": 99
    },
    {
      "epoch": 0.005343735805701766,
      "grad_norm": 3.534181833267212,
      "learning_rate": 0.00019898439170408383,
      "loss": 2.5574,
      "step": 100
    },
    {
      "epoch": 0.005397173163758783,
      "grad_norm": 2.7186295986175537,
      "learning_rate": 0.0001989737010904426,
      "loss": 2.2784,
      "step": 101
    },
    {
      "epoch": 0.005450610521815801,
      "grad_norm": 1.6162762641906738,
      "learning_rate": 0.00019896301047680136,
      "loss": 2.0946,
      "step": 102
    },
    {
      "epoch": 0.005504047879872819,
      "grad_norm": 1.6976977586746216,
      "learning_rate": 0.00019895231986316017,
      "loss": 2.2792,
      "step": 103
    },
    {
      "epoch": 0.0055574852379298365,
      "grad_norm": 1.8707302808761597,
      "learning_rate": 0.00019894162924951892,
      "loss": 2.3543,
      "step": 104
    },
    {
      "epoch": 0.0056109225959868544,
      "grad_norm": 2.0895769596099854,
      "learning_rate": 0.0001989309386358777,
      "loss": 2.3004,
      "step": 105
    },
    {
      "epoch": 0.0056643599540438724,
      "grad_norm": 2.0797436237335205,
      "learning_rate": 0.00019892024802223648,
      "loss": 2.2402,
      "step": 106
    },
    {
      "epoch": 0.0057177973121008896,
      "grad_norm": 2.4910085201263428,
      "learning_rate": 0.00019890955740859526,
      "loss": 2.3601,
      "step": 107
    },
    {
      "epoch": 0.0057712346701579076,
      "grad_norm": 2.357196807861328,
      "learning_rate": 0.00019889886679495404,
      "loss": 2.3339,
      "step": 108
    },
    {
      "epoch": 0.005824672028214925,
      "grad_norm": 1.7829447984695435,
      "learning_rate": 0.00019888817618131282,
      "loss": 2.2579,
      "step": 109
    },
    {
      "epoch": 0.005878109386271943,
      "grad_norm": 1.9549471139907837,
      "learning_rate": 0.00019887748556767157,
      "loss": 2.2803,
      "step": 110
    },
    {
      "epoch": 0.005931546744328961,
      "grad_norm": 2.221635341644287,
      "learning_rate": 0.00019886679495403038,
      "loss": 2.3409,
      "step": 111
    },
    {
      "epoch": 0.005984984102385978,
      "grad_norm": 2.5541090965270996,
      "learning_rate": 0.00019885610434038916,
      "loss": 2.4929,
      "step": 112
    },
    {
      "epoch": 0.006038421460442996,
      "grad_norm": 2.1521177291870117,
      "learning_rate": 0.0001988454137267479,
      "loss": 2.3489,
      "step": 113
    },
    {
      "epoch": 0.006091858818500014,
      "grad_norm": 2.678241014480591,
      "learning_rate": 0.0001988347231131067,
      "loss": 2.2437,
      "step": 114
    },
    {
      "epoch": 0.006145296176557031,
      "grad_norm": 2.028127908706665,
      "learning_rate": 0.00019882403249946547,
      "loss": 2.2929,
      "step": 115
    },
    {
      "epoch": 0.006198733534614049,
      "grad_norm": 2.691897392272949,
      "learning_rate": 0.00019881334188582425,
      "loss": 2.3493,
      "step": 116
    },
    {
      "epoch": 0.006252170892671066,
      "grad_norm": 2.0189437866210938,
      "learning_rate": 0.00019880265127218303,
      "loss": 2.2464,
      "step": 117
    },
    {
      "epoch": 0.006305608250728084,
      "grad_norm": 3.2763943672180176,
      "learning_rate": 0.0001987919606585418,
      "loss": 2.5909,
      "step": 118
    },
    {
      "epoch": 0.006359045608785102,
      "grad_norm": 2.2224647998809814,
      "learning_rate": 0.0001987812700449006,
      "loss": 2.4513,
      "step": 119
    },
    {
      "epoch": 0.006412482966842119,
      "grad_norm": 1.892526388168335,
      "learning_rate": 0.00019877057943125937,
      "loss": 2.3511,
      "step": 120
    },
    {
      "epoch": 0.006465920324899137,
      "grad_norm": 3.0634074211120605,
      "learning_rate": 0.00019875988881761812,
      "loss": 2.5005,
      "step": 121
    },
    {
      "epoch": 0.006519357682956154,
      "grad_norm": 3.1696016788482666,
      "learning_rate": 0.00019874919820397693,
      "loss": 2.5338,
      "step": 122
    },
    {
      "epoch": 0.006572795041013172,
      "grad_norm": 2.0189437866210938,
      "learning_rate": 0.0001987385075903357,
      "loss": 2.2577,
      "step": 123
    },
    {
      "epoch": 0.00662623239907019,
      "grad_norm": 2.5295908451080322,
      "learning_rate": 0.00019872781697669446,
      "loss": 2.4013,
      "step": 124
    },
    {
      "epoch": 0.006679669757127207,
      "grad_norm": 2.1718509197235107,
      "learning_rate": 0.00019871712636305324,
      "loss": 2.1857,
      "step": 125
    },
    {
      "epoch": 0.006733107115184225,
      "grad_norm": 2.6861414909362793,
      "learning_rate": 0.00019870643574941202,
      "loss": 2.6086,
      "step": 126
    },
    {
      "epoch": 0.006786544473241243,
      "grad_norm": 1.8559672832489014,
      "learning_rate": 0.0001986957451357708,
      "loss": 2.2779,
      "step": 127
    },
    {
      "epoch": 0.0068399818312982604,
      "grad_norm": 1.7880313396453857,
      "learning_rate": 0.00019868505452212958,
      "loss": 2.555,
      "step": 128
    },
    {
      "epoch": 0.0068934191893552784,
      "grad_norm": 2.1239988803863525,
      "learning_rate": 0.00019867436390848836,
      "loss": 2.31,
      "step": 129
    },
    {
      "epoch": 0.0069468565474122956,
      "grad_norm": 1.6436840295791626,
      "learning_rate": 0.00019866367329484714,
      "loss": 2.255,
      "step": 130
    },
    {
      "epoch": 0.0070002939054693136,
      "grad_norm": 1.931606411933899,
      "learning_rate": 0.00019865298268120592,
      "loss": 2.3587,
      "step": 131
    },
    {
      "epoch": 0.0070537312635263315,
      "grad_norm": 2.388674259185791,
      "learning_rate": 0.00019864229206756467,
      "loss": 2.4263,
      "step": 132
    },
    {
      "epoch": 0.007107168621583349,
      "grad_norm": 2.4609148502349854,
      "learning_rate": 0.00019863160145392345,
      "loss": 2.0797,
      "step": 133
    },
    {
      "epoch": 0.007160605979640367,
      "grad_norm": 1.9375379085540771,
      "learning_rate": 0.00019862091084028226,
      "loss": 2.4952,
      "step": 134
    },
    {
      "epoch": 0.007214043337697385,
      "grad_norm": 2.3791916370391846,
      "learning_rate": 0.000198610220226641,
      "loss": 2.4797,
      "step": 135
    },
    {
      "epoch": 0.007267480695754402,
      "grad_norm": 2.1189119815826416,
      "learning_rate": 0.0001985995296129998,
      "loss": 2.6165,
      "step": 136
    },
    {
      "epoch": 0.00732091805381142,
      "grad_norm": 2.1463654041290283,
      "learning_rate": 0.00019858883899935857,
      "loss": 2.2578,
      "step": 137
    },
    {
      "epoch": 0.007374355411868437,
      "grad_norm": 1.3656697273254395,
      "learning_rate": 0.00019857814838571735,
      "loss": 2.09,
      "step": 138
    },
    {
      "epoch": 0.007427792769925455,
      "grad_norm": 3.0640625953674316,
      "learning_rate": 0.00019856745777207613,
      "loss": 2.5065,
      "step": 139
    },
    {
      "epoch": 0.007481230127982473,
      "grad_norm": 1.8807659149169922,
      "learning_rate": 0.0001985567671584349,
      "loss": 2.4108,
      "step": 140
    },
    {
      "epoch": 0.00753466748603949,
      "grad_norm": 1.5126312971115112,
      "learning_rate": 0.0001985460765447937,
      "loss": 2.1425,
      "step": 141
    },
    {
      "epoch": 0.007588104844096508,
      "grad_norm": 1.6577357053756714,
      "learning_rate": 0.00019853538593115247,
      "loss": 2.4614,
      "step": 142
    },
    {
      "epoch": 0.007641542202153525,
      "grad_norm": 1.877882480621338,
      "learning_rate": 0.00019852469531751122,
      "loss": 2.2686,
      "step": 143
    },
    {
      "epoch": 0.007694979560210543,
      "grad_norm": 1.9157358407974243,
      "learning_rate": 0.00019851400470387,
      "loss": 2.0299,
      "step": 144
    },
    {
      "epoch": 0.007748416918267561,
      "grad_norm": 1.752771258354187,
      "learning_rate": 0.0001985033140902288,
      "loss": 2.3542,
      "step": 145
    },
    {
      "epoch": 0.007801854276324578,
      "grad_norm": 2.1627349853515625,
      "learning_rate": 0.00019849262347658756,
      "loss": 2.2514,
      "step": 146
    },
    {
      "epoch": 0.007855291634381596,
      "grad_norm": 1.1296221017837524,
      "learning_rate": 0.00019848193286294634,
      "loss": 2.0682,
      "step": 147
    },
    {
      "epoch": 0.007908728992438614,
      "grad_norm": 1.6073658466339111,
      "learning_rate": 0.00019847124224930512,
      "loss": 2.1776,
      "step": 148
    },
    {
      "epoch": 0.007962166350495632,
      "grad_norm": 1.6716794967651367,
      "learning_rate": 0.0001984605516356639,
      "loss": 2.4486,
      "step": 149
    },
    {
      "epoch": 0.008015603708552648,
      "grad_norm": 1.9482555389404297,
      "learning_rate": 0.00019844986102202268,
      "loss": 2.3839,
      "step": 150
    },
    {
      "epoch": 0.008069041066609666,
      "grad_norm": 1.4163340330123901,
      "learning_rate": 0.00019843917040838146,
      "loss": 2.1088,
      "step": 151
    },
    {
      "epoch": 0.008122478424666684,
      "grad_norm": 1.71939218044281,
      "learning_rate": 0.0001984284797947402,
      "loss": 2.2511,
      "step": 152
    },
    {
      "epoch": 0.008175915782723702,
      "grad_norm": 1.5992496013641357,
      "learning_rate": 0.00019841778918109902,
      "loss": 2.4163,
      "step": 153
    },
    {
      "epoch": 0.00822935314078072,
      "grad_norm": 1.8481218814849854,
      "learning_rate": 0.00019840709856745777,
      "loss": 2.271,
      "step": 154
    },
    {
      "epoch": 0.008282790498837737,
      "grad_norm": 1.4616034030914307,
      "learning_rate": 0.00019839640795381655,
      "loss": 2.1638,
      "step": 155
    },
    {
      "epoch": 0.008336227856894755,
      "grad_norm": 2.3104922771453857,
      "learning_rate": 0.00019838571734017533,
      "loss": 2.3733,
      "step": 156
    },
    {
      "epoch": 0.008389665214951773,
      "grad_norm": 1.8859935998916626,
      "learning_rate": 0.0001983750267265341,
      "loss": 2.0947,
      "step": 157
    },
    {
      "epoch": 0.00844310257300879,
      "grad_norm": 1.828004002571106,
      "learning_rate": 0.0001983643361128929,
      "loss": 2.0471,
      "step": 158
    },
    {
      "epoch": 0.008496539931065809,
      "grad_norm": 2.4341464042663574,
      "learning_rate": 0.00019835364549925167,
      "loss": 2.2798,
      "step": 159
    },
    {
      "epoch": 0.008549977289122825,
      "grad_norm": 2.6948931217193604,
      "learning_rate": 0.00019834295488561042,
      "loss": 2.4895,
      "step": 160
    },
    {
      "epoch": 0.008603414647179843,
      "grad_norm": 1.510308027267456,
      "learning_rate": 0.00019833226427196923,
      "loss": 2.2279,
      "step": 161
    },
    {
      "epoch": 0.008656852005236861,
      "grad_norm": 1.9699573516845703,
      "learning_rate": 0.000198321573658328,
      "loss": 2.2115,
      "step": 162
    },
    {
      "epoch": 0.008710289363293879,
      "grad_norm": 1.791363000869751,
      "learning_rate": 0.00019831088304468676,
      "loss": 2.0016,
      "step": 163
    },
    {
      "epoch": 0.008763726721350897,
      "grad_norm": 2.740567684173584,
      "learning_rate": 0.00019830019243104557,
      "loss": 2.2513,
      "step": 164
    },
    {
      "epoch": 0.008817164079407915,
      "grad_norm": 1.9829981327056885,
      "learning_rate": 0.00019828950181740432,
      "loss": 2.2649,
      "step": 165
    },
    {
      "epoch": 0.008870601437464931,
      "grad_norm": 2.47855544090271,
      "learning_rate": 0.0001982788112037631,
      "loss": 2.4758,
      "step": 166
    },
    {
      "epoch": 0.008924038795521949,
      "grad_norm": 2.771904706954956,
      "learning_rate": 0.00019826812059012188,
      "loss": 2.3403,
      "step": 167
    },
    {
      "epoch": 0.008977476153578967,
      "grad_norm": 1.563524842262268,
      "learning_rate": 0.00019825742997648066,
      "loss": 2.219,
      "step": 168
    },
    {
      "epoch": 0.009030913511635985,
      "grad_norm": 2.464757204055786,
      "learning_rate": 0.00019824673936283944,
      "loss": 2.3529,
      "step": 169
    },
    {
      "epoch": 0.009084350869693003,
      "grad_norm": 2.588552474975586,
      "learning_rate": 0.00019823604874919822,
      "loss": 2.5771,
      "step": 170
    },
    {
      "epoch": 0.00913778822775002,
      "grad_norm": 2.417567253112793,
      "learning_rate": 0.00019822535813555697,
      "loss": 2.0823,
      "step": 171
    },
    {
      "epoch": 0.009191225585807037,
      "grad_norm": 1.8058382272720337,
      "learning_rate": 0.00019821466752191578,
      "loss": 2.3618,
      "step": 172
    },
    {
      "epoch": 0.009244662943864055,
      "grad_norm": 2.4998278617858887,
      "learning_rate": 0.00019820397690827456,
      "loss": 2.6167,
      "step": 173
    },
    {
      "epoch": 0.009298100301921073,
      "grad_norm": 2.038851261138916,
      "learning_rate": 0.0001981932862946333,
      "loss": 2.3543,
      "step": 174
    },
    {
      "epoch": 0.009351537659978091,
      "grad_norm": 2.487422466278076,
      "learning_rate": 0.0001981825956809921,
      "loss": 2.068,
      "step": 175
    },
    {
      "epoch": 0.009404975018035108,
      "grad_norm": 1.7561699151992798,
      "learning_rate": 0.0001981719050673509,
      "loss": 2.0829,
      "step": 176
    },
    {
      "epoch": 0.009458412376092126,
      "grad_norm": 1.5048489570617676,
      "learning_rate": 0.00019816121445370965,
      "loss": 2.1128,
      "step": 177
    },
    {
      "epoch": 0.009511849734149144,
      "grad_norm": 1.6518044471740723,
      "learning_rate": 0.00019815052384006843,
      "loss": 2.2952,
      "step": 178
    },
    {
      "epoch": 0.009565287092206162,
      "grad_norm": 1.6791489124298096,
      "learning_rate": 0.0001981398332264272,
      "loss": 2.2058,
      "step": 179
    },
    {
      "epoch": 0.00961872445026318,
      "grad_norm": 1.806159496307373,
      "learning_rate": 0.000198129142612786,
      "loss": 2.2239,
      "step": 180
    },
    {
      "epoch": 0.009672161808320196,
      "grad_norm": 2.2939858436584473,
      "learning_rate": 0.00019811845199914477,
      "loss": 2.2677,
      "step": 181
    },
    {
      "epoch": 0.009725599166377214,
      "grad_norm": 1.5588476657867432,
      "learning_rate": 0.00019810776138550352,
      "loss": 2.2894,
      "step": 182
    },
    {
      "epoch": 0.009779036524434232,
      "grad_norm": 2.135014295578003,
      "learning_rate": 0.00019809707077186233,
      "loss": 2.3482,
      "step": 183
    },
    {
      "epoch": 0.00983247388249125,
      "grad_norm": 3.130028486251831,
      "learning_rate": 0.0001980863801582211,
      "loss": 2.4595,
      "step": 184
    },
    {
      "epoch": 0.009885911240548268,
      "grad_norm": 2.3617007732391357,
      "learning_rate": 0.00019807568954457986,
      "loss": 2.2736,
      "step": 185
    },
    {
      "epoch": 0.009939348598605286,
      "grad_norm": 1.951005220413208,
      "learning_rate": 0.00019806499893093864,
      "loss": 2.3125,
      "step": 186
    },
    {
      "epoch": 0.009992785956662302,
      "grad_norm": 1.8079955577850342,
      "learning_rate": 0.00019805430831729744,
      "loss": 2.2513,
      "step": 187
    },
    {
      "epoch": 0.01004622331471932,
      "grad_norm": 1.51261305809021,
      "learning_rate": 0.0001980436177036562,
      "loss": 2.1954,
      "step": 188
    },
    {
      "epoch": 0.010099660672776338,
      "grad_norm": 1.9357469081878662,
      "learning_rate": 0.00019803292709001498,
      "loss": 2.267,
      "step": 189
    },
    {
      "epoch": 0.010153098030833356,
      "grad_norm": 1.9593473672866821,
      "learning_rate": 0.00019802223647637376,
      "loss": 2.2414,
      "step": 190
    },
    {
      "epoch": 0.010206535388890374,
      "grad_norm": 1.5358587503433228,
      "learning_rate": 0.00019801154586273254,
      "loss": 2.2082,
      "step": 191
    },
    {
      "epoch": 0.01025997274694739,
      "grad_norm": 1.7860612869262695,
      "learning_rate": 0.00019800085524909132,
      "loss": 2.2528,
      "step": 192
    },
    {
      "epoch": 0.010313410105004408,
      "grad_norm": 1.4999393224716187,
      "learning_rate": 0.00019799016463545007,
      "loss": 2.1751,
      "step": 193
    },
    {
      "epoch": 0.010366847463061426,
      "grad_norm": 1.9045521020889282,
      "learning_rate": 0.00019797947402180885,
      "loss": 1.9488,
      "step": 194
    },
    {
      "epoch": 0.010420284821118444,
      "grad_norm": 1.627297282218933,
      "learning_rate": 0.00019796878340816766,
      "loss": 2.211,
      "step": 195
    },
    {
      "epoch": 0.010473722179175462,
      "grad_norm": 1.5917466878890991,
      "learning_rate": 0.0001979580927945264,
      "loss": 2.1858,
      "step": 196
    },
    {
      "epoch": 0.010527159537232478,
      "grad_norm": 1.4751759767532349,
      "learning_rate": 0.0001979474021808852,
      "loss": 2.0659,
      "step": 197
    },
    {
      "epoch": 0.010580596895289496,
      "grad_norm": 1.794964075088501,
      "learning_rate": 0.00019793671156724397,
      "loss": 2.1209,
      "step": 198
    },
    {
      "epoch": 0.010634034253346514,
      "grad_norm": 1.8415664434432983,
      "learning_rate": 0.00019792602095360275,
      "loss": 2.3496,
      "step": 199
    },
    {
      "epoch": 0.010687471611403532,
      "grad_norm": 1.5526354312896729,
      "learning_rate": 0.00019791533033996153,
      "loss": 2.0418,
      "step": 200
    },
    {
      "epoch": 0.01074090896946055,
      "grad_norm": 1.6219415664672852,
      "learning_rate": 0.0001979046397263203,
      "loss": 2.1485,
      "step": 201
    },
    {
      "epoch": 0.010794346327517567,
      "grad_norm": 2.909191846847534,
      "learning_rate": 0.00019789394911267906,
      "loss": 2.3218,
      "step": 202
    },
    {
      "epoch": 0.010847783685574585,
      "grad_norm": 1.991490364074707,
      "learning_rate": 0.00019788325849903787,
      "loss": 2.1348,
      "step": 203
    },
    {
      "epoch": 0.010901221043631603,
      "grad_norm": 1.8083890676498413,
      "learning_rate": 0.00019787256788539664,
      "loss": 2.2096,
      "step": 204
    },
    {
      "epoch": 0.01095465840168862,
      "grad_norm": 2.298269510269165,
      "learning_rate": 0.0001978618772717554,
      "loss": 2.0963,
      "step": 205
    },
    {
      "epoch": 0.011008095759745639,
      "grad_norm": 2.2239108085632324,
      "learning_rate": 0.0001978511866581142,
      "loss": 2.2159,
      "step": 206
    },
    {
      "epoch": 0.011061533117802657,
      "grad_norm": 2.3935747146606445,
      "learning_rate": 0.00019784049604447296,
      "loss": 2.4161,
      "step": 207
    },
    {
      "epoch": 0.011114970475859673,
      "grad_norm": 1.877626895904541,
      "learning_rate": 0.00019782980543083174,
      "loss": 2.3907,
      "step": 208
    },
    {
      "epoch": 0.011168407833916691,
      "grad_norm": 2.1585710048675537,
      "learning_rate": 0.00019781911481719052,
      "loss": 2.2906,
      "step": 209
    },
    {
      "epoch": 0.011221845191973709,
      "grad_norm": 2.2746617794036865,
      "learning_rate": 0.0001978084242035493,
      "loss": 2.5741,
      "step": 210
    },
    {
      "epoch": 0.011275282550030727,
      "grad_norm": 2.2482640743255615,
      "learning_rate": 0.00019779773358990808,
      "loss": 2.2525,
      "step": 211
    },
    {
      "epoch": 0.011328719908087745,
      "grad_norm": 2.741502285003662,
      "learning_rate": 0.00019778704297626685,
      "loss": 2.3312,
      "step": 212
    },
    {
      "epoch": 0.011382157266144761,
      "grad_norm": 1.865246295928955,
      "learning_rate": 0.0001977763523626256,
      "loss": 2.2199,
      "step": 213
    },
    {
      "epoch": 0.011435594624201779,
      "grad_norm": 1.6434811353683472,
      "learning_rate": 0.00019776566174898441,
      "loss": 2.0947,
      "step": 214
    },
    {
      "epoch": 0.011489031982258797,
      "grad_norm": 2.6523470878601074,
      "learning_rate": 0.0001977549711353432,
      "loss": 2.1767,
      "step": 215
    },
    {
      "epoch": 0.011542469340315815,
      "grad_norm": 1.6652511358261108,
      "learning_rate": 0.00019774428052170195,
      "loss": 2.2073,
      "step": 216
    },
    {
      "epoch": 0.011595906698372833,
      "grad_norm": 2.195754051208496,
      "learning_rate": 0.00019773358990806073,
      "loss": 2.3311,
      "step": 217
    },
    {
      "epoch": 0.01164934405642985,
      "grad_norm": 3.438298463821411,
      "learning_rate": 0.0001977228992944195,
      "loss": 2.5587,
      "step": 218
    },
    {
      "epoch": 0.011702781414486867,
      "grad_norm": 1.3485708236694336,
      "learning_rate": 0.00019771220868077829,
      "loss": 2.1136,
      "step": 219
    },
    {
      "epoch": 0.011756218772543885,
      "grad_norm": 3.2579479217529297,
      "learning_rate": 0.00019770151806713706,
      "loss": 2.2569,
      "step": 220
    },
    {
      "epoch": 0.011809656130600903,
      "grad_norm": 2.0421736240386963,
      "learning_rate": 0.00019769082745349582,
      "loss": 2.2771,
      "step": 221
    },
    {
      "epoch": 0.011863093488657921,
      "grad_norm": 2.9052767753601074,
      "learning_rate": 0.00019768013683985462,
      "loss": 2.5058,
      "step": 222
    },
    {
      "epoch": 0.011916530846714938,
      "grad_norm": 1.5456726551055908,
      "learning_rate": 0.0001976694462262134,
      "loss": 2.2721,
      "step": 223
    },
    {
      "epoch": 0.011969968204771956,
      "grad_norm": 1.706896185874939,
      "learning_rate": 0.00019765875561257216,
      "loss": 2.2572,
      "step": 224
    },
    {
      "epoch": 0.012023405562828974,
      "grad_norm": 1.8504962921142578,
      "learning_rate": 0.00019764806499893094,
      "loss": 2.1956,
      "step": 225
    },
    {
      "epoch": 0.012076842920885992,
      "grad_norm": 1.4890812635421753,
      "learning_rate": 0.00019763737438528974,
      "loss": 2.2131,
      "step": 226
    },
    {
      "epoch": 0.01213028027894301,
      "grad_norm": 1.6949875354766846,
      "learning_rate": 0.0001976266837716485,
      "loss": 2.1467,
      "step": 227
    },
    {
      "epoch": 0.012183717637000028,
      "grad_norm": 1.5813792943954468,
      "learning_rate": 0.00019761599315800728,
      "loss": 1.8115,
      "step": 228
    },
    {
      "epoch": 0.012237154995057044,
      "grad_norm": 2.0067131519317627,
      "learning_rate": 0.00019760530254436605,
      "loss": 2.3508,
      "step": 229
    },
    {
      "epoch": 0.012290592353114062,
      "grad_norm": 1.1246662139892578,
      "learning_rate": 0.00019759461193072483,
      "loss": 2.1677,
      "step": 230
    },
    {
      "epoch": 0.01234402971117108,
      "grad_norm": 1.7800709009170532,
      "learning_rate": 0.00019758392131708361,
      "loss": 2.3487,
      "step": 231
    },
    {
      "epoch": 0.012397467069228098,
      "grad_norm": 1.4524718523025513,
      "learning_rate": 0.0001975732307034424,
      "loss": 2.1984,
      "step": 232
    },
    {
      "epoch": 0.012450904427285116,
      "grad_norm": 2.1085140705108643,
      "learning_rate": 0.00019756254008980117,
      "loss": 2.4997,
      "step": 233
    },
    {
      "epoch": 0.012504341785342132,
      "grad_norm": 1.248300313949585,
      "learning_rate": 0.00019755184947615995,
      "loss": 2.2101,
      "step": 234
    },
    {
      "epoch": 0.01255777914339915,
      "grad_norm": 2.640291929244995,
      "learning_rate": 0.0001975411588625187,
      "loss": 2.6063,
      "step": 235
    },
    {
      "epoch": 0.012611216501456168,
      "grad_norm": 1.8572418689727783,
      "learning_rate": 0.00019753046824887749,
      "loss": 2.2879,
      "step": 236
    },
    {
      "epoch": 0.012664653859513186,
      "grad_norm": 1.9282485246658325,
      "learning_rate": 0.0001975197776352363,
      "loss": 2.0593,
      "step": 237
    },
    {
      "epoch": 0.012718091217570204,
      "grad_norm": 1.3157105445861816,
      "learning_rate": 0.00019750908702159504,
      "loss": 2.2296,
      "step": 238
    },
    {
      "epoch": 0.01277152857562722,
      "grad_norm": 1.6582422256469727,
      "learning_rate": 0.00019749839640795382,
      "loss": 2.407,
      "step": 239
    },
    {
      "epoch": 0.012824965933684238,
      "grad_norm": 1.5244755744934082,
      "learning_rate": 0.0001974877057943126,
      "loss": 2.2811,
      "step": 240
    },
    {
      "epoch": 0.012878403291741256,
      "grad_norm": 1.8624200820922852,
      "learning_rate": 0.00019747701518067138,
      "loss": 2.436,
      "step": 241
    },
    {
      "epoch": 0.012931840649798274,
      "grad_norm": 2.400510787963867,
      "learning_rate": 0.00019746632456703016,
      "loss": 2.4986,
      "step": 242
    },
    {
      "epoch": 0.012985278007855292,
      "grad_norm": 2.416327953338623,
      "learning_rate": 0.00019745563395338894,
      "loss": 2.3875,
      "step": 243
    },
    {
      "epoch": 0.013038715365912308,
      "grad_norm": 1.6085342168807983,
      "learning_rate": 0.0001974449433397477,
      "loss": 2.2275,
      "step": 244
    },
    {
      "epoch": 0.013092152723969326,
      "grad_norm": 1.516238808631897,
      "learning_rate": 0.0001974342527261065,
      "loss": 2.1047,
      "step": 245
    },
    {
      "epoch": 0.013145590082026344,
      "grad_norm": 1.732181191444397,
      "learning_rate": 0.00019742356211246525,
      "loss": 2.361,
      "step": 246
    },
    {
      "epoch": 0.013199027440083362,
      "grad_norm": 1.6561458110809326,
      "learning_rate": 0.00019741287149882403,
      "loss": 2.3346,
      "step": 247
    },
    {
      "epoch": 0.01325246479814038,
      "grad_norm": 1.286171317100525,
      "learning_rate": 0.00019740218088518281,
      "loss": 1.8738,
      "step": 248
    },
    {
      "epoch": 0.013305902156197398,
      "grad_norm": 1.586031198501587,
      "learning_rate": 0.0001973914902715416,
      "loss": 2.318,
      "step": 249
    },
    {
      "epoch": 0.013359339514254415,
      "grad_norm": 1.7317025661468506,
      "learning_rate": 0.00019738079965790037,
      "loss": 2.1821,
      "step": 250
    },
    {
      "epoch": 0.013412776872311433,
      "grad_norm": 6.731567859649658,
      "learning_rate": 0.00019737010904425915,
      "loss": 2.3858,
      "step": 251
    },
    {
      "epoch": 0.01346621423036845,
      "grad_norm": 1.4971562623977661,
      "learning_rate": 0.00019735941843061793,
      "loss": 2.1827,
      "step": 252
    },
    {
      "epoch": 0.013519651588425469,
      "grad_norm": 1.8602439165115356,
      "learning_rate": 0.0001973487278169767,
      "loss": 2.3221,
      "step": 253
    },
    {
      "epoch": 0.013573088946482487,
      "grad_norm": 2.0484297275543213,
      "learning_rate": 0.0001973380372033355,
      "loss": 2.3549,
      "step": 254
    },
    {
      "epoch": 0.013626526304539503,
      "grad_norm": 2.347074508666992,
      "learning_rate": 0.00019732734658969424,
      "loss": 2.262,
      "step": 255
    },
    {
      "epoch": 0.013679963662596521,
      "grad_norm": 2.5021588802337646,
      "learning_rate": 0.00019731665597605305,
      "loss": 2.3532,
      "step": 256
    },
    {
      "epoch": 0.013733401020653539,
      "grad_norm": 1.3558979034423828,
      "learning_rate": 0.0001973059653624118,
      "loss": 2.2396,
      "step": 257
    },
    {
      "epoch": 0.013786838378710557,
      "grad_norm": 1.9154551029205322,
      "learning_rate": 0.00019729527474877058,
      "loss": 2.228,
      "step": 258
    },
    {
      "epoch": 0.013840275736767575,
      "grad_norm": 3.1598494052886963,
      "learning_rate": 0.00019728458413512936,
      "loss": 2.0794,
      "step": 259
    },
    {
      "epoch": 0.013893713094824591,
      "grad_norm": 1.345278024673462,
      "learning_rate": 0.00019727389352148814,
      "loss": 2.3169,
      "step": 260
    },
    {
      "epoch": 0.013947150452881609,
      "grad_norm": 1.6322556734085083,
      "learning_rate": 0.00019726320290784692,
      "loss": 2.1137,
      "step": 261
    },
    {
      "epoch": 0.014000587810938627,
      "grad_norm": 1.275966763496399,
      "learning_rate": 0.0001972525122942057,
      "loss": 2.0456,
      "step": 262
    },
    {
      "epoch": 0.014054025168995645,
      "grad_norm": 1.6600183248519897,
      "learning_rate": 0.00019724182168056445,
      "loss": 2.2741,
      "step": 263
    },
    {
      "epoch": 0.014107462527052663,
      "grad_norm": 1.759407877922058,
      "learning_rate": 0.00019723113106692326,
      "loss": 2.364,
      "step": 264
    },
    {
      "epoch": 0.01416089988510968,
      "grad_norm": 1.834096074104309,
      "learning_rate": 0.00019722044045328204,
      "loss": 2.5592,
      "step": 265
    },
    {
      "epoch": 0.014214337243166697,
      "grad_norm": 1.7968651056289673,
      "learning_rate": 0.0001972097498396408,
      "loss": 2.3194,
      "step": 266
    },
    {
      "epoch": 0.014267774601223715,
      "grad_norm": 1.9356722831726074,
      "learning_rate": 0.00019719905922599957,
      "loss": 2.3197,
      "step": 267
    },
    {
      "epoch": 0.014321211959280733,
      "grad_norm": 1.4751859903335571,
      "learning_rate": 0.00019718836861235835,
      "loss": 2.3349,
      "step": 268
    },
    {
      "epoch": 0.014374649317337751,
      "grad_norm": 1.7063785791397095,
      "learning_rate": 0.00019717767799871713,
      "loss": 2.2312,
      "step": 269
    },
    {
      "epoch": 0.01442808667539477,
      "grad_norm": 2.111907720565796,
      "learning_rate": 0.0001971669873850759,
      "loss": 2.2744,
      "step": 270
    },
    {
      "epoch": 0.014481524033451786,
      "grad_norm": 1.9191499948501587,
      "learning_rate": 0.0001971562967714347,
      "loss": 2.2765,
      "step": 271
    },
    {
      "epoch": 0.014534961391508804,
      "grad_norm": 1.8256696462631226,
      "learning_rate": 0.00019714560615779347,
      "loss": 2.2111,
      "step": 272
    },
    {
      "epoch": 0.014588398749565822,
      "grad_norm": 2.2760839462280273,
      "learning_rate": 0.00019713491554415225,
      "loss": 2.3981,
      "step": 273
    },
    {
      "epoch": 0.01464183610762284,
      "grad_norm": 1.6807100772857666,
      "learning_rate": 0.000197124224930511,
      "loss": 2.2342,
      "step": 274
    },
    {
      "epoch": 0.014695273465679858,
      "grad_norm": 2.155566930770874,
      "learning_rate": 0.0001971135343168698,
      "loss": 2.463,
      "step": 275
    },
    {
      "epoch": 0.014748710823736874,
      "grad_norm": 1.4235788583755493,
      "learning_rate": 0.0001971028437032286,
      "loss": 2.3791,
      "step": 276
    },
    {
      "epoch": 0.014802148181793892,
      "grad_norm": 1.179872989654541,
      "learning_rate": 0.00019709215308958734,
      "loss": 2.0417,
      "step": 277
    },
    {
      "epoch": 0.01485558553985091,
      "grad_norm": 1.2439278364181519,
      "learning_rate": 0.00019708146247594612,
      "loss": 2.111,
      "step": 278
    },
    {
      "epoch": 0.014909022897907928,
      "grad_norm": 1.444427728652954,
      "learning_rate": 0.0001970707718623049,
      "loss": 2.2418,
      "step": 279
    },
    {
      "epoch": 0.014962460255964946,
      "grad_norm": 1.4565761089324951,
      "learning_rate": 0.00019706008124866368,
      "loss": 2.3222,
      "step": 280
    },
    {
      "epoch": 0.015015897614021962,
      "grad_norm": 1.575871229171753,
      "learning_rate": 0.00019704939063502246,
      "loss": 2.3932,
      "step": 281
    },
    {
      "epoch": 0.01506933497207898,
      "grad_norm": 1.6027491092681885,
      "learning_rate": 0.00019703870002138124,
      "loss": 2.2566,
      "step": 282
    },
    {
      "epoch": 0.015122772330135998,
      "grad_norm": 1.521506667137146,
      "learning_rate": 0.00019702800940774002,
      "loss": 2.0971,
      "step": 283
    },
    {
      "epoch": 0.015176209688193016,
      "grad_norm": 1.4854158163070679,
      "learning_rate": 0.0001970173187940988,
      "loss": 2.1723,
      "step": 284
    },
    {
      "epoch": 0.015229647046250034,
      "grad_norm": 1.490531325340271,
      "learning_rate": 0.00019700662818045755,
      "loss": 2.0702,
      "step": 285
    },
    {
      "epoch": 0.01528308440430705,
      "grad_norm": 1.8931411504745483,
      "learning_rate": 0.00019699593756681633,
      "loss": 2.4066,
      "step": 286
    },
    {
      "epoch": 0.015336521762364068,
      "grad_norm": 1.7550147771835327,
      "learning_rate": 0.00019698524695317514,
      "loss": 2.1709,
      "step": 287
    },
    {
      "epoch": 0.015389959120421086,
      "grad_norm": 1.12906813621521,
      "learning_rate": 0.0001969745563395339,
      "loss": 2.0291,
      "step": 288
    },
    {
      "epoch": 0.015443396478478104,
      "grad_norm": 2.241873025894165,
      "learning_rate": 0.00019696386572589267,
      "loss": 2.4844,
      "step": 289
    },
    {
      "epoch": 0.015496833836535122,
      "grad_norm": 2.7168161869049072,
      "learning_rate": 0.00019695317511225145,
      "loss": 2.4032,
      "step": 290
    },
    {
      "epoch": 0.01555027119459214,
      "grad_norm": 2.783116340637207,
      "learning_rate": 0.00019694248449861023,
      "loss": 2.4211,
      "step": 291
    },
    {
      "epoch": 0.015603708552649156,
      "grad_norm": 2.0058441162109375,
      "learning_rate": 0.000196931793884969,
      "loss": 2.4713,
      "step": 292
    },
    {
      "epoch": 0.015657145910706174,
      "grad_norm": 2.6176083087921143,
      "learning_rate": 0.0001969211032713278,
      "loss": 2.3305,
      "step": 293
    },
    {
      "epoch": 0.015710583268763192,
      "grad_norm": 1.164573073387146,
      "learning_rate": 0.00019691041265768657,
      "loss": 2.1476,
      "step": 294
    },
    {
      "epoch": 0.01576402062682021,
      "grad_norm": 1.9512273073196411,
      "learning_rate": 0.00019689972204404535,
      "loss": 2.2602,
      "step": 295
    },
    {
      "epoch": 0.01581745798487723,
      "grad_norm": 1.9704699516296387,
      "learning_rate": 0.0001968890314304041,
      "loss": 2.3751,
      "step": 296
    },
    {
      "epoch": 0.015870895342934246,
      "grad_norm": 1.685815691947937,
      "learning_rate": 0.00019687834081676288,
      "loss": 2.2835,
      "step": 297
    },
    {
      "epoch": 0.015924332700991264,
      "grad_norm": 1.6485916376113892,
      "learning_rate": 0.0001968676502031217,
      "loss": 2.3439,
      "step": 298
    },
    {
      "epoch": 0.01597777005904828,
      "grad_norm": 1.8568354845046997,
      "learning_rate": 0.00019685695958948044,
      "loss": 2.2396,
      "step": 299
    },
    {
      "epoch": 0.016031207417105297,
      "grad_norm": 1.53928804397583,
      "learning_rate": 0.00019684626897583922,
      "loss": 2.3245,
      "step": 300
    },
    {
      "epoch": 0.016084644775162315,
      "grad_norm": 2.2608256340026855,
      "learning_rate": 0.000196835578362198,
      "loss": 2.3091,
      "step": 301
    },
    {
      "epoch": 0.016138082133219333,
      "grad_norm": 1.516178011894226,
      "learning_rate": 0.00019682488774855678,
      "loss": 2.2187,
      "step": 302
    },
    {
      "epoch": 0.01619151949127635,
      "grad_norm": 2.634747266769409,
      "learning_rate": 0.00019681419713491556,
      "loss": 2.4919,
      "step": 303
    },
    {
      "epoch": 0.01624495684933337,
      "grad_norm": 1.651749849319458,
      "learning_rate": 0.00019680350652127434,
      "loss": 2.3897,
      "step": 304
    },
    {
      "epoch": 0.016298394207390387,
      "grad_norm": 1.9723024368286133,
      "learning_rate": 0.0001967928159076331,
      "loss": 2.1688,
      "step": 305
    },
    {
      "epoch": 0.016351831565447405,
      "grad_norm": 2.0453081130981445,
      "learning_rate": 0.0001967821252939919,
      "loss": 2.4991,
      "step": 306
    },
    {
      "epoch": 0.016405268923504423,
      "grad_norm": 1.9980307817459106,
      "learning_rate": 0.00019677143468035065,
      "loss": 2.2217,
      "step": 307
    },
    {
      "epoch": 0.01645870628156144,
      "grad_norm": 1.6497584581375122,
      "learning_rate": 0.00019676074406670943,
      "loss": 2.3419,
      "step": 308
    },
    {
      "epoch": 0.01651214363961846,
      "grad_norm": 1.9262864589691162,
      "learning_rate": 0.0001967500534530682,
      "loss": 2.2534,
      "step": 309
    },
    {
      "epoch": 0.016565580997675473,
      "grad_norm": 2.180208206176758,
      "learning_rate": 0.000196739362839427,
      "loss": 2.314,
      "step": 310
    },
    {
      "epoch": 0.01661901835573249,
      "grad_norm": 2.1183111667633057,
      "learning_rate": 0.00019672867222578577,
      "loss": 2.1948,
      "step": 311
    },
    {
      "epoch": 0.01667245571378951,
      "grad_norm": 1.4080612659454346,
      "learning_rate": 0.00019671798161214455,
      "loss": 2.1517,
      "step": 312
    },
    {
      "epoch": 0.016725893071846527,
      "grad_norm": 1.6375370025634766,
      "learning_rate": 0.0001967072909985033,
      "loss": 2.1603,
      "step": 313
    },
    {
      "epoch": 0.016779330429903545,
      "grad_norm": 1.4604445695877075,
      "learning_rate": 0.0001966966003848621,
      "loss": 2.2201,
      "step": 314
    },
    {
      "epoch": 0.016832767787960563,
      "grad_norm": 1.3064701557159424,
      "learning_rate": 0.0001966859097712209,
      "loss": 2.1033,
      "step": 315
    },
    {
      "epoch": 0.01688620514601758,
      "grad_norm": 1.5236225128173828,
      "learning_rate": 0.00019667521915757964,
      "loss": 2.0369,
      "step": 316
    },
    {
      "epoch": 0.0169396425040746,
      "grad_norm": 1.3869305849075317,
      "learning_rate": 0.00019666452854393845,
      "loss": 2.1949,
      "step": 317
    },
    {
      "epoch": 0.016993079862131617,
      "grad_norm": 1.2172577381134033,
      "learning_rate": 0.00019665383793029723,
      "loss": 2.0486,
      "step": 318
    },
    {
      "epoch": 0.017046517220188635,
      "grad_norm": 1.3590731620788574,
      "learning_rate": 0.00019664314731665598,
      "loss": 1.8919,
      "step": 319
    },
    {
      "epoch": 0.01709995457824565,
      "grad_norm": 1.4523439407348633,
      "learning_rate": 0.00019663245670301476,
      "loss": 2.1908,
      "step": 320
    },
    {
      "epoch": 0.017153391936302668,
      "grad_norm": 2.2986013889312744,
      "learning_rate": 0.00019662176608937354,
      "loss": 2.3291,
      "step": 321
    },
    {
      "epoch": 0.017206829294359686,
      "grad_norm": 1.6117777824401855,
      "learning_rate": 0.00019661107547573232,
      "loss": 2.2605,
      "step": 322
    },
    {
      "epoch": 0.017260266652416704,
      "grad_norm": 1.4682368040084839,
      "learning_rate": 0.0001966003848620911,
      "loss": 2.2054,
      "step": 323
    },
    {
      "epoch": 0.017313704010473722,
      "grad_norm": 1.5568195581436157,
      "learning_rate": 0.00019658969424844985,
      "loss": 2.1177,
      "step": 324
    },
    {
      "epoch": 0.01736714136853074,
      "grad_norm": 2.0581018924713135,
      "learning_rate": 0.00019657900363480866,
      "loss": 2.3674,
      "step": 325
    },
    {
      "epoch": 0.017420578726587758,
      "grad_norm": 1.8323602676391602,
      "learning_rate": 0.00019656831302116744,
      "loss": 2.1817,
      "step": 326
    },
    {
      "epoch": 0.017474016084644776,
      "grad_norm": 1.8304784297943115,
      "learning_rate": 0.0001965576224075262,
      "loss": 2.3724,
      "step": 327
    },
    {
      "epoch": 0.017527453442701794,
      "grad_norm": 1.864850401878357,
      "learning_rate": 0.00019654693179388497,
      "loss": 2.5782,
      "step": 328
    },
    {
      "epoch": 0.017580890800758812,
      "grad_norm": 1.7029331922531128,
      "learning_rate": 0.00019653624118024378,
      "loss": 2.1581,
      "step": 329
    },
    {
      "epoch": 0.01763432815881583,
      "grad_norm": 2.1743996143341064,
      "learning_rate": 0.00019652555056660253,
      "loss": 2.1713,
      "step": 330
    },
    {
      "epoch": 0.017687765516872844,
      "grad_norm": 3.396580457687378,
      "learning_rate": 0.0001965148599529613,
      "loss": 2.3627,
      "step": 331
    },
    {
      "epoch": 0.017741202874929862,
      "grad_norm": 2.4321463108062744,
      "learning_rate": 0.0001965041693393201,
      "loss": 2.2084,
      "step": 332
    },
    {
      "epoch": 0.01779464023298688,
      "grad_norm": 2.184349298477173,
      "learning_rate": 0.00019649347872567887,
      "loss": 1.9508,
      "step": 333
    },
    {
      "epoch": 0.017848077591043898,
      "grad_norm": 2.2527313232421875,
      "learning_rate": 0.00019648278811203765,
      "loss": 2.1596,
      "step": 334
    },
    {
      "epoch": 0.017901514949100916,
      "grad_norm": 1.2998868227005005,
      "learning_rate": 0.0001964720974983964,
      "loss": 1.9677,
      "step": 335
    },
    {
      "epoch": 0.017954952307157934,
      "grad_norm": 1.8274343013763428,
      "learning_rate": 0.00019646140688475518,
      "loss": 2.3404,
      "step": 336
    },
    {
      "epoch": 0.018008389665214952,
      "grad_norm": 1.9581352472305298,
      "learning_rate": 0.00019645071627111399,
      "loss": 2.0904,
      "step": 337
    },
    {
      "epoch": 0.01806182702327197,
      "grad_norm": 2.3672029972076416,
      "learning_rate": 0.00019644002565747274,
      "loss": 2.2664,
      "step": 338
    },
    {
      "epoch": 0.018115264381328988,
      "grad_norm": 1.6438535451889038,
      "learning_rate": 0.00019642933504383152,
      "loss": 2.2047,
      "step": 339
    },
    {
      "epoch": 0.018168701739386006,
      "grad_norm": 2.1392667293548584,
      "learning_rate": 0.00019641864443019032,
      "loss": 2.3227,
      "step": 340
    },
    {
      "epoch": 0.01822213909744302,
      "grad_norm": 2.389693260192871,
      "learning_rate": 0.00019640795381654908,
      "loss": 2.3894,
      "step": 341
    },
    {
      "epoch": 0.01827557645550004,
      "grad_norm": 1.3576246500015259,
      "learning_rate": 0.00019639726320290786,
      "loss": 1.9348,
      "step": 342
    },
    {
      "epoch": 0.018329013813557057,
      "grad_norm": 2.067734956741333,
      "learning_rate": 0.00019638657258926664,
      "loss": 2.4078,
      "step": 343
    },
    {
      "epoch": 0.018382451171614075,
      "grad_norm": 1.8153337240219116,
      "learning_rate": 0.00019637588197562542,
      "loss": 2.4434,
      "step": 344
    },
    {
      "epoch": 0.018435888529671093,
      "grad_norm": 1.6775426864624023,
      "learning_rate": 0.0001963651913619842,
      "loss": 2.3513,
      "step": 345
    },
    {
      "epoch": 0.01848932588772811,
      "grad_norm": 2.236999988555908,
      "learning_rate": 0.00019635450074834298,
      "loss": 2.3394,
      "step": 346
    },
    {
      "epoch": 0.01854276324578513,
      "grad_norm": 2.1314921379089355,
      "learning_rate": 0.00019634381013470173,
      "loss": 2.2935,
      "step": 347
    },
    {
      "epoch": 0.018596200603842147,
      "grad_norm": 1.52981698513031,
      "learning_rate": 0.00019633311952106053,
      "loss": 2.1449,
      "step": 348
    },
    {
      "epoch": 0.018649637961899165,
      "grad_norm": 1.2679015398025513,
      "learning_rate": 0.0001963224289074193,
      "loss": 2.0439,
      "step": 349
    },
    {
      "epoch": 0.018703075319956183,
      "grad_norm": 1.2990000247955322,
      "learning_rate": 0.00019631173829377807,
      "loss": 2.1577,
      "step": 350
    },
    {
      "epoch": 0.0187565126780132,
      "grad_norm": 1.254313588142395,
      "learning_rate": 0.00019630104768013685,
      "loss": 2.0955,
      "step": 351
    },
    {
      "epoch": 0.018809950036070215,
      "grad_norm": 1.9866148233413696,
      "learning_rate": 0.00019629035706649563,
      "loss": 2.1631,
      "step": 352
    },
    {
      "epoch": 0.018863387394127233,
      "grad_norm": 1.3959647417068481,
      "learning_rate": 0.0001962796664528544,
      "loss": 2.1566,
      "step": 353
    },
    {
      "epoch": 0.01891682475218425,
      "grad_norm": 1.6550424098968506,
      "learning_rate": 0.00019626897583921319,
      "loss": 2.2306,
      "step": 354
    },
    {
      "epoch": 0.01897026211024127,
      "grad_norm": 2.0562872886657715,
      "learning_rate": 0.00019625828522557194,
      "loss": 2.2609,
      "step": 355
    },
    {
      "epoch": 0.019023699468298287,
      "grad_norm": 1.5405733585357666,
      "learning_rate": 0.00019624759461193074,
      "loss": 2.0369,
      "step": 356
    },
    {
      "epoch": 0.019077136826355305,
      "grad_norm": 1.3865865468978882,
      "learning_rate": 0.00019623690399828952,
      "loss": 2.2382,
      "step": 357
    },
    {
      "epoch": 0.019130574184412323,
      "grad_norm": 2.133028268814087,
      "learning_rate": 0.00019622621338464828,
      "loss": 2.3701,
      "step": 358
    },
    {
      "epoch": 0.01918401154246934,
      "grad_norm": 1.4404391050338745,
      "learning_rate": 0.00019621552277100706,
      "loss": 2.1121,
      "step": 359
    },
    {
      "epoch": 0.01923744890052636,
      "grad_norm": 2.182229518890381,
      "learning_rate": 0.00019620483215736584,
      "loss": 2.1723,
      "step": 360
    },
    {
      "epoch": 0.019290886258583377,
      "grad_norm": 2.014420509338379,
      "learning_rate": 0.00019619414154372462,
      "loss": 2.3511,
      "step": 361
    },
    {
      "epoch": 0.01934432361664039,
      "grad_norm": 1.757541537284851,
      "learning_rate": 0.0001961834509300834,
      "loss": 2.1759,
      "step": 362
    },
    {
      "epoch": 0.01939776097469741,
      "grad_norm": 1.9685503244400024,
      "learning_rate": 0.00019617276031644218,
      "loss": 2.1858,
      "step": 363
    },
    {
      "epoch": 0.019451198332754428,
      "grad_norm": 1.4649550914764404,
      "learning_rate": 0.00019616206970280095,
      "loss": 2.2868,
      "step": 364
    },
    {
      "epoch": 0.019504635690811446,
      "grad_norm": 1.843182921409607,
      "learning_rate": 0.00019615137908915973,
      "loss": 2.3602,
      "step": 365
    },
    {
      "epoch": 0.019558073048868464,
      "grad_norm": 2.1367135047912598,
      "learning_rate": 0.0001961406884755185,
      "loss": 2.6718,
      "step": 366
    },
    {
      "epoch": 0.01961151040692548,
      "grad_norm": 2.3372647762298584,
      "learning_rate": 0.0001961299978618773,
      "loss": 2.3883,
      "step": 367
    },
    {
      "epoch": 0.0196649477649825,
      "grad_norm": 1.8020437955856323,
      "learning_rate": 0.00019611930724823607,
      "loss": 2.1933,
      "step": 368
    },
    {
      "epoch": 0.019718385123039518,
      "grad_norm": 1.6743323802947998,
      "learning_rate": 0.00019610861663459483,
      "loss": 1.9604,
      "step": 369
    },
    {
      "epoch": 0.019771822481096536,
      "grad_norm": 1.011966347694397,
      "learning_rate": 0.0001960979260209536,
      "loss": 2.0275,
      "step": 370
    },
    {
      "epoch": 0.019825259839153554,
      "grad_norm": 1.484629511833191,
      "learning_rate": 0.00019608723540731239,
      "loss": 2.3198,
      "step": 371
    },
    {
      "epoch": 0.01987869719721057,
      "grad_norm": 1.866796612739563,
      "learning_rate": 0.00019607654479367116,
      "loss": 2.0776,
      "step": 372
    },
    {
      "epoch": 0.019932134555267586,
      "grad_norm": 1.4470391273498535,
      "learning_rate": 0.00019606585418002994,
      "loss": 2.2892,
      "step": 373
    },
    {
      "epoch": 0.019985571913324604,
      "grad_norm": 1.2080490589141846,
      "learning_rate": 0.00019605516356638872,
      "loss": 2.0423,
      "step": 374
    },
    {
      "epoch": 0.020039009271381622,
      "grad_norm": 2.18241548538208,
      "learning_rate": 0.0001960444729527475,
      "loss": 2.2523,
      "step": 375
    },
    {
      "epoch": 0.02009244662943864,
      "grad_norm": 1.9202834367752075,
      "learning_rate": 0.00019603378233910628,
      "loss": 2.2496,
      "step": 376
    },
    {
      "epoch": 0.020145883987495658,
      "grad_norm": 1.365255355834961,
      "learning_rate": 0.00019602309172546504,
      "loss": 2.2767,
      "step": 377
    },
    {
      "epoch": 0.020199321345552676,
      "grad_norm": 1.7156450748443604,
      "learning_rate": 0.00019601240111182382,
      "loss": 2.3301,
      "step": 378
    },
    {
      "epoch": 0.020252758703609694,
      "grad_norm": 1.8139128684997559,
      "learning_rate": 0.00019600171049818262,
      "loss": 2.1138,
      "step": 379
    },
    {
      "epoch": 0.020306196061666712,
      "grad_norm": 2.0475480556488037,
      "learning_rate": 0.00019599101988454138,
      "loss": 2.2855,
      "step": 380
    },
    {
      "epoch": 0.02035963341972373,
      "grad_norm": 1.5645244121551514,
      "learning_rate": 0.00019598032927090015,
      "loss": 2.4369,
      "step": 381
    },
    {
      "epoch": 0.020413070777780748,
      "grad_norm": 1.7425687313079834,
      "learning_rate": 0.00019596963865725893,
      "loss": 2.4459,
      "step": 382
    },
    {
      "epoch": 0.020466508135837762,
      "grad_norm": 1.7663627862930298,
      "learning_rate": 0.00019595894804361771,
      "loss": 2.4063,
      "step": 383
    },
    {
      "epoch": 0.02051994549389478,
      "grad_norm": 1.4760801792144775,
      "learning_rate": 0.0001959482574299765,
      "loss": 2.3433,
      "step": 384
    },
    {
      "epoch": 0.0205733828519518,
      "grad_norm": 1.4966237545013428,
      "learning_rate": 0.00019593756681633527,
      "loss": 2.187,
      "step": 385
    },
    {
      "epoch": 0.020626820210008816,
      "grad_norm": 1.5994693040847778,
      "learning_rate": 0.00019592687620269405,
      "loss": 2.1935,
      "step": 386
    },
    {
      "epoch": 0.020680257568065834,
      "grad_norm": 1.582716464996338,
      "learning_rate": 0.00019591618558905283,
      "loss": 2.367,
      "step": 387
    },
    {
      "epoch": 0.020733694926122852,
      "grad_norm": 1.410301923751831,
      "learning_rate": 0.00019590549497541159,
      "loss": 2.2841,
      "step": 388
    },
    {
      "epoch": 0.02078713228417987,
      "grad_norm": 1.8106001615524292,
      "learning_rate": 0.00019589480436177036,
      "loss": 2.4835,
      "step": 389
    },
    {
      "epoch": 0.02084056964223689,
      "grad_norm": 1.8069345951080322,
      "learning_rate": 0.00019588411374812917,
      "loss": 2.3393,
      "step": 390
    },
    {
      "epoch": 0.020894007000293906,
      "grad_norm": 1.4058692455291748,
      "learning_rate": 0.00019587342313448792,
      "loss": 2.0817,
      "step": 391
    },
    {
      "epoch": 0.020947444358350924,
      "grad_norm": 1.6656426191329956,
      "learning_rate": 0.0001958627325208467,
      "loss": 2.2399,
      "step": 392
    },
    {
      "epoch": 0.021000881716407942,
      "grad_norm": 1.5856181383132935,
      "learning_rate": 0.00019585204190720548,
      "loss": 2.4157,
      "step": 393
    },
    {
      "epoch": 0.021054319074464957,
      "grad_norm": 2.1450612545013428,
      "learning_rate": 0.00019584135129356426,
      "loss": 2.1656,
      "step": 394
    },
    {
      "epoch": 0.021107756432521975,
      "grad_norm": 1.409704327583313,
      "learning_rate": 0.00019583066067992304,
      "loss": 2.2677,
      "step": 395
    },
    {
      "epoch": 0.021161193790578993,
      "grad_norm": 1.6665501594543457,
      "learning_rate": 0.00019581997006628182,
      "loss": 2.0794,
      "step": 396
    },
    {
      "epoch": 0.02121463114863601,
      "grad_norm": 1.0707180500030518,
      "learning_rate": 0.00019580927945264057,
      "loss": 2.1483,
      "step": 397
    },
    {
      "epoch": 0.02126806850669303,
      "grad_norm": 1.2828142642974854,
      "learning_rate": 0.00019579858883899938,
      "loss": 2.1464,
      "step": 398
    },
    {
      "epoch": 0.021321505864750047,
      "grad_norm": 2.5235960483551025,
      "learning_rate": 0.00019578789822535813,
      "loss": 2.2938,
      "step": 399
    },
    {
      "epoch": 0.021374943222807065,
      "grad_norm": 2.111114025115967,
      "learning_rate": 0.00019577720761171691,
      "loss": 2.241,
      "step": 400
    },
    {
      "epoch": 0.021428380580864083,
      "grad_norm": 1.5108869075775146,
      "learning_rate": 0.0001957665169980757,
      "loss": 2.0496,
      "step": 401
    },
    {
      "epoch": 0.0214818179389211,
      "grad_norm": 1.4925700426101685,
      "learning_rate": 0.00019575582638443447,
      "loss": 2.2275,
      "step": 402
    },
    {
      "epoch": 0.02153525529697812,
      "grad_norm": 1.7199853658676147,
      "learning_rate": 0.00019574513577079325,
      "loss": 2.2018,
      "step": 403
    },
    {
      "epoch": 0.021588692655035133,
      "grad_norm": 1.8864065408706665,
      "learning_rate": 0.00019573444515715203,
      "loss": 2.123,
      "step": 404
    },
    {
      "epoch": 0.02164213001309215,
      "grad_norm": 1.1504452228546143,
      "learning_rate": 0.0001957237545435108,
      "loss": 1.9894,
      "step": 405
    },
    {
      "epoch": 0.02169556737114917,
      "grad_norm": 1.6304064989089966,
      "learning_rate": 0.0001957130639298696,
      "loss": 2.2186,
      "step": 406
    },
    {
      "epoch": 0.021749004729206187,
      "grad_norm": 1.850245475769043,
      "learning_rate": 0.00019570237331622837,
      "loss": 2.2485,
      "step": 407
    },
    {
      "epoch": 0.021802442087263205,
      "grad_norm": 1.1990010738372803,
      "learning_rate": 0.00019569168270258712,
      "loss": 2.1127,
      "step": 408
    },
    {
      "epoch": 0.021855879445320223,
      "grad_norm": 2.3066883087158203,
      "learning_rate": 0.00019568099208894593,
      "loss": 2.3212,
      "step": 409
    },
    {
      "epoch": 0.02190931680337724,
      "grad_norm": 1.4544934034347534,
      "learning_rate": 0.00019567030147530468,
      "loss": 2.2913,
      "step": 410
    },
    {
      "epoch": 0.02196275416143426,
      "grad_norm": 1.6724469661712646,
      "learning_rate": 0.00019565961086166346,
      "loss": 2.304,
      "step": 411
    },
    {
      "epoch": 0.022016191519491277,
      "grad_norm": 1.6279677152633667,
      "learning_rate": 0.00019564892024802224,
      "loss": 2.3738,
      "step": 412
    },
    {
      "epoch": 0.022069628877548295,
      "grad_norm": 1.3418816328048706,
      "learning_rate": 0.00019563822963438102,
      "loss": 2.1854,
      "step": 413
    },
    {
      "epoch": 0.022123066235605313,
      "grad_norm": 1.5234036445617676,
      "learning_rate": 0.0001956275390207398,
      "loss": 2.2349,
      "step": 414
    },
    {
      "epoch": 0.022176503593662328,
      "grad_norm": 1.4872546195983887,
      "learning_rate": 0.00019561684840709858,
      "loss": 2.3764,
      "step": 415
    },
    {
      "epoch": 0.022229940951719346,
      "grad_norm": 1.5069973468780518,
      "learning_rate": 0.00019560615779345733,
      "loss": 2.2336,
      "step": 416
    },
    {
      "epoch": 0.022283378309776364,
      "grad_norm": 1.6426215171813965,
      "learning_rate": 0.00019559546717981614,
      "loss": 2.2119,
      "step": 417
    },
    {
      "epoch": 0.022336815667833382,
      "grad_norm": 2.135556697845459,
      "learning_rate": 0.00019558477656617492,
      "loss": 2.3547,
      "step": 418
    },
    {
      "epoch": 0.0223902530258904,
      "grad_norm": 2.499321699142456,
      "learning_rate": 0.00019557408595253367,
      "loss": 2.2448,
      "step": 419
    },
    {
      "epoch": 0.022443690383947418,
      "grad_norm": 1.5638816356658936,
      "learning_rate": 0.00019556339533889245,
      "loss": 2.0937,
      "step": 420
    },
    {
      "epoch": 0.022497127742004436,
      "grad_norm": 1.658165454864502,
      "learning_rate": 0.00019555270472525123,
      "loss": 2.177,
      "step": 421
    },
    {
      "epoch": 0.022550565100061454,
      "grad_norm": 1.5044136047363281,
      "learning_rate": 0.00019554201411161,
      "loss": 2.2475,
      "step": 422
    },
    {
      "epoch": 0.022604002458118472,
      "grad_norm": 1.7413270473480225,
      "learning_rate": 0.0001955313234979688,
      "loss": 2.1585,
      "step": 423
    },
    {
      "epoch": 0.02265743981617549,
      "grad_norm": 1.666377067565918,
      "learning_rate": 0.00019552063288432757,
      "loss": 2.3403,
      "step": 424
    },
    {
      "epoch": 0.022710877174232504,
      "grad_norm": 1.6102144718170166,
      "learning_rate": 0.00019550994227068635,
      "loss": 2.1854,
      "step": 425
    },
    {
      "epoch": 0.022764314532289522,
      "grad_norm": 1.4352996349334717,
      "learning_rate": 0.00019549925165704513,
      "loss": 2.1658,
      "step": 426
    },
    {
      "epoch": 0.02281775189034654,
      "grad_norm": 1.4771225452423096,
      "learning_rate": 0.00019548856104340388,
      "loss": 2.1162,
      "step": 427
    },
    {
      "epoch": 0.022871189248403558,
      "grad_norm": 1.5282219648361206,
      "learning_rate": 0.0001954778704297627,
      "loss": 2.1798,
      "step": 428
    },
    {
      "epoch": 0.022924626606460576,
      "grad_norm": 1.9245152473449707,
      "learning_rate": 0.00019546717981612147,
      "loss": 2.485,
      "step": 429
    },
    {
      "epoch": 0.022978063964517594,
      "grad_norm": 1.490512728691101,
      "learning_rate": 0.00019545648920248022,
      "loss": 2.0756,
      "step": 430
    },
    {
      "epoch": 0.023031501322574612,
      "grad_norm": 3.5692527294158936,
      "learning_rate": 0.000195445798588839,
      "loss": 2.6035,
      "step": 431
    },
    {
      "epoch": 0.02308493868063163,
      "grad_norm": 1.775394082069397,
      "learning_rate": 0.0001954351079751978,
      "loss": 2.3631,
      "step": 432
    },
    {
      "epoch": 0.023138376038688648,
      "grad_norm": 1.9869366884231567,
      "learning_rate": 0.00019542441736155656,
      "loss": 2.1336,
      "step": 433
    },
    {
      "epoch": 0.023191813396745666,
      "grad_norm": 1.7394651174545288,
      "learning_rate": 0.00019541372674791534,
      "loss": 2.3423,
      "step": 434
    },
    {
      "epoch": 0.023245250754802684,
      "grad_norm": 1.5531123876571655,
      "learning_rate": 0.00019540303613427412,
      "loss": 2.1749,
      "step": 435
    },
    {
      "epoch": 0.0232986881128597,
      "grad_norm": 1.1321598291397095,
      "learning_rate": 0.0001953923455206329,
      "loss": 2.1495,
      "step": 436
    },
    {
      "epoch": 0.023352125470916717,
      "grad_norm": 1.6347970962524414,
      "learning_rate": 0.00019538165490699168,
      "loss": 2.3563,
      "step": 437
    },
    {
      "epoch": 0.023405562828973735,
      "grad_norm": 1.37310791015625,
      "learning_rate": 0.00019537096429335043,
      "loss": 2.3454,
      "step": 438
    },
    {
      "epoch": 0.023459000187030753,
      "grad_norm": 2.4041125774383545,
      "learning_rate": 0.0001953602736797092,
      "loss": 2.4931,
      "step": 439
    },
    {
      "epoch": 0.02351243754508777,
      "grad_norm": 1.7578755617141724,
      "learning_rate": 0.00019534958306606802,
      "loss": 2.1209,
      "step": 440
    },
    {
      "epoch": 0.02356587490314479,
      "grad_norm": 1.4815069437026978,
      "learning_rate": 0.00019533889245242677,
      "loss": 2.3518,
      "step": 441
    },
    {
      "epoch": 0.023619312261201807,
      "grad_norm": 1.346793293952942,
      "learning_rate": 0.00019532820183878555,
      "loss": 2.1413,
      "step": 442
    },
    {
      "epoch": 0.023672749619258825,
      "grad_norm": 1.59746253490448,
      "learning_rate": 0.00019531751122514433,
      "loss": 2.2829,
      "step": 443
    },
    {
      "epoch": 0.023726186977315843,
      "grad_norm": 1.4369903802871704,
      "learning_rate": 0.0001953068206115031,
      "loss": 2.2007,
      "step": 444
    },
    {
      "epoch": 0.02377962433537286,
      "grad_norm": 1.640157699584961,
      "learning_rate": 0.0001952961299978619,
      "loss": 2.2621,
      "step": 445
    },
    {
      "epoch": 0.023833061693429875,
      "grad_norm": 1.7678275108337402,
      "learning_rate": 0.00019528543938422067,
      "loss": 2.1785,
      "step": 446
    },
    {
      "epoch": 0.023886499051486893,
      "grad_norm": 2.1613359451293945,
      "learning_rate": 0.00019527474877057942,
      "loss": 2.3874,
      "step": 447
    },
    {
      "epoch": 0.02393993640954391,
      "grad_norm": 1.2673709392547607,
      "learning_rate": 0.00019526405815693823,
      "loss": 2.0311,
      "step": 448
    },
    {
      "epoch": 0.02399337376760093,
      "grad_norm": 1.5374022722244263,
      "learning_rate": 0.00019525336754329698,
      "loss": 2.2203,
      "step": 449
    },
    {
      "epoch": 0.024046811125657947,
      "grad_norm": 1.5975531339645386,
      "learning_rate": 0.00019524267692965576,
      "loss": 2.2332,
      "step": 450
    },
    {
      "epoch": 0.024100248483714965,
      "grad_norm": 1.9423389434814453,
      "learning_rate": 0.00019523198631601457,
      "loss": 2.2466,
      "step": 451
    },
    {
      "epoch": 0.024153685841771983,
      "grad_norm": 1.4145885705947876,
      "learning_rate": 0.00019522129570237332,
      "loss": 2.2765,
      "step": 452
    },
    {
      "epoch": 0.024207123199829,
      "grad_norm": 1.6494556665420532,
      "learning_rate": 0.0001952106050887321,
      "loss": 2.3276,
      "step": 453
    },
    {
      "epoch": 0.02426056055788602,
      "grad_norm": 1.2665228843688965,
      "learning_rate": 0.00019519991447509088,
      "loss": 2.1897,
      "step": 454
    },
    {
      "epoch": 0.024313997915943037,
      "grad_norm": 1.881937026977539,
      "learning_rate": 0.00019518922386144966,
      "loss": 2.4594,
      "step": 455
    },
    {
      "epoch": 0.024367435274000055,
      "grad_norm": 1.4754122495651245,
      "learning_rate": 0.00019517853324780844,
      "loss": 2.3743,
      "step": 456
    },
    {
      "epoch": 0.02442087263205707,
      "grad_norm": 1.5660134553909302,
      "learning_rate": 0.00019516784263416722,
      "loss": 2.0857,
      "step": 457
    },
    {
      "epoch": 0.024474309990114088,
      "grad_norm": 1.8273221254348755,
      "learning_rate": 0.00019515715202052597,
      "loss": 2.3449,
      "step": 458
    },
    {
      "epoch": 0.024527747348171106,
      "grad_norm": 1.7010369300842285,
      "learning_rate": 0.00019514646140688478,
      "loss": 2.1599,
      "step": 459
    },
    {
      "epoch": 0.024581184706228124,
      "grad_norm": 1.334182858467102,
      "learning_rate": 0.00019513577079324356,
      "loss": 2.1056,
      "step": 460
    },
    {
      "epoch": 0.02463462206428514,
      "grad_norm": 1.033002495765686,
      "learning_rate": 0.0001951250801796023,
      "loss": 2.0707,
      "step": 461
    },
    {
      "epoch": 0.02468805942234216,
      "grad_norm": 1.2207106351852417,
      "learning_rate": 0.0001951143895659611,
      "loss": 2.0797,
      "step": 462
    },
    {
      "epoch": 0.024741496780399178,
      "grad_norm": 1.7076621055603027,
      "learning_rate": 0.00019510369895231987,
      "loss": 2.2944,
      "step": 463
    },
    {
      "epoch": 0.024794934138456196,
      "grad_norm": 1.6337462663650513,
      "learning_rate": 0.00019509300833867865,
      "loss": 2.2865,
      "step": 464
    },
    {
      "epoch": 0.024848371496513214,
      "grad_norm": 1.4570035934448242,
      "learning_rate": 0.00019508231772503743,
      "loss": 2.1124,
      "step": 465
    },
    {
      "epoch": 0.02490180885457023,
      "grad_norm": 1.6131442785263062,
      "learning_rate": 0.00019507162711139618,
      "loss": 2.0855,
      "step": 466
    },
    {
      "epoch": 0.024955246212627246,
      "grad_norm": 1.6882671117782593,
      "learning_rate": 0.000195060936497755,
      "loss": 2.3363,
      "step": 467
    },
    {
      "epoch": 0.025008683570684264,
      "grad_norm": 2.1365067958831787,
      "learning_rate": 0.00019505024588411377,
      "loss": 2.1806,
      "step": 468
    },
    {
      "epoch": 0.025062120928741282,
      "grad_norm": 2.0290260314941406,
      "learning_rate": 0.00019503955527047252,
      "loss": 2.1939,
      "step": 469
    },
    {
      "epoch": 0.0251155582867983,
      "grad_norm": 1.8123079538345337,
      "learning_rate": 0.0001950288646568313,
      "loss": 2.3007,
      "step": 470
    },
    {
      "epoch": 0.025168995644855318,
      "grad_norm": 1.6627124547958374,
      "learning_rate": 0.0001950181740431901,
      "loss": 2.2224,
      "step": 471
    },
    {
      "epoch": 0.025222433002912336,
      "grad_norm": 1.4242817163467407,
      "learning_rate": 0.00019500748342954886,
      "loss": 2.1412,
      "step": 472
    },
    {
      "epoch": 0.025275870360969354,
      "grad_norm": 1.7171142101287842,
      "learning_rate": 0.00019499679281590764,
      "loss": 2.2832,
      "step": 473
    },
    {
      "epoch": 0.025329307719026372,
      "grad_norm": 1.901180386543274,
      "learning_rate": 0.00019498610220226642,
      "loss": 2.391,
      "step": 474
    },
    {
      "epoch": 0.02538274507708339,
      "grad_norm": 2.2097671031951904,
      "learning_rate": 0.0001949754115886252,
      "loss": 2.2794,
      "step": 475
    },
    {
      "epoch": 0.025436182435140408,
      "grad_norm": 2.028759717941284,
      "learning_rate": 0.00019496472097498398,
      "loss": 2.3262,
      "step": 476
    },
    {
      "epoch": 0.025489619793197426,
      "grad_norm": 1.851659893989563,
      "learning_rate": 0.00019495403036134273,
      "loss": 2.2633,
      "step": 477
    },
    {
      "epoch": 0.02554305715125444,
      "grad_norm": 1.7000919580459595,
      "learning_rate": 0.00019494333974770154,
      "loss": 2.1925,
      "step": 478
    },
    {
      "epoch": 0.02559649450931146,
      "grad_norm": 1.4994149208068848,
      "learning_rate": 0.00019493264913406032,
      "loss": 2.0432,
      "step": 479
    },
    {
      "epoch": 0.025649931867368476,
      "grad_norm": 1.6292952299118042,
      "learning_rate": 0.00019492195852041907,
      "loss": 2.2275,
      "step": 480
    },
    {
      "epoch": 0.025703369225425494,
      "grad_norm": 1.2673231363296509,
      "learning_rate": 0.00019491126790677785,
      "loss": 2.2141,
      "step": 481
    },
    {
      "epoch": 0.025756806583482512,
      "grad_norm": 1.658721923828125,
      "learning_rate": 0.00019490057729313666,
      "loss": 2.0951,
      "step": 482
    },
    {
      "epoch": 0.02581024394153953,
      "grad_norm": 2.232370138168335,
      "learning_rate": 0.0001948898866794954,
      "loss": 2.2737,
      "step": 483
    },
    {
      "epoch": 0.02586368129959655,
      "grad_norm": 1.5627198219299316,
      "learning_rate": 0.0001948791960658542,
      "loss": 2.3156,
      "step": 484
    },
    {
      "epoch": 0.025917118657653566,
      "grad_norm": 1.6117973327636719,
      "learning_rate": 0.00019486850545221297,
      "loss": 2.3256,
      "step": 485
    },
    {
      "epoch": 0.025970556015710584,
      "grad_norm": 2.249568223953247,
      "learning_rate": 0.00019485781483857175,
      "loss": 2.2664,
      "step": 486
    },
    {
      "epoch": 0.026023993373767602,
      "grad_norm": 1.8979620933532715,
      "learning_rate": 0.00019484712422493053,
      "loss": 2.4969,
      "step": 487
    },
    {
      "epoch": 0.026077430731824617,
      "grad_norm": 2.128509759902954,
      "learning_rate": 0.0001948364336112893,
      "loss": 2.2922,
      "step": 488
    },
    {
      "epoch": 0.026130868089881635,
      "grad_norm": 1.6429524421691895,
      "learning_rate": 0.00019482574299764806,
      "loss": 2.1732,
      "step": 489
    },
    {
      "epoch": 0.026184305447938653,
      "grad_norm": 2.024879217147827,
      "learning_rate": 0.00019481505238400687,
      "loss": 2.143,
      "step": 490
    },
    {
      "epoch": 0.02623774280599567,
      "grad_norm": 1.5489376783370972,
      "learning_rate": 0.00019480436177036562,
      "loss": 2.1844,
      "step": 491
    },
    {
      "epoch": 0.02629118016405269,
      "grad_norm": 1.3861124515533447,
      "learning_rate": 0.0001947936711567244,
      "loss": 2.353,
      "step": 492
    },
    {
      "epoch": 0.026344617522109707,
      "grad_norm": 1.7259782552719116,
      "learning_rate": 0.0001947829805430832,
      "loss": 2.2519,
      "step": 493
    },
    {
      "epoch": 0.026398054880166725,
      "grad_norm": 1.6109014749526978,
      "learning_rate": 0.00019477228992944196,
      "loss": 2.3396,
      "step": 494
    },
    {
      "epoch": 0.026451492238223743,
      "grad_norm": 1.8184728622436523,
      "learning_rate": 0.00019476159931580074,
      "loss": 2.3335,
      "step": 495
    },
    {
      "epoch": 0.02650492959628076,
      "grad_norm": 1.2416975498199463,
      "learning_rate": 0.00019475090870215952,
      "loss": 2.2105,
      "step": 496
    },
    {
      "epoch": 0.02655836695433778,
      "grad_norm": 1.635772466659546,
      "learning_rate": 0.0001947402180885183,
      "loss": 2.0831,
      "step": 497
    },
    {
      "epoch": 0.026611804312394797,
      "grad_norm": 1.3396542072296143,
      "learning_rate": 0.00019472952747487708,
      "loss": 2.0149,
      "step": 498
    },
    {
      "epoch": 0.02666524167045181,
      "grad_norm": 1.4073172807693481,
      "learning_rate": 0.00019471883686123586,
      "loss": 2.2383,
      "step": 499
    },
    {
      "epoch": 0.02671867902850883,
      "grad_norm": 1.2398576736450195,
      "learning_rate": 0.0001947081462475946,
      "loss": 2.2385,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 18713,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 660345796217856.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
